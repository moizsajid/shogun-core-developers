--- Log opened Fri Feb 10 00:00:19 2012
 n4nd0	I am getting lot of compile errors to make shogun with cplex
 n4nd0	not just in the examples but also in more important files such as mathematics/Cplex
 n4nd0	is that normal?
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has quit [Remote host closed the connection]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Quit: wiking]
-!- dfrx [~f-x@inet-hqmc07-o.oracle.com] has joined #shogun
-!- shogun-buildbot [~shogun-bu@7nn.de] has quit [Ping timeout: 240 seconds]
-!- shogun-buildbot [~shogun-bu@7nn.de] has joined #shogun
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- CIA-11 [~CIA@cia.atheme.org] has quit [Ping timeout: 416 seconds]
-!- CIA-18 [~CIA@cia.atheme.org] has joined #shogun
-!- shogun-buildbot_ [~shogun-bu@7nn.de] has joined #shogun
-!- shogun-buildbot [~shogun-bu@7nn.de] has quit [Ping timeout: 252 seconds]
-!- wiking [~wiking@78-22-115-59.access.telenet.be] has joined #shogun
-!- wiking [~wiking@78-22-115-59.access.telenet.be] has quit [Changing host]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Quit: wiking]
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has quit [Ping timeout: 244 seconds]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- Netsplit *.net <-> *.split quits: naywhayare
-!- Netsplit over, joins: naywhayare
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has quit [Ping timeout: 265 seconds]
-!- Netsplit *.net <-> *.split quits: @sonney2k, CIA-18, shogun-buildbot_, naywhayare, wiking, dfrx
-!- Netsplit over, joins: naywhayare, shogun-buildbot_, CIA-18, dfrx, @sonney2k
-!- naywhaya1e [~ryan@spoon.lugatgt.org] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- naywhayare [~ryan@spoon.lugatgt.org] has quit [Ping timeout: 252 seconds]
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- n4nd0 [~n4nd0@n145-p102.kthopen.kth.se] has joined #shogun
-!- dfrx [~f-x@inet-hqmc07-o.oracle.com] has quit [Quit: Leaving.]
-!- n4nd0 [~n4nd0@n145-p102.kthopen.kth.se] has quit [Quit: Leaving]
-!- wiking_ [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Ping timeout: 276 seconds]
-!- wiking_ is now known as wiking
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
 CIA-18	shogun: Soeren Sonnenburg master * rdb752f7 / (6 files in 2 dirs):
 CIA-18	shogun: Merge pull request #368 from vigsterkr/master
 CIA-18	shogun: Add Jensen-Shannon kernel - http://git.io/hOxavQ
 CIA-18	shogun: Viktor Gal master * re0a1155 / (6 files in 2 dirs):
 CIA-18	shogun: Add Jensen-Shanon kernel This patch adds a CDotKernel based Jensen-Shannon
 CIA-18	shogun: kernel to shogun-toolbox. Sources of modular interface has been changed so it
 CIA-18	shogun: can be used via the modular interfaces as well. - http://git.io/z196rQ
 CIA-18	shogun: Viktor Gal master * r51fcf29 / src/shogun/kernel/JensenShannonKernel.cpp : Change to CMath::log2 in JensenShannonKernel - http://git.io/MMk_jQ
 wiking	yeey my first patch applied to HEAD \o/
-!- naywhaya1e is now known as naywhayare
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has joined #shogun
 n4nd0	hi there
 n4nd0	can someone help me with cplex installation?
 n4nd0	I have found quite a bit of compile errors
-!- wiking_ [~wiking@huwico/staff/wiking] has joined #shogun
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has quit [Ping timeout: 245 seconds]
-!- wiking [~wiking@huwico/staff/wiking] has quit [Ping timeout: 252 seconds]
-!- wiking_ is now known as wiking
-!- blackburn [~qdrgsm@83.234.54.44] has joined #shogun
 blackburn	wiking: is this J-S kernel better on histograms?
 wiking	blackburn: for my data set definitely
 blackburn	what is your dataset?
 wiking	there's a +4% on average accuracy
 blackburn	better than HIK?
 wiking	but then again it's quite slower as well :P
 wiking	yep better than HIK
 blackburn	huh
 blackburn	!
 blackburn	interesting
 wiking	but as said i think it very much depends on your dataset as well
 wiking	as always though :P
 wiking	but i have a simple bag of visual words setup
 wiking	and if i l2-norm the tf vectors
 wiking	js is better than hik
 blackburn	I've used HOG with HIK, was better than linear of course
 blackburn	but I didn't know anything about js kernel
 wiking	and about the train/infer speed... it could be easily make it in a way to use linear classification
 wiking	http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CCYQFjAA&url=http%3A%2F%2Fwww.vlfeat.org%2F~vedaldi%2Fassets%2Fpubs%2Fvedaldi11efficient.pdf&ei=uG01T6qzJcui-gbWwfSSAg&usg=AFQjCNEOGMdgVs0gaPgm60i76bNakkyicQ&sig2=TAz11bIm2OAogWtcp8fACQ
 wiking	but it requires some extra coding for shogun...
 blackburn	btw are you a student?
 wiking	ye phd
 wiking	so the title is: Efficient Additive Kernels via Explicit Feature Maps
 blackburn	yeah seen that
 wiking	part of vlfeat project...
 wiking	so there's already code of course
 wiking	i mean i was already surprised about HIK performance...
 blackburn	I'm just curious whether you are going to apply as gsoc student :)
 wiking	hahahahaha
 wiking	i don't know
 wiking	did once a GSoC project
 wiking	was fun
 wiking	but last year was weird
 blackburn	which one?
 wiking	i've applied to a gsoc project in opencv proj... and didn't got it... and on the end the whole gsoc project just failed; it seems the guy who got it just went off...
 wiking	so it's a shame
 wiking	for shogun it'd be great to add some new machines as part of gsoc project
 blackburn	so you wasn't a gsoc student?
 blackburn	weren't sorry :D
 wiking	last year not
 wiking	i was in 2009
 blackburn	oh I see
 blackburn	I was shogun gsoc student last year
 wiking	what have you've done?
 blackburn	dim reduction
 wiking	ah cool, the k(pac) part?
 wiking	*pca
 wiking	so i've ment (k)pca
 wiking	:P
 blackburn	mainly LLE and similar once
 blackburn	ones
 blackburn	:D
 wiking	:>
 blackburn	http://shogun-toolbox.org/edrt/
 wiking	i've seen on last year's project ideas that there was an interest in doing latent s-svm
 wiking	oh fuck
 wiking	niice
 blackburn	recently I've submitted a paper on that thing
 wiking	isn't it merged into shogun master branch?
 blackburn	well merged.. :)
 wiking	on which thing?
 wiking	you mean on your gsoc proj?
 blackburn	not really, I called it toolkit
 blackburn	and said it is integrated to shogun
 blackburn	http://dl.dropbox.com/u/10139213/shogun/lisitsyn12a.pdf
 wiking	ah cool
 wiking	i'll go through it
 blackburn	nothing interesting :)
 wiking	heheh
 wiking	niiice it's a jmlr article
 wiking	i'm working now on a ieee transaction on multimedia paper... and will try to submit it to pami
 wiking	but my hopes aren't so ... high :P
 blackburn	it is my first paper _ever_ so I hope it would be accepted :)
 wiking	hihi
 wiking	today i should get as well a notification for a paper of mine... wonder when is it going to happen :>
 wiking	so yeah try the j-s stuff and let me know how's it doing for you
 blackburn	sure
 wiking	ah yeah use l2-norm....
 blackburn	I'm using HOG-like thing
 wiking	mmm
 blackburn	without blocks in fact :)
 wiking	do u have a good (meaning c) implementation of it
 wiking	?
 blackburn	nope
 blackburn	stole python one from google
 wiking	hahahaha
 wiking	how's that working for you?
 wiking	fast enough?
 blackburn	http://code.google.com/p/python-cvgreyc/source/browse/trunk/cvgreyc/features/HOG.py?r=83
 blackburn	well svm training takes 3h so I don't care
 blackburn	:D
 blackburn	I do road signs recognition
 wiking	what's your average resolution for a picture?
 blackburn	I resize it to 60x60
 wiking	because i have like tons of pix (one is a 14 gigs the other is 22 gigs dataset)
 blackburn	oh
 wiking	so i'm already using hadoop cluster to get the features
 blackburn	dataset I use contains 39K training vectors and 12.6K test ones
 wiking	maybe i'll reimplement then this python thingy in opencv
 wiking	ah ok
 wiking	niiice
 blackburn	hmm doesn't opencv have HOG already?
 wiking	mmm
 wiking	they have something specific
 wiking	for pedestrian detection
 blackburn	yeah
 blackburn	I find opencv rather unusable
 wiking	hehehe yeah
 wiking	i'm just using it for some basic shit
 wiking	it's very bloated now
 wiking	and now they have this whole transition of the api
 wiking	and some of the bugs i've encountered... so it gave me quite a headache sometimes
 wiking	but anyhow some basic functions are there, so it's ok
 blackburn	I would be disappointed if JS will be better on my dataset :D
 wiking	hahahaha why? :)
 blackburn	I was near to submit a paper to some local journal here
 blackburn	in russian
 wiking	well
 blackburn	it was about HIK and HOG
 wiking	just change fast the thingy
 wiking	to JS and HOG
 wiking	:>>.
 blackburn	:D yeah but I have formulated multiclass thing
 wiking	you know: s/HIK/JS/g
 wiking	and there you go
 blackburn	the one I said yesterday
 wiking	:)
 blackburn	with O(log)
 wiking	ah
 blackburn	anyway it is pretty obvious
 blackburn	wiking: so JS is the best histogram kernel you know?
 wiking	hehehehe well you know occ.'s razor... the most obvious should be the best
 wiking	blackburn: hehehehe well this is so far giving me the best results
 wiking	i've read now fast several papers on it
 blackburn	I see
 wiking	some people of course like chi2 as well
 wiking	but that said you should do l1-norm on your features before using chi2 kernel
 wiking	makes a big difference :P
 blackburn	I see
 wiking	but yeah i've tried here chi2, hik and now js on my dataset... and js came out the best
 wiking	with LaRank machine
 blackburn	I use GMNP now
 blackburn	but I guess your dataset is too big
 wiking	mmm it's not that bad actually
 wiking	on the end i'm now only having 4k features and 2k samples
 blackburn	ah
 wiking	so it's a pretty small
 blackburn	then try GMNP too if you didn't
 wiking	have
 wiking	LaRank gave me better accuracy
 blackburn	really?
 wiking	well at least with HIK
 wiking	haven't tried now with JS
 blackburn	that's damn strange!
 wiking	i can give it a go...
 wiking	:)
 blackburn	I usually get better results with GMNP
 blackburn	wiking: I just glanced over you blog ;)
 wiking	hahaha
 blackburn	are you hungarian?
 wiking	very un-up-to-date and very not about my research :>
 wiking	sort of
 wiking	born in serbia
 blackburn	serbia? nice
 wiking	ok lemme try gnmp
 wiking	*gmnp that is
 wiking	do i have to switch to vpn
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
 wiking	back
 blackburn	have you ever wondered what is GMNP stands for? :)
 blackburn	I bet the reason why you understand my broken english is you are non-native as well
 blackburn	:D
 wiking	hahahah
 wiking	no worries your english is fine
 wiking	ok running the test... what's gmnp stands for?
 blackburn	hah
 blackburn	it stands for generalized minimal norm problem
 wiking	hehe still training ;)
 wiking	takes longer than larank for sure
 blackburn	yeah it is slower a little
 blackburn	did you tune epsilon?
 wiking	well i set it to the same as larank 1e-5
 blackburn	in my experience epsilon=1e-2 was ok
 blackburn	ah I see
 wiking	let's see what it does with the same eps
 blackburn	it would train much slower
 wiking	btw in kernels there could be some SIMD instructions
 wiking	so like in case of JS
 wiking	the whole for loop could be parallelized
 blackburn	sure but we have to do it generic..
 blackburn	in shogun
 wiking	yeah i know but still
 wiking	is there use of OpenMP
 wiking	?
 blackburn	no, we use pthread
 wiking	so simply pthread?
 wiking	that's kind of hc :>
 wiking	i mean hardcore
 blackburn	yeah, take a look on shogun/converter/LocallyLinearEmbedding.cpp
 blackburn	I would say it is hardcore porn
 blackburn	I bet the most hardcore code is shogun's arpack wrapper written by me as well
 blackburn	shogun/mathematics/arpack.cpp
 blackburn	it embeds superlu, blas, lapack and arpack at the same time
 wiking	hhihihi
 wiking	ok now it's doing the inference...
 blackburn	btw how many classes?
 wiking	18
 blackburn	I see
 blackburn	I have 43
 wiking	mmm
 wiking	if there's difference then it's like within 1%
 wiking	just got the average acc
 wiking	and i cannot recall the digits after the decimal point :P
 blackburn	is it worse?
 blackburn	:D
 wiking	so if then there is like 0.3% difference
 wiking	let's see with another eps
 wiking	anyhow i should do the hog thingy
 wiking	i'm using currently DSIFT features
 wiking	no actually i'm not telling the truth... now it's with affine-sift :P
 blackburn	I'm pretty lame with SIFT still
 blackburn	what is the main difference between SIFT and HOG?
 blackburn	I saw SIFT uses histograms too
 wiking	well
 wiking	first of all sift has a key point detector part as well
 wiking	so it's not just giving you a descriptor
 wiking	but key points (scale invariant) in a pic
 blackburn	so, it finds key points
 blackburn	and then calculates features similar to hog around these points?
 wiking	as said in it's name it should be scale invariant
 blackburn	the most attractive thing for me is that HOG can be formulated as integral image
 wiking	but yeah when u ask for a sift descriptor it's pretty similar with hog
 wiking	but of course the shit part with sift is that it's pattented
 wiking	i mean u can use it for academic purposes but then again
 blackburn	I don't really care about patents for now
 wiking	but yeah i've seen the bag of visual words being patented as well
 wiking	so it's pretty fucked up with USA  :>
 blackburn	well even for commercial purpose I can use HOG/SIFT/anyway-patented here :)
 wiking	damn i should implement my MKL thingy
 wiking	hehehe yeah russia is cool
 blackburn	cause I live in snowy nigeria hah
 wiking	and afaik in eu it's the same
 blackburn	do you use mkl for images?
 wiking	wel
 wiking	i have different type of features
 blackburn	I've been thinking about it
 blackburn	ahh I see
 wiking	some of them coming from textual
 wiking	and the textual part is quite sparse
 wiking	so i should actually do some laplacian kernel on the textual (sparse) features
 wiking	and JS on the histogram features
 wiking	i'm hoping that it'd make a difference
 blackburn	sad JS can't be formulated with this O(log N)
 blackburn	as HIK
 wiking	until now i was just concatenating all the features and used a simple polykernel
 wiking	mkl should do better
 blackburn	sure
 wiking	eheheh
 wiking	so
 wiking	0.7841796875 with eps 1e-5
 wiking	0.78515625 with eps 1e-2
 wiking	so there's a difference
 wiking	but quite insignificant
 blackburn	yeah looks like 3 examples or so? :)
 wiking	yeah something like that
 blackburn	what is larank best?
 wiking	mmm good queston
 wiking	i know it was as well 0.78...
 blackburn	I see
 blackburn	I have best 97.32% accuracy on my data
 wiking	heheheh
 blackburn	hope to get it to 98%
 blackburn	with some improvements
 wiking	what's your test set size?
 blackburn	12630
 blackburn	729 features
 wiking	have u tried deep learning?
 blackburn	nope
 blackburn	yeah it is better on that data I know
 blackburn	but I hope to get similar accuracy with svm
 wiking	i should try deep learning
 wiking	haven't got around it yet
 wiking	theano seems to be a good tool for it
 blackburn	I don't like NNs without any reason :D
 wiking	hahahahha
 wiking	well they'll learn everything... just needs some time
 wiking	and overfit :>
 blackburn	and needs some crazy tuning..
 wiking	hehheheheh yeps
 wiking	but anyhow it works as well
 wiking	quite well
 blackburn	sure
 wiking	0.783203125 eps 1e-2
 blackburn	larank?
 wiking	y
 blackburn	so gmnp was better!
 blackburn	:)
 wiking	hahahaha
 wiking	so yeah my best result on this dataset with many more features but a simple poly kernel is 0.879
 wiking	so i'm hoping that maybe i can do something now about this with mkl
 blackburn	hmm strange such naive way was so better?
 wiking	but yeah
 wiking	with may more features...
 wiking	now with a poly kernel on the same feature set it's 0.73 with polykernel
 wiking	but yeah JS is cool :>>.
 wiking	but slow :)
 blackburn	it would be unbelievably cool if using JS gave me ~98% accuracy :)
 wiking	hahahhaha
 wiking	well give it a go
 wiking	or it's hard to switch the kernel?
 wiking	0.783203125 with eps 1e-5
 blackburn	no, not hard
 wiking	so yeah actually GMNP is better :>
 blackburn	one line to switch
 blackburn	:)
 wiking	than for the tipp
 blackburn	the only problem is that training takes 8900s
 wiking	ahhahaha
 wiking	try then the vlfeat stuff
 wiking	extra coding though... buuut
 wiking	much faster training
 blackburn	which one?
 wiking	because it has JS kernel
 wiking	but used with this homogeneous kernel mapping
 blackburn	hmm how can it help?
 wiking	so it's actually running linear
 blackburn	ah
 wiking	they guy wrote the mapping for both JS and HIK kernel
 wiking	and even has matlab api if u prefer that maybe
 blackburn	I got no time to get into that paper
 blackburn	how can it be possible?
 wiking	what?
 blackburn	-> linear it
 wiking	well as said
 wiking	in that paper i've mentioned
 blackburn	is it creating new features?
 wiking	well mapping the features as far as i understand
 blackburn	explicitly?
 wiking	and then those features are simply fed to a linear svm
 blackburn	I see
 wiking	http://www.vlfeat.org/api/homkermap.html#homkermap-overview
 wiking	according to the paper he not only has faster running time with this of course
 wiking	but sometimes the accuracy is better as well
 blackburn	hmm
 blackburn	size of d(2n+1)?!
 blackburn	why is it faster?
 blackburn	with such vast feature spaces
 wiking	:)
 wiking	well check the paper for running times
 wiking	same or better accuracy with 1/7 training time
 blackburn	I just can't understand how can it be faster than Maji's approximation
 blackburn	afaik it doesn't depend on number of support vectors..
-!- dfrx [~f-x@inet-hqmc06-o.oracle.com] has joined #shogun
 wiking	yeees
 wiking	one paper accepted
 wiking	!
 wiking	:))
 wiking	mmm i wasn't expecting the result of this one yet... so let's see how the other paper will do...fingerzcrossed!!
-!- n4nd0 [~androirc@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- nando_ [~n4nd0@s83-179-44-135.cust.tele2.se] has joined #shogun
 nando_	blackburn, hey!
-!- n4nd0 [~androirc@s83-179-44-135.cust.tele2.se] has quit [Client Quit]
-!- nando_ [~n4nd0@s83-179-44-135.cust.tele2.se] has left #shogun []
-!- nando_ [~n4nd0@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- nando_ [~n4nd0@s83-179-44-135.cust.tele2.se] has left #shogun []
-!- n4nd0 [~n4nd0@s83-179-44-135.cust.tele2.se] has joined #shogun
 blackburn	n4nd0: hey
 blackburn	wiking: congrats!
 wiking	blackburn: cheers
 n4nd0	blackburn, I faced some trouble trying to compile shogun with support for cplex
 blackburn	n4nd0: okay, are there any errors?
 n4nd0	n4nd0, yeah, I had to make a couple of changes in the configure file to detect my cplex version
 n4nd0	blackburn, I don't know if I screwed something when doing that, but before it was not detecting cplex when doing configure
 blackburn	hmm that's strange
 n4nd0	blackburn, and well after that ... there were quite a few of compile errors
 blackburn	let me check whether it is being detected here
 n4nd0	you think is strange that I needed to change the configure?
 blackburn	sure
 n4nd0	I read something on that in a forum, it think that the configure script is prepared to work out of fox with version 9 of cplex
 blackburn	oh never thought it is proprietary :D
 blackburn	okay lets check what you have changed?
 n4nd0	all right
 blackburn	I guess it would take more time to install/test things here than just 'use' you :)
 n4nd0	http://snipt.org/uIfg6
 n4nd0	I commented three lines there and just did minor changes
 n4nd0	I was also surprised with the software being proprietary :-O
 n4nd0	and took a long while to get the trial version for students!
 blackburn	okay sure it is hardcoded now
 blackburn	so you had to fix it
 n4nd0	the lines I added are the ones that are just above the commented ones
 blackburn	so, now it is being detected right?
 n4nd0	yes :-)
 n4nd0	that was the good part
 blackburn	okay then lets check errors
 n4nd0	the bad part ... when I configure with support for cplex
 n4nd0	make explodes :-O
 n4nd0	so a common error I found in several files
 blackburn	is it a nuclear explosion or just like some bomb?
 blackburn	:D
 n4nd0	:-P still alive around here
 n4nd0	so this common error, like in classifier/SubGradientLPM.cpp
 n4nd0	there were quite a bit references to a class called CLinearClassifier
 n4nd0	that was not found, it doesn't appear in the doc either
 blackburn	hmmmmmmm
 blackburn	wait
 blackburn	haha
 blackburn	indeed
 n4nd0	I changed those to CLinearMachine ... don't know if that was a good decision
 blackburn	yeah it was obviously right decision
 blackburn	let me change it
 n4nd0	cool
 n4nd0	as far I remember those were in
 n4nd0	SubGradientLPM header and source
 blackburn	LPBoost too
 n4nd0	yeah
 n4nd0	ah and another one in SubGradientLPM.h
 blackburn	hmm
 n4nd0	the include of qpbsvmlib.h
 n4nd0	the file is called QPBSVMLib.h
 blackburn	do you know how to use github pull requests?
 n4nd0	yeah
 blackburn	I just thought you could do it :)
 n4nd0	sure, cool
 blackburn	and I would just merge it
 blackburn	LinearClassifier and LinearMachine is an issue related to some transition we have made an year ago
 n4nd0	ok
 blackburn	to generalize classification/regression things
 blackburn	we have renamed Classifier to Machine
 blackburn	that was damn ugly to Regression derived from Classification :)
 blackburn	and the reason why we haven't detected it is obvious too :)
 n4nd0	:)
 n4nd0	there were also some issues with classifier/svm/CPLEXSVM*
 n4nd0	and mathematics/Cplex*
 n4nd0	can you reproduce the compile error?
 n4nd0	ping
 blackburn	here
 blackburn	n4nd0: not really
--- Log closed Sat Feb 11 00:00:16 2012
