--- Log opened Wed May 25 00:00:10 2016
-!- besser82 [~besser82@fedora/besser82] has quit [Ping timeout: 250 seconds]
-!- sanuj [~sanuj@117.203.19.130] has joined #shogun
-!- sanuj [~sanuj@117.203.19.130] has quit [Ping timeout: 244 seconds]
-!- sanuj [~sanuj@117.203.19.130] has joined #shogun
-!- Saurabh7 [Saurabh7@gateway/shell/panicbnc/x-qkwwseokbysimqpf] has quit [Ping timeout: 260 seconds]
@wiking	buuuyaaaa
@wiking	where are all the gsoc slaves? :D
 sanuj	wiking, hello
 sanuj	:P
@wiking	whatsup
@wiking	how's the tagging going?
 sanuj	i fixed a cookbook bug yesterday
 sanuj	sending pr for that
@wiking	cook
@wiking	*cool
@wiking	have you encountered any problems with the PR bot?
 sanuj	i need to make shogun support new tags and parameter
 sanuj	merge it in develop
@wiking	mmm
@wiking	i would do that first in a feature branch
@wiking	plz
 sanuj	yes feature branch of course
 sanuj	then merge it
@wiking	do you have a feature branch for that?
 sanuj	that's first priority as OXPHOS needs to do serialization for that
@wiking	dont see one
 sanuj	no
@wiking	ok i'll start a feature branch
 sanuj	yes :)
@wiking	so you can pr to that one
 sanuj	thanks
@wiking	ok
@wiking	there it is
@wiking	feature/tags
 sanuj	cool
 lisitsyn	BOOYAAA
 sanuj	lisitsyn, are you in office?
 lisitsyn	not yet
 sanuj	okay
 lisitsyn	why?
 sanuj	just asking
 lisitsyn	I see :)
 sanuj	when does it snow in moscow
 sanuj	:P
 lisitsyn	sanuj: deffs not now :D
 sanuj	haha
 lisitsyn	sanuj: well a few times in october usually
 sanuj	it's 40 degree C here
 sanuj	hot hot hot
 lisitsyn	then it is all covered with snow in mid of november
 sanuj	ohkay
 lisitsyn	and finally in early april it is gone
@wiking	SAMARA FOR LIFE :)
 lisitsyn	it is unusually cold now - like 16C
 sanuj	nice
@wiking	lisitsyn: ^ :>
 sanuj	okay
 lisitsyn	wiking: well, moscow :D
@wiking	but you are not from mosco
@wiking	w
@wiking	:P
 lisitsyn	nor from samar
 lisitsyn	a
 lisitsyn	:D
 sanuj	there are many good sport coders from russia
@wiking	lisitsyn: loool?
@wiking	where then?
 sanuj	lisitsyn, are you also one of them? ;)
 lisitsyn	wiking: https://en.wikipedia.org/wiki/Tolyatti
 lisitsyn	sanuj: nah
@wiking	oblast smara!
 lisitsyn	sanuj: don't like it at all
@wiking	there you go
 lisitsyn	wiking: hah yes
 sanuj	lisitsyn, we need to do it here for job interviews :P
 lisitsyn	sanuj: sure
 lisitsyn	sanuj: actually not for me I think
 lisitsyn	I mean
 sanuj	even i don't do it on my own
 lisitsyn	if I was to change my job it would be quite stupid to ask me to rotate a tree
 sanuj	haha
 lisitsyn	as I am leading a team of 6 now
 lisitsyn	:D
 sanuj	great!
@wiking	lisitsyn: hahahah
@wiking	lisitsyn: they still do ask me that :)
 lisitsyn	wiking: yeah totally stupid
@wiking	last time i told the guy
@wiking	look man
@wiking	i worked with this shit for the last 4 years writing my phd thesis
@wiking	i think i know what i was doing
@wiking	they took my answer personally :P
 lisitsyn	wiking: well I don't mind if they ask puzzles
 lisitsyn	wiking: but not like in that ACM
@wiking	ah no it wans't a puzzle
@wiking	it was like
@wiking	what is an avl-tree
@wiking	and how it works
@wiking	:)))
 lisitsyn	haha ok
 lisitsyn	well
@wiking	or same for kd-tree
@wiking	and it was clear for me
 lisitsyn	well it is not that bad
@wiking	that they guy who asked me the question
 lisitsyn	it would be terrible if they asked you this type of problem
 lisitsyn	like
@wiking	had no clue what is a kd-tree
 lisitsyn	nah I don't know
@wiking	because he used only psql's function to find the nearest points
@wiking	:)
 lisitsyn	hah
@wiking	so i was a bit asshole when i answered obviously
@wiking	but then i realised that i really dont even wanna work with them :)
 sanuj	wiking, you have 2 phds!?
 sanuj	i saw your linkedin :P
 lisitsyn	oh I have a story about linkedin
 lisitsyn	I've deleted mine
 lisitsyn	:D
 sanuj	haha
@wiking	sanuj: ahahhah no
 sanuj	the less stories the better
@wiking	sanuj: i've just started my phd somewhere
@wiking	and finsihed somewhere else
@wiking	lisitsyn: man i wanted to do that
@wiking	how did you do it?!
 sanuj	wiking,  so you will be having 2 phds after a few years
 lisitsyn	wiking: well clicked somewhere in help
@wiking	sanuj: hahah i'll not finish taht first one :(
@wiking	and one is more than enough for me trust me
 sanuj	for anyone i guess
 sanuj	:)
 sanuj	wiking, you work with medical images?
@wiking	sanuj: used to
 sanuj	okay
@wiking	as my research yes
 sanuj	cool
 sanuj	wiking, lisitsyn i think both of you have blogs
 lisitsyn	yeah but I fail to continue that
 lisitsyn	:D
 lisitsyn	I've found some will to post a few things
 sanuj	lisitsyn, why do you blog
 sanuj	i need inspiration to do that
 sanuj	okay
@wiking	lisitsyn: i'll start pinging you on my blog
@wiking	and maybe you'll start answering
@wiking	:DDD
@wiking	i think the last time i wrote a blog was ages ago
@wiking	anyhow
@wiking	fuuuuuuuuuuuck
@wiking	arianepaola: hey
@wiking	so we need to setup benchmarking bots
 lisitsyn	sanuj: well it helps to write things
 lisitsyn	like a defrag for your brain
 sanuj	okay
 shogun-buildbot	build #2873 of bsd1 - libshogun is complete: Failure [failed configure]  Build details are at http://buildbot.shogun-toolbox.org/builders/bsd1%20-%20libshogun/builds/2873  blamelist: Viktor Gal <viktor.gal@maeth.com>
 lisitsyn	wiking: that's gonna help the humanity
 lisitsyn	two useful blogs
@wiking	:>>>.
-!- besser82 [~besser82@fedora/besser82] has joined #shogun
-!- mode/#shogun [+o besser82] by ChanServ
-!- Saurabh7 [~Saurabh7@59.96.102.84] has joined #shogun
 sanuj	time for food
-!- sanuj [~sanuj@117.203.19.130] has quit [Ping timeout: 276 seconds]
 shogun-buildbot	build #11 of xenial - libshogun is complete: Failure [failed test]  Build details are at http://buildbot.shogun-toolbox.org/builders/xenial%20-%20libshogun/builds/11  blamelist: Viktor Gal <viktor.gal@maeth.com>
-!- Saurabh7- [Saurabh7@gateway/shell/panicbnc/x-tkhunxevfgkzzhnq] has joined #shogun
-!- leagoetz [~leagoetz@host-92-0-162-192.as43234.net] has joined #shogun
-!- travis-ci [~travis-ci@ec2-174-129-185-139.compute-1.amazonaws.com] has joined #shogun
 travis-ci	it's Viktor Gal's turn to pay the next round of drinks for the massacre he caused in shogun-toolbox/shogun: https://travis-ci.org/shogun-toolbox/shogun/builds/132773374
-!- travis-ci [~travis-ci@ec2-174-129-185-139.compute-1.amazonaws.com] has left #shogun []
-!- leagoetz [~leagoetz@host-92-0-162-192.as43234.net] has quit []
-!- lambday [8028b10a@gateway/web/freenode/ip.128.40.177.10] has joined #shogun
-!- mode/#shogun [+o lambday] by ChanServ
-!- sonne|osx [~sonne@x4db48e9e.dyn.telefonica.de] has joined #shogun
-!- sanuj [~sanuj@117.203.19.130] has joined #shogun
-!- sonne|osx [~sonne@x4db48e9e.dyn.telefonica.de] has quit [Quit: sonne|osx]
-!- besser82 [~besser82@fedora/besser82] has quit [Ping timeout: 264 seconds]
-!- sonne|osx [~sonne@x4db48e9e.dyn.telefonica.de] has joined #shogun
-!- Saurabh7 [~Saurabh7@59.96.102.84] has quit [Quit: Leaving]
-!- HeikoS [~heiko@nat-206-247.internal.eduroam.ucl.ac.uk] has joined #shogun
-!- mode/#shogun [+o HeikoS] by ChanServ
 sanuj	HeikoS, there?
 sanuj	lisitsyn, there?
 lisitsyn	sanuj: yes
 sanuj	Int is working now in cookbook
 sanuj	but Int num_components = 3
 sanuj	is translated to
 sanuj	double num_components = 3;
 sanuj	which gives error
 sanuj	in java
 sanuj	lisitsyn, i checked java.json
 sanuj	looks fine
-!- sanuj [~sanuj@117.203.19.130] has quit [Remote host closed the connection]
 rcurtin	HeikoS: you were looking for me yesterday
@wiking	rcurtin: do you guys maybe have
@wiking	a server to share for the benchmark project ?:)
@wiking	rcurtin: currenlty i'm planning to put openstack on our machines
@wiking	and setup benchmarking like that
@wiking	(and yeah aws instances are super slow :P)
 rcurtin	wiking: I have one machine that we have used in the past
 rcurtin	is that enough for your needs?
 rcurtin	it's not incredibly powerful or fast, just a desktop with an i5 and 16GB RAM
@wiking	rcurtin: yeah i mean if you would like to contribute :P
@wiking	but i dont know if that actually would work
@wiking	having 3 different machines
@wiking	but then again
@wiking	if the (dataset+model) from different libs are running on the same machine
@wiking	it should be fine
 rcurtin	I dunno what you mean, do you mean you want me to give the system to the shogun project?
@wiking	nono
@wiking	i meant that we setup an openstack
@wiking	and run benchmarks on that openstack
@wiking	https://www.openstack.org/
 rcurtin	hang on, I have to step out
@wiking	kk
@wiking	no worries
 rcurtin	let me get back to you later
@wiking	this was just an idea
@wiking	still wondering about stuff
@wiking	how to do the benchmarking
 rcurtin	yeah I just need to do a little reading
 rcurtin	I will get back to you :)
@wiking	because now we more or less moved
@wiking	our stuff on aws
@wiking	so we've got our machines freed up for benchmarks
@wiking	arianepaola: pong pong pooooooooooooong
@wiking	Saurabh7-: yoyoyo
-!- Saurabh7 [~Saurabh7@1.39.96.25] has joined #shogun
-!- sanuj [~sanuj@117.203.19.130] has joined #shogun
@wiking	Saurabh7: ping?
 Saurabh7	wiking, hi !
@wiking	Saurabh7: how's thing going with the thread safetly
@wiking	*safe
@wiking	changes
@wiking	do you need any hadn help?
 Saurabh7	wiking, I am trying to use the features copy constructors for now
 Saurabh7	for DenseFeatures it looks good but i dont knwo how to generalize
@wiking	?
@wiking	copy ctr?
@wiking	for what?
@wiking	i mean how?
@wiking	if you can share a code
@wiking	i can help
@wiking	any code
 Saurabh7	hm
@wiking	just to understand
@wiking	what happens with the code
 Saurabh7	CDenseFeatures feat =features
 Saurabh7	feat->add_subset()
 Saurabh7	something like this
 Saurabh7	when we want to add multiple subsets
@wiking	mmm
@wiking	so feat->add_subset() would return a CDenseFeatures
@wiking	?
 Saurabh7	no it just adds subset to subsetstack
 Saurabh7	feat is copy of features
 lisitsyn	make it immutable!!!
@lambday	Saurabh7: may I ask what are you working on regarding features?
 lisitsyn	:D
@lambday	Saurabh7: you want to create a shallow copy?
 Saurabh7	lambday, yes
@wiking	Saurabh7: yeah as lisitsyn said and we said yesterday
@wiking	immutable
@wiking	so just const
@wiking	Saurabh7: you are about to eliminate CSubsetStack
@wiking	?
 Saurabh7	ok
 Saurabh7	wiking, no
@lambday	Saurabh7: so for dense features, you want that the underlying feature matrix is the same, but the subsets are copied only?
 Saurabh7	just create a copy and add another subset right ?
 Saurabh7	lambday, yes
 Saurabh7	lambday, and adding different new ones
@lambday	Saurabh7: https://github.com/shogun-toolbox/shogun/blob/feature/bigtest/src/shogun/statistical_testing/internals/FeaturesUtil.cpp#L46
@lambday	I added a similar thing here..
@wiking	huh
@wiking	lambday: why did you do this ?
@wiking	:)
@lambday	Saurabh7: maybe this can be of any use?
@wiking	i mean why do we have FeaturesUtil
@wiking	?
@wiking	should have been part of CFeatures
@lambday	wiking: I needed this to create blocks of features (for testing things blockwise) with just different subsets on
@wiking	yeah
@lambday	although the underlying matrix is same
 Saurabh7	lambday, hmm yes
@wiking	but why not part of CFeatures?
@wiking	as a simple static function of it? :)
 Saurabh7	is that only or denseFeatures ?
 Saurabh7	for*
@lambday	wiking: well, I could add it there.. but I just temporarily added it only for dense
@wiking	lambday: yeah as we learned
@wiking	if you put a code temporary
@wiking	it'll be permanent :D
@lambday	wiking: would something like this be useful for CFeatures in general?
@lambday	not sure!
@wiking	well
@wiking	it's part of CFeatures
@wiking	so why not
@lambday	wiking: alright then..
@wiking	i mean if it's only working for
@wiking	DenseFeature
@wiking	just add it there
@wiking	:)
@lambday	alright I'll do that
@wiking	thnx
@lambday	wiking: would we like to have it in develop right now?
@lambday	cause I'm working on this feature branch.. and it's all there for now
@wiking	lambday: it will not break anything or?
@lambday	wiking: hopefully not..
@wiking	aaah
@wiking	if this is not yet in develop
@wiking	then it's fine
@wiking	just move it in your feature branch
@lambday	wiking: so, in CFeatures, we provide this API.. and keep it SGNOTIMPLEMENTED.. and only implement it for DenseFeatures for now..
 Saurabh7	lambday, isnt it similar to this https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/features/DenseFeatures.cpp#L29
@lambday	Saurabh7: I think the other one shares the same subsetstack object as well..
@lambday	Saurabh7: not sure though... can you confirm?
@wiking	lambday: yeah sounds good
@wiking	although
@wiking	i hate SGNOTIMPLEMENTED placeholders :P
@wiking	but yeah i know it would take an effort now
@lambday	wiking: hehe but we can't just run away from it
@HeikoS	jojojo
@wiking	to get that implemented everywhere
@wiking	lambday: = 0
@wiking	and then just implement it every derived class
@wiking	:D
@wiking	Saurabh7: can help in that
@wiking	:>
@wiking	i mean
@wiking	if yo udont mind
@wiking	he could copy this
@wiking	in his branch
@lambday	wiking: yeah but does subset make sense for all the feature subclasses? not sure :/
@wiking	and then you'll just rebase and cherrypick his branch for yours
@wiking	lambday: imho it shouldn't matter of the type
@wiking	i mean why not?
@wiking	any feature set should be slicable
@wiking	in any ways
@wiking	HeikoS: standup: 0/1 ?
@lambday	wiking: alright.. this needs some investigation then :D
@wiking	lambday: which part?
@HeikoS	wiking: I like it, would keep it to the mentor student pair though, otherwise it is hard to track
@HeikoS	so "getting in touch daily" +1
@wiking	HeikoS: doesn't need to be sync
@wiking	it can be async
@lambday	wiking: to check if all the features are subset-able
@wiking	that's why i said just to dump here the stuff
@HeikoS	wiking: good then
@HeikoS	we only have 4
@wiking	and we can look at the logs
@HeikoS	so easy to keep track
@wiking	lambday: well make it
@wiking	:)
@HeikoS	sanuj, Saurabh7 Saurabh7-  jojo
@wiking	i mean this is comptures
@wiking	you can do any fucking thing you want
@wiking	:)
@HeikoS	OXPHOS: jo
@HeikoS	discussing linalg?
 sanuj	hi
@wiking	lambday: just add a pure virtual
 sanuj	HeikoS, i have a question
@wiking	and then worst case where it's not supported
@wiking	you add a not supported
@lambday	wiking: yeah.. gonna take some time though.. but I'll do that
@wiking	time?
@HeikoS	sanuj: shoot
@wiking	give it to Saurabh7
 sanuj	HeikoS, Int is working in cookbook now
 sanuj	HeikoS, but for Java
@wiking	lambday: i mean he needs this
@lambday	wiking: as in, I want to make it work for those it is supposed to work
@wiking	lambday: but why dont you pass this to Saurabh7 ?
@lambday	wiking: oh.. then let me start with this
@lambday	umm yeah that's also an idea
@wiking	he'll make good use of it
 sanuj	HeikoS, Int component_num = 1
@wiking	and he can finetune it for you
 sanuj	HeikoS, is converted to
@lambday	wiking: that sounds cool!
@HeikoS	sanuj: I saw, this is defined throught the json file
 sanuj	HeikoS, json is fine
@lambday	Saurabh7: ^
 sanuj	HeikoS, json has "Int": "int",
@lambday	Saurabh7: can you confirm that the copy ctor actually uses the same underlying stack?
@HeikoS	sanuj: but the json defines the text to text replacement
@HeikoS	if it translates to double, then there is a mistake
@HeikoS	cant really be
 Saurabh7	weee
 Saurabh7	lambday, it should be same right? lemme check
@lambday	Saurabh7: I meant same object..
@lambday	I want to add different subsets to each of these shallow copies
@lambday	without affecting the other
@HeikoS	sanuj: lets talk private
@lambday	and still want to share the same underlying feature matrix
 Saurabh7	lambday, https://github.com/shogun-toolbox/shogun/blob/develop/src/shogun/features/SubsetStack.cpp#L42
 Saurabh7	it createsa copy with same subsets
 Saurabh7	so i think it should be good to add new ones onto it
@lambday	Saurabh7: doesn't it add the same CSubset* objects?
@lambday	Saurabh7: not sure whether that would be a problem
@lambday	Saurabh7: don't worry about this.. if the copy ctor serves your purpose.. go ahead with it
@lambday	Saurabh7: I'll check once again if it serves my purpose..
@lambday	if not, maybe we can add the shallow copy thing .. I'll create an issue
 Saurabh7	lambday,ok i will try to check  with yours too whats the difference
@lambday	Saurabh7: cool! let me know
 Saurabh7	brb
@wiking	btw
@wiking	anybody has idea how we should patch cmake so that it realises that omp is available in clang 3.8?
@wiking	mmm
@wiking	we eliminated the use of lp solve? :)
@wiking	HeikoS: that pr
@wiking	is no goodie
@wiking	:)
@wiking	not every int
@wiking	is a int32_t
@wiking	i mean yes on your machine probable
@wiking	but shogun runs on other things than x86
@HeikoS	wiking: I see
@HeikoS	the point is to have it defined somewhere
@HeikoS	for meta examples
@HeikoS	it needs to be visible in the ctags
@HeikoS	any idea how to get that otherwise?
@HeikoS	lisitsyn:  ^
@wiking	well it's in  stdint.h
@wiking	which is included in common.h
@wiking	mmm HeikoS we really dont use lpsolve anymore
@wiking	anywhere
@wiking	i'll remove the dependency check ok?
@HeikoS	please do
@wiking	i did a git grep USE_LPSOLVE
@wiking	and it's only defined
@wiking	in config.h.in
@wiking	but nowhere is being used
@HeikoS	probably forgotting to be guarded
@HeikoS	maybe grep for something where it might be used
@wiking	mmm tried differnt lp functions
@wiking	but if guarding would be a problem
@wiking	then it would have errored
@wiking	on my machine
@wiking	as it didn't detect lpsolve
@wiking	on my machine
@wiking	still it managed to compile everyting
@wiking	so i think we removed lp solve dependency
@wiking	:D
@wiking	HeikoS: you reckon we wanna have that back ever?
@wiking	woah
@wiking	they just released a new version
@wiking	after 5 years
@wiking	;D
 rcurtin	wiking: so I am still not sure what you mean
 rcurtin	it is definitely possible to run the benchmark system on an openstack machine
 rcurtin	but I am not sure how we could be helpful with that
 rcurtin	I don't have any openstack setup
 rcurtin	all I have are a handful of identical desktops we do benchmarking on
@wiking	ok so what i thought
@wiking	that we join our mahcine forces
@wiking	using openstack
 rcurtin	another consideration is that openstack will give you Vms
 rcurtin	so timings may have high variance
@wiking	yeah but it's kvm
@wiking	so it's barebone
@wiking	machine
@wiking	the only difference could be
@wiking	if you run for example
@wiking	kmeans on one machine
 rcurtin	(on the train and on a phone, slow responses)
@wiking	scikit-learn kmeans on one machine
@wiking	and then mlpack kmeans on another machine
@wiking	but if it's running on the same openstack instance
-!- Saurabh7 [~Saurabh7@1.39.96.25] has quit [Ping timeout: 240 seconds]
@wiking	so if we run 1 algo but all it's different versions (mlpack, shogun, scikit-learn) in the same openstack instance
@wiking	then the timing will be the same
@wiking	i mean it will not vary
@wiking	although
@wiking	and this is the shitty part
@wiking	that we are not only interested in relative diff
@wiking	but absolute diff
@wiking	:(
 rcurtin	but still this is running on a VM and so if any other VMs are running on the same hardware you get slowdown
@wiking	rcurtin: openstack
@wiking	i mean it will not give you more machines
@wiking	than it is avaialble on that node
@wiking	i.e there will not be any race condition for cpu/memory
 rcurtin	yes but what I am saying is when you have more than one Vm on a physical host your timings are no longer accurate
 rcurtin	if you are saying that openstack is one Vm per physical host
 rcurtin	then this is different
@wiking	no
@wiking	but why would it be the timing different
 rcurtin	I am not intricately familiar with openstack
@wiking	i mean if you take
@wiking	n instances on the machine
 arianepaola	using openstack will just add more complexity
@wiking	that fits into the machine
@wiking	they will not cause timing problems
 arianepaola	you can even run docker in openstack on top of vagrant....
@wiking	arianepaola: but i just want to have a way to be able to share resources
@wiking	for that we need some sort of resource managment
 rcurtin	wiking: if you can prove tht variability of runs is not larger than with a single non Vm I will believe you
 arianepaola	every layer brings down performance
@wiking	to know where resource i available if any
 rcurtin	but at the moment I do not
 rcurtin	I think that a better way to collaborate
@wiking	rcurtin: this is not a belief
@wiking	:)
@wiking	i mean this is being used in many places :D
 rcurtin	is to have shared setup scripts or something like this
@wiking	like rackspace
@wiking	but then again
@wiking	it was just an idea
@wiking	if you dont believe in it it's fine
@wiking	i'm not going to spend time to prove something that is know to be working
@wiking	sorry
 rcurtin	wiking: that does not mean variance in timing runs is not higher than with a physical host :)
@wiking	do you know what's kvm?
@wiking	or xen for that matter?
@wiking	either of them
@wiking	will make sure that you are actually running the actual instruction
@wiking	on your cpu
@wiking	it's not vm
 rcurtin	yes
 rcurtin	but you still have limited disk Io
@wiking	and we care about disk io perfomance in case of kmeans?
 rcurtin	if you are benchmarking data load time also then yes
 rcurtin	and that is currently what the benchmark scripts do
@wiking	ok so you cannot separate data loading time
@wiking	from the actual modeling method?
@wiking	because in that case i suppose the best way to beat the benchmarking is
 rcurtin	ah I need to think about that
 rcurtin	there are ways but they can be complex
@wiking	that you write a fully threadable read IO
@wiking	and:)
 rcurtin	so I need to revisit the code and get back to ou
@wiking	no worries
@wiking	i'm just trying to figure out
@wiking	how both of us can benefit
@wiking	because having this benchmarks
@wiking	would be great for all of us
@wiking	not just shogun or mlpack
 rcurtin	yes absolutely
 rcurtin	but we should keep in mind that the sustem is (or should be) easy to deploy and run
@wiking	yeah
@wiking	other option is to use mesos :P
@wiking	and docker images
 rcurtin	so it should not be 100% nevessary to run on the same hardware
@wiking	yep
@wiking	i just want to be able to run this
 rcurtin	docker might be useful here because getting the roght dependencies can be a pain
@wiking	on more than one machine
@wiking	yeah
@wiking	true
@wiking	i have packer.io configs for having the build deps for shogun
@wiking	it took me a good day
 rcurtin	it would be easy to write a dockerfile for this, maybe I will do this sometime
@wiking	to define that for fedora, centos, debian,ubuntu
@wiking	rcurtin: i would highly advise to use packer.io for this
 rcurtin	I have other setup to do with our build server first though :(
@wiking	that way you can generate a docker image
@wiking	as well as aws ami
@wiking	as well as virtualbox image
@wiking	etc.
@wiking	;)
@wiking	so it's a good tool for such things
@wiking	so once you define your packer.io config
@wiking	you are golden
@wiking	for any type of 'instance'
 rcurtin	possibly
 rcurtin	though I will not lie, I have little excitement about learning a new tool :)
@wiking	yeah i can understand
@wiking	but this way you can hit more birds with 1 stone
@wiking	:P
 shogun-buildbot	build #12 of xenial - libshogun is complete: Failure [failed test]  Build details are at http://buildbot.shogun-toolbox.org/builders/xenial%20-%20libshogun/builds/12  blamelist: Viktor Gal <viktor.gal@maeth.com>
 rcurtin	that is true
 shogun-buildbot	build #2874 of bsd1 - libshogun is complete: Failure [failed configure]  Build details are at http://buildbot.shogun-toolbox.org/builders/bsd1%20-%20libshogun/builds/2874  blamelist: Viktor Gal <viktor.gal@maeth.com>
@wiking	rcurtin: do you guys use eigne?
 rcurtin	back to the original question, what can I do to help with openstack?
 rcurtin	no, we use armadillo
@wiking	rcurtin: i'm still investigating
 rcurtin	I don't want to commit to using openstack o produce pur benchmarks
@wiking	i was just wondering if you'd be interested
@wiking	in this type of collaboration
 rcurtin	*to produce our
 rcurtin	but I do want to help
 rcurtin	and if we can share our configurations this I think would be a good way to go
@wiking	k
 rcurtin	the thing to keep in mind is, it is very likely at some point we will disagree on how the benchmarks should be done
 rcurtin	there is already an issue by the elki maintainer to this matter
 rcurtin	hang on
 rcurtin	getting off the train
 rcurtin	will finish the thought shortly
@wiking	:)
@wiking	hehehe ok
-!- leagoetz [~leagoetz@nat-214-50.internal.eduroam.ucl.ac.uk] has joined #shogun
-!- sanuj [~sanuj@117.203.19.130] has quit [Ping timeout: 272 seconds]
 OXPHOS	HeikoS: sry I didn't log out last night
 rcurtin	so as more libraries become inteested in this more people will have opinions and resolving disputes will become intractable
 rcurtin	disputes like "do we include I/O" or "what do we set the parameters to for these datasets"
 OXPHOS	HeikoS wiking: linalg: https://gist.github.com/OXPHOS/0eaa17411e00936f1ed61388ec5c0af6
 rcurtin	because these little questions can make big differences for which library looks best
 rcurtin	so I think we should try to avoid these altogether by taking the approach of "you should set this system up yourself and run it in your own setting!" and this should help avoid a lot of long discussions
 rcurtin	wiking: what do you think?
@wiking	just a sec
@wiking	was doing some other shit
 rcurtin	sure, no hurry.  I am at my office now so I can type a little faster :)
@wiking	heheh
@wiking	yep sounds reasonable
 rcurtin	https://github.com/zoq/benchmarks/issues/9 is an example of what I mean
 rcurtin	it started as a simple request and discussion, then suddenly turned into "you should be doing your k-means benchmarking completely differently"
@wiking	ah fuck
@wiking	ok nerd war
@wiking	:)
 rcurtin	yeah basically that is what it was, I want to avoid more of those where possible :)
 rcurtin	it sounds to me, like maybe what would be most useful might be a packer.io config (I guess I will learn it...) that generates images with the benchmarking system installed?
 rcurtin	this way you can just deploy one of these images on openstack, and I can deploy them on my antiquated desktops and live in the past :)
-!- c4goldsw [5da420e6@gateway/web/cgi-irc/kiwiirc.com/ip.93.164.32.230] has joined #shogun
@lambday	OXPHOS: hello
 rcurtin	I think that at some point I would like to compare the variability of runs on openstack and on my desktop, but that should be pretty easy once the system is set up... just run some benchmark ten times or so over a week or something like this, then compare variance
@lambday	OXPHOS: how's it going?
 rcurtin	and if the variance is very small I will shut up and maybe move to openstack :)
 OXPHOS	lambday Hey! I add something here: https://gist.github.com/OXPHOS/0eaa17411e00936f1ed61388ec5c0af6
 OXPHOS	lambday: I have tested the base-derived class in shogun independent module and I'm now writing dot test case
@lambday	OXPHOS: yeah it would be awesome to have a running demo :)
@wiking	OXPHOS: what happens if you dont have HAVE_VIENNACL ?
@lambday	OXPHOS: BTW I was thinking that the thing that wiking suggested, would have been easily doable if we'd have used CUDA
@wiking	i mean the problem here is that
@lambday	I was wondering whether we can do the same with opencl+viennacl but I am afraid we cannot
@wiking	this thing will be part of the modular fw
@wiking	and that case you dont care HAVE_VIENNACL
@wiking	because either you have it or dont
@wiking	that means if you dont = you dont have the .so for doing that
@wiking	and actually the most interesting part
@wiking	about the gpu backand
@wiking	will we have
@lambday	wiking: without viennacl, there is no way we can define a memory in GPU, that, when viennacl **IS** present, can utilize
@wiking	viennacl_cuda
@wiking	and viennacl_opencl .so?
@lambday	wiking: with CUDA we could have a simply int* gpu_vec...
@wiking	lambday: but you dont get it right?
@wiking	you dont care about that
@lambday	with viennacl it is not possible, cause it uses cl_mem
@wiking	you dont need that ifdef
@wiking	it should be an opaq pointer
@wiking	not used in the header
@wiking	because either you can do shit with it
@wiking	because you have the backend
@wiking	and then you compile
@HeikoS	lisitsyn:  around?
@wiking	or you dont have the backend
@wiking	and then you dont have the .so
@wiking	or you have the .so
@wiking	but it throws back a
@wiking	'fuck i dont have a gpu/opencl compatible shit'
@lambday	wiking: I agree.. this of course should be a pimpl
@wiking	GPUptr
@wiking	should be an opaque pointer
@wiking	that is only actually used
@wiking	in .cpp
@lambday	wiking: agreed!
@wiking	ok i'm back to debugging fucking qemu
@lambday	OXPHOS: you got the idea?
 OXPHOS	lambday: the GPUvector subclass need a opaque pointer?
@lambday	OXPHOS: yep!
@wiking	lambday: and actually
@wiking	think about how we are gonna support
@HeikoS	wiking: got another question
@HeikoS	typedef double float64_t;
@HeikoS	so this is fine?
@wiking	different gpu pointers
@wiking	HeikoS: not at all
@wiking	:)
@HeikoS	it is in common.h
@wiking	HeikoS: then remove it
@wiking	:)
@HeikoS	can you pls check?
@HeikoS	there is more of that kind
@lambday	wiking: actually we're trying to solve a problem that viennacl guys did already.. they already work with (1) cpu memory (2) opencl gpu memory and (3) cuda memory..
@HeikoS	wiking: the reason I touched this is kind of totally unrelated to the sense or nonsense of such definitions
@lambday	their vectors and matrices work with all
@lambday	we're trying to solve the same problem..
@HeikoS	so I kind of want to sidestep getting into that for now, but just make the meta examples work
@wiking	lambday: yep, buuuut how you wanna solve
@wiking	when somebody implements
@HeikoS	so I wonder, shall we put this as an issue, resolve all of them at once at some point?
@wiking	a magma based linalg backend? :)
@wiking	that should be a reasonable idea no? :)
@lambday	wiking: have a different subclass that does the thing..
@wiking	subclass :P
-!- c4goldsw [5da420e6@gateway/web/cgi-irc/kiwiirc.com/ip.93.164.32.230] has quit [Quit: http://www.kiwiirc.com/ - A hand crafted IRC client]
@lambday	wiking: and then have the factory return **that** type when asked for a gpu type
@wiking	i mean of GPUVector?
@wiking	because yeah subclass of gpu backend
@wiking	yes yes
@wiking	but i mean
@wiking	the interface
@wiking	since we want to be opaque like this
@wiking	that GPUBackend
@wiking	= whichever
@wiking	and it'll have the right types
@wiking	HeikoS: looking aroudn
@lambday	wiking: I think subclasses of GPUVectors would have to do then..
@lambday	or maybe, have the impl class use the #ifdef's
@lambday	but then in a platform where multiple of these are available, then we can only use a single one.. so... not good idea!
@wiking	lambday: but no
@wiking	again
@wiking	you dont care if multiples are avaialb
@wiking	e
@wiking	you will have n different .so
@wiking	that implement a gpu backend
@wiking	the user will define it
@wiking	which one you wanna use
@wiking	and in worst case that backend will
@wiking	throw a runtime exception
@wiking	on registration
@wiking	when it checks that actually that shared library
@wiking	can access the right hw or not that it requires to run properly
@wiking	see what i mean?
@wiking	and actually this is why we would like to have a config file now for shoung
@wiking	where you can define these globals
@wiking	like: where are all the shared libraries
@wiking	or what's the gpu/cpu backend you wanna use
@wiking	etc
@wiking	which of course you can override at runtime
@wiking	just having a default via a conf would be good
@lambday	wiking: can you please clarify what we're supposed to do in registration?
@lambday	does it sort of specify that -- this is the *.so that we wanna use
@wiking	yes
@wiking	this is the .so you will use
@wiking	and that should be loaded
@lambday	(shogun_gpu_impl_XXX.so) [XXX=viennacl]
@lambday	and so on
@lambday	alright..
@wiking	yeah
@wiking	and then you dlopen that
@wiking	register it in your linalg backend
@wiking	etc
@wiking	i mean you can do this
@HeikoS	wiking: whats your thoughts on common.h?
@wiking	while you do init_shogun()
@wiking	because we have that shit anyways
@wiking	:)
@wiking	HeikoS: yeah i'm looking
@wiking	HeikoS: about the float
@wiking	the ctags i dont know
@wiking	but dont do what you did in your pr
@HeikoS	yeah sure
@wiking	because that's not true
@HeikoS	looking for a better way
@lambday	wiking: alright.. so about implementing it this way, we got to have different subclasses of GPUVectors I assume.. the ones that uses viennacl will be compiled and dumped into that shogun_gpu_impl_viennacl.so and so on
@lambday	and for that, we have to keep the GPUvector generic.. no impl dependency
@lambday	wiking: is that what you mea
@lambday	mean*
@wiking	lambday: why do you have to have different subclass?
@wiking	you can just have different implementation of it
@lambday	wiking: okay I see what you mean
@wiking	because if you use GPUVector of viennacl backend
@wiking	that linalg backend will implement the gpu 'interface'
@wiking	and the backend will know what the opaque pointer actually stands for
@wiking	right?
@lambday	wiking: yeah
@wiking	but yeah each gpu backend should have it's own gpuvector
@wiking	and these are not interchangable
@lambday	wiking: yeah and that's what the different impls will have to take care of
@wiking	but the interface (header) should be the same
@wiking	yep yep
@lambday	wiking: yeah I totally agree
@lambday	cool!
@lambday	OXPHOS: ^ :)
 OXPHOS	lambday: trying to understand
@lambday	OXPHOS: see the main thing here is to develop this architecture..
@lambday	and dot is our lab-rat
@lambday	OXPHOS: so the idea is - GPUVector should be more of an interface of how we communicate with gpu vectors of different impls
 OXPHOS	lambday: it seems the idea of *GPUbackend in linalg class can stays the same - and the GPUvector will be like GPUbackend
@wiking	lambday: my only concern is a bit that branch in the dot() function
@wiking	:()
@lambday	wiking: what do you mean?
@wiking	well you know that part
 OXPHOS	lambday: I was thinking users can specify what GPUbackend they want to use by passing different instance of GPUbackend
 OXPHOS	lambday: but realized now GPUbackend ONLY works for ViennaCL
@wiking	where you check the SGVectors type,.... where it is stored
@wiking	lambday: it might be a good idea to have for the dot function
@wiking	a flag
@wiking	like
@wiking	dot(vectorA, vectorB, type)
@wiking	where type is
@wiking	= AUTO
@wiking	= GPU
@wiking	= CPU
@wiking	and by default the type is AUTO
@wiking	this way you can avoid that branching in the code
@wiking	i mean i know it's just couple of instructions
@wiking	but we were bithching about the overhead of virtual
@wiking	:P
@lambday	wiking: actually I thought of handling it via virtuals yesterday :D
@lambday	wiking: by a way that the vector impls *know* how to do dot on themselves
@lambday	but then we won't need the linalg::dot(...) method at all..
@lambday	because one can always do
@lambday	auto a = factory::getgpuinstance(a)
@lambday	a->dot(b)
@lambday	where a is the base class of the vector type
@wiking	yep
@lambday	a is *of
@lambday	wiking: that eliminates the branching
@lambday	but the downside is, all linalg operations are member methods now
@lambday	kinda like the linearoperator, linearsolver and eigensolver interface we have
@lambday	wiking: if having it this way is okay API-wise, then we can have this instead
@lambday	I mean, eigen does it anyway
@lambday	a.dot(b)
@lambday	m.doshit(a)
@lambday	wiking: what do you think? ^^
@wiking	just a sec
@wiking	trying to figure out why the hell qemu is shit on jessie
@lambday	haha
 OXPHOS	lambday: how can we have different implement in the same GPUVector(class)? (Just generally asking)
@lambday	OXPHOS: we have have that using pimpls
@lambday	OXPHOS: say, for example, for viennacl type of vectors, we need an array of vcl::opencl::mem_handle to store the vector
@lambday	OXPHOS: and for CUDA, we store it with simple T*
@lambday	but in the header, we don't declare shit
@lambday	:D
@lambday	we just say, here, have a pointer to an impl.. which knows how to do all those stuff
@lambday	and then we define the impl in the cpp, not in *.h
@lambday	so.. in the cpp, based on different flags, we write code..
 OXPHOS	so we have a pointer unique_ptr<GPUV> pt, but GPUV can be different based on flags in GPUV .cpp
@lambday	when we compile, we can have different *.o based on those flags
@lambday	OXPHOS: precisely :)
 OXPHOS	lambday: so we still need flags - maybe for the whole class - thought about sth fancier :P
@lambday	OXPHOS: yes but we push the flags to the cpp
@lambday	OXPHOS: the header is flag free
 OXPHOS	lambday: got it
@lambday	and, inside the header, we don't have to say how to store it.. because the storage is different
@lambday	so that solves the problem :)
 OXPHOS	lambday: and the same idea for the GPUBackend class?
@lambday	OXPHOS: let me check what we had in there
@lambday	OXPHOS: actually, this class can just have a bunch of static methods I think
@lambday	umm but yeah maybe we need the pimpl there also
 OXPHOS	lambday: yes seems it can ONLY support one type of GPU module
@lambday	I mean, we won't implement anything in the header anyway
@lambday	yeah this would need some polishing
@lambday	but let me hear wiking on having it the other way we discussed yesterday... that is, the linalg operations are a part of the vector/matrix classes thmselves..
 OXPHOS	lambday: sure
@lambday	a->dot(b) instead of dot(a, b)
@lambday	I am trying to think of operations where this type of API can pose problems
@lambday	we already have eigensolver and linear solver that works with this type of interface
 OXPHOS	it looks ugly..
 OXPHOS	but nevermind :)
@lambday	OXPHOS: haha the a.dot(b) thing?
 OXPHOS	lambday: yeah..[feels like] all methods are in SGVector class. But since a, b here are specifically designed for linalg, I guess it's fine.. XD
@lambday	yeah a and b are not SGVector/SGMatrix at all
@lambday	but yeah i see what you mean
-!- sanuj [~sanuj@117.203.19.130] has joined #shogun
@lambday	we just want to see whether the if-else in every linalg call is a good idea or not..
@lambday	cause if we have it this way, then we save the branching
 shogun-buildbot	build #247 of deb1 - libshogun - PR is complete: Failure [failed git]  Build details are at http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun%20-%20PR/builds/247  blamelist: karlnapf
@HeikoS	sanuj: I think I ixed it
@HeikoS	sanuj: see my pr
 sanuj	okay
 sanuj	checking
@HeikoS	sanuj: I now forbid the use of "Int" "Real", but rahter use "int" "real"
@HeikoS	these are basic types and no include path is checked for them
@HeikoS	so no problems with ctags
@HeikoS	sanuj: waiting for travis
 sanuj	HeikoS, how does it find stuff if you include nothing?
@wiking	HeikoS: dunno wtf is happening with git
@wiking	fatal: Could not parse object 'f5adee722739e5a892ee968bc51461c5b907236a'.
@wiking	fatal: Couldn't find remote ref refs/pull/3205/head
 shogun-buildbot	build #248 of deb1 - libshogun - PR is complete: Success [build successful]  Build details are at http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun%20-%20PR/builds/248
 sanuj	lisitsyn, where to add 'any' in shogun?
 lisitsyn	sanuj: lib
 sanuj	cool
@HeikoS	wiking: yeah git is delayed
@HeikoS	wiking: when I pushedto my fork
@HeikoS	it didnt show up on the page for a few mins
@HeikoS	thats why I closed and re-opened old PR
@HeikoS	sanuj: sorry what?
@HeikoS	sanuj: I now just treat basic types as such, so no include lookup is being done in c++
 sanuj	i see
 sanuj	HeikoS, i'll update my pr once you merge yours
@HeikoS	sanuj: had to change something and restart travis
@HeikoS	but should do it now
@HeikoS	sanuj: only thing you need to change is to use lower case basic type names
 sanuj	yes and rebase
@HeikoS	yep exactly
@HeikoS	sanuj: how are things going with the tags?
 sanuj	shifting them into shogun, couldn't do much
 sanuj	doing it now
 sanuj	and i'm reading sergey's tags implementation from aer
 sanuj	it has better code
 lisitsyn	it has the best code ever
 lisitsyn	:D
 lisitsyn	because it is written by the boss
 lisitsyn	:D
 sanuj	lisitsyn, ;)
@HeikoS	lisitsyn: ^
@HeikoS	lisitsyn!
@HeikoS	lisitsyn dance
@HeikoS	mmh
@HeikoS	doesnt work
@HeikoS	;)
 sanuj	hahaha
@HeikoS	thought you might say sorry
 lisitsyn	sorry
 lisitsyn	autosorry
 sanuj	lisitsyn, shall i put Tag also in lib?
 lisitsyn	autosorry
 lisitsyn	sanuj: yeah probably
 sanuj	cool
 lisitsyn	the difference between base and lib is vague
 lisitsyn	but probably it's lib
 sanuj	okay
 sanuj	lisitsyn, any is in aer::impl::any
 sanuj	any reason for impl?
 lisitsyn	no not really
 sanuj	shall i just keep shogun::any
 lisitsyn	yeap
 sanuj	oh actually its a bit different
 sanuj	the policies are in impl
 sanuj	any is in shogun itself
 sanuj	so no problem
 lisitsyn	yeah feel free to flatten namespaces
 sanuj	lisitsyn, and the comment stuff that is there in every code file
 sanuj	the licensing part
 lisitsyn	what comment stuff?
 sanuj	and written copyright etc
 lisitsyn	what license is it?
 lisitsyn	bsd?
 sanuj	other shogun code has gpl
 lisitsyn	keep it bsd
 sanuj	okay
 sanuj	HeikoS, build is failing
 sanuj	https://travis-ci.org/shogun-toolbox/shogun/builds/132882156
@HeikoS	checking
 sanuj	knn failed
 sanuj	the integration tests
 sanuj	HeikoS,  we'll have to change the data
@HeikoS	I think thats it
@HeikoS	might have to change more than that
@HeikoS	since I changed types
@HeikoS	checking locally
@HeikoS	make test worked for me a bit earlier
 sanuj	cool
@HeikoS	yeah its the data
 sanuj	fine then
@HeikoS	sanuj: updated
@HeikoS	the others should also fail
@HeikoS	though maybe not
@HeikoS	no "Int" definitions were in them
@HeikoS	but lets wait for travis
 sanuj	HeikoS, others didn't fail last time
@HeikoS	better wait, you never know what might go wrong :)
@HeikoS	dont wanna break the build in gsoc
 sanuj	correct :)
 sanuj	haha
@HeikoS	annoying for everyone
-!- besser82 [~besser82@fedora/besser82] has joined #shogun
-!- mode/#shogun [+o besser82] by ChanServ
 sanuj	HeikoS, for BSD what comment do i need to add in the beginning of a new code file
@wiking	sanuj: check this file: src/shogun/ensemble/CombinationRule.h
@wiking	as an example
 sanuj	okay
@HeikoS	https://gist.github.com/karlnapf/24763634d5142f92f5213d282cf67e0f
@HeikoS	add you name
@HeikoS	^ this one has encoding, as suggested by besser82
@wiking	ah yeah
@wiking	that one was actually a shitty one
@wiking	that was gpl
@wiking	sanuj: sorry
 sanuj	okay thanks
 sanuj	i'm adding sergey's name too
 sanuj	lisitsyn, what email
-!- leagoetz [~leagoetz@nat-214-50.internal.eduroam.ucl.ac.uk] has quit []
 lisitsyn	sanuj: lisitsyn.s.o@gmail.com
@HeikoS	sanuj:  merged
 sanuj	okay
 sanuj	i'll rebase and update
 shogun-buildbot	build #3745 of deb1 - libshogun is complete: Failure [failed compile]  Build details are at http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun/builds/3745  blamelist: Heiko Strathmann <heiko.strathmann@gmail.com>
 shogun-buildbot	build #661 of trusty - libshogun - viennacl is complete: Failure [failed test]  Build details are at http://buildbot.shogun-toolbox.org/builders/trusty%20-%20libshogun%20-%20viennacl/builds/661  blamelist: Heiko Strathmann <heiko.strathmann@gmail.com>
@HeikoS	gnaaa
@HeikoS	build broken
 shogun-buildbot	build #662 of trusty - libshogun - viennacl is complete: Success [build successful]  Build details are at http://buildbot.shogun-toolbox.org/builders/trusty%20-%20libshogun%20-%20viennacl/builds/662
@HeikoS	deb1 - libshogun wasnt broken by my commit actually
-!- besser82 [~besser82@fedora/besser82] has quit [Ping timeout: 260 seconds]
@HeikoS	shogun-buildbot: force build --branch=develop 'deb1 - libshogun'
 shogun-buildbot	build forced [ETA 12m53s]
 shogun-buildbot	I'll give a shout when the build finishes
 shogun-buildbot	build #3746 of deb1 - libshogun is complete: Success [build successful]  Build details are at http://buildbot.shogun-toolbox.org/builders/deb1%20-%20libshogun/builds/3746
-!- besser82 [~besser82@fedora/besser82] has joined #shogun
-!- mode/#shogun [+o besser82] by ChanServ
-!- sanuj [~sanuj@117.203.19.130] has quit [Ping timeout: 240 seconds]
-!- HeikoS [~heiko@nat-206-247.internal.eduroam.ucl.ac.uk] has quit [Quit: Leaving.]
-!- besser82 [~besser82@fedora/besser82] has quit [Ping timeout: 252 seconds]
-!- besser82 [~besser82@fedora/besser82] has joined #shogun
-!- mode/#shogun [+o besser82] by ChanServ
-!- lambday [8028b10a@gateway/web/freenode/ip.128.40.177.10] has quit [Ping timeout: 250 seconds]
@wiking	\o/ we have a f23 aarch64 bot
 arianepaola	:-)
 arianepaola	wiking: I would like to try to integrate the nightly rpm build into that bot
@wiking	ok
@wiking	what do you need from me/
@wiking	we have 2 fedora bots
@wiking	one is f22 the other is f23
@wiking	and now this aarch64 f23
 arianepaola	wiking: if you have a base config, I can merge the snippet that I posted to the fedora issue
@wiking	oh those steps
@wiking	i can try to add
@wiking	can you give me the pr
@wiking	link
-!- shogun-buildbot [~shogun-bu@7nn.de] has quit [Quit: buildmaster reconfigured: bot disconnecting]
-!- shogun-buildbot [~shogun-bu@7nn.de] has joined #shogun
@wiking	shogun-buildbot: force build --branch=develop 'FC23 - libshogun - aarch64'
 shogun-buildbot	build #0 forced
 shogun-buildbot	I'll give a shout when the build finishes
@wiking	lets see
 arianepaola	wiking: https://github.com/shogun-toolbox/shogun/issues/3131
@wiking	okey
@wiking	lemme see
 shogun-buildbot	build #0 of FC23 - libshogun - aarch64 is complete: Failure [failed compile]  Build details are at http://buildbot.shogun-toolbox.org/builders/FC23%20-%20libshogun%20-%20aarch64/builds/0
-!- besser82 [~besser82@fedora/besser82] has quit [Ping timeout: 264 seconds]
-!- besser82 [~besser82@fedora/besser82] has joined #shogun
-!- mode/#shogun [+o besser82] by ChanServ
-!- thoralf [~thoralf@ip5b4189cf.dynamic.kabel-deutschland.de] has joined #shogun
-!- mode/#shogun [+o thoralf] by ChanServ
@thoralf	Hey *
@wiking	ho
 shogun-buildbot	build #13 of xenial - libshogun is complete: Failure [failed test]  Build details are at http://buildbot.shogun-toolbox.org/builders/xenial%20-%20libshogun/builds/13  blamelist: Viktor Gal <viktor.gal@maeth.com>
 shogun-buildbot	build #2875 of bsd1 - libshogun is complete: Failure [failed configure]  Build details are at http://buildbot.shogun-toolbox.org/builders/bsd1%20-%20libshogun/builds/2875  blamelist: Viktor Gal <viktor.gal@maeth.com>
 arianepaola	bye everyone, see you all tomorrow
-!- travis-ci [~travis-ci@ec2-54-166-247-85.compute-1.amazonaws.com] has joined #shogun
 travis-ci	it's Heiko Strathmann's turn to pay the next round of drinks for the massacre he caused in shogun-toolbox/shogun: https://travis-ci.org/shogun-toolbox/shogun/builds/132913670
-!- travis-ci [~travis-ci@ec2-54-166-247-85.compute-1.amazonaws.com] has left #shogun []
--- Log closed Thu May 26 00:00:12 2016
