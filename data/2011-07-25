--- Log opened Mon Jul 25 00:00:12 2011
--- Day changed Mon Jul 25 2011
 blackburn	sonney2k: libsvm doesn't work neither in python nor java
 blackburn	SystemError: [ERROR] assertion kernel->get_num_vec_lhs()==problem.l failed in file classifier/svm/LibSVM.cpp line 85
@sonney2k	I don't understand
@sonney2k	you said the minimal example worked?
@sonney2k	that should be using libsvm too
 blackburn	minimal example yes, classifier_libsvm_modular.py - not
@sonney2k	but where is the difference?
@sonney2k	they both do the same
 blackburn	I would fix that already if I knew
@sonney2k	blackburn, I suspect that the number of labels doesn't match the matrix
@sonney2k	does python work?
@sonney2k	or java?
@sonney2k	or none?
 blackburn	none
 blackburn	ah yes
 blackburn	my fauld
 blackburn	fault*
@sonney2k	btw the minimal example in java cannot work
@sonney2k	forget what I just said
@sonney2k	blackburn, your fault?
@sonney2k	what means ah yes?
 blackburn	number of labels wrong
 blackburn	sonney2k: why minimal example in java cannot work?
@sonney2k	ok
@sonney2k	did you read what I said above?
 blackburn	yes I did
@sonney2k	<sonney2k> btw the minimal example in java cannot work
@sonney2k	<sonney2k> forget what I just said
@sonney2k	:D
 blackburn	sonney2k: libsvm produces the same results
@sonney2k	blackburn, as in same labels.get_labels() ?
 blackburn	blackburn@blackburn-laptop:~/shogun/shogun/examples/undocumented/java_modular$ ./check.sh classifier_libsvm_modular.java
 blackburn	[0.1938791717197525, 0.19659259940936621]
 blackburn	blackburn@blackburn-laptop:~/shogun/shogun/examples/undocumented/python_modular$ python classifier_libsvm_modular.py
 blackburn	LibSVM
 blackburn	[ 0.19387917  0.1965926 ]
@sonney2k	wait
@sonney2k	2 outputs only?
@sonney2k	we should have 92
 blackburn	yes, I modified data
@sonney2k	ok
 blackburn	I can't check 92 numbers for equality
@sonney2k	better check for the whole sample
 blackburn	takes more time
@sonney2k	just print them and do a diff
@sonney2k	yes I understand but it is impossible to have 92 numbers to match by chance
 blackburn	okay okay
@sonney2k	ohh dam'd I am compiling shogun on debian unstable
@sonney2k	lots of new warnings....
 blackburn	?
@sonney2k	I think I should start preparing a debian package
@sonney2k	for the new thing
 blackburn	sonney2k: the same for 92
@sonney2k	hurray!
 blackburn	ok, i'm pretty tired with java today
@sonney2k	blackburn, what did you expect?
@sonney2k	I think it ran rather smoothly
 blackburn	whaT?
@sonney2k	it basically worked out of the box
 blackburn	can't understand what is you talking about
@sonney2k	didn't you only have to modify load.py / Load.java?
@sonney2k	no bugs in typemaps (so far)
@sonney2k	?
 blackburn	yes
@sonney2k	so yes - that is trivial compared to typemap bugs
 blackburn	it is
@sonney2k	so we are lucky
@sonney2k	and chances are that other examples will just work
 blackburn	I saw some strange outputs in some of them
 blackburn	NaN or so
 blackburn	tomorrow will take a look
@sonney2k	I mean now it remains to test if string based typemaps work
@sonney2k	e.g. stringfeatures
@sonney2k	weighteddegreestringkernel
@sonney2k	and then some really complex example with preprocessors attached or multiple kernels
@sonney2k	if something big works then the rest is just minor issues
@sonney2k	blackburn, I really think we need someone doing a tutorial with some nice data set
@sonney2k	I mean like we have a certain data set and no idea about it
@sonney2k	so we do pca or so first
@sonney2k	and visualize it
@sonney2k	then we do some classification or so
 blackburn	with as much methods used as it could be?
@sonney2k	yeah
@sonney2k	like some story line from very explorative unsupervised
@sonney2k	to simple supervised
@sonney2k	e.g. linear
@sonney2k	then e.g. svm w/ kernels
@sonney2k	and then maybe even multiple kernels / data sources
 blackburn	good idea
@sonney2k	that could work for anything, 2-class classification, regression, multiclass
@sonney2k	would be cool to use heikos x-validation on top already for that
 blackburn	I would say I can do it if I wasn't embarassed with my manifold learning algos
@sonney2k	blackburn, don't worry at some point we will have shogun 1.0 and then we might have time to work on some nice applications too :)
@sonney2k	anyway
@sonney2k	going to bed now
@sonney2k	cu
 blackburn	see you
-!- blackburn [~blackburn@188.122.239.253] has quit [Ping timeout: 255 seconds]
-!- f-x [~user@117.192.199.217] has joined #shogun
-!- f-x_ [fx@213.155.190.134] has joined #shogun
-!- f-x [~user@117.192.199.217] has quit [Ping timeout: 260 seconds]
-!- in3xes [~in3xes@180.149.49.230] has quit [Quit: Leaving]
-!- gsomix [~gsomix@178.45.88.77] has joined #shogun
-!- [1]warpy [~warpy@bzq-79-181-43-167.red.bezeqint.net] has quit [Quit:  HydraIRC -> http://www.hydrairc.com <- Like it?  Visit #hydrairc on EFNet]
-!- gsomix [~gsomix@178.45.88.77] has quit [Read error: Connection reset by peer]
-!- f-x [~user@117.192.204.84] has joined #shogun
-!- sploving1 [~sploving@124.16.139.134] has joined #shogun
@sonney2k	sploving1, could you please test on the kernel example first?
 sploving1	I tested all of them
@sonney2k	and?
 sploving1	there is no same result!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 sploving1	why??
@sonney2k	why what?
@sonney2k	when you just set features and get the features
 sploving1	the result are not same(python , lua)
@sonney2k	are they the same as in python?
@sonney2k	sploving1, yes you said that already - now we need to debug why not.
 sploving1	sonne2y, why classifier_averaged_perceptron_modular.py run twice not the same?
 sploving1	sonney2k
@sonney2k	they were not the same in (python,java) either. but now they are at least for some examples
@sonney2k	sploving1, please first try a simpler example like just setting features / getting features
 sploving1	you can give me the name of example, and I will try. and I do not know what setting/getting features mean
@sonney2k	sploving1, I see
@sonney2k	lets try features_simple_real_modular.py
 sploving1	works well~
 sploving1	sonney2k, I mean in python it works well
@sonney2k	sploving1, ok - now compare if that works in lua too
 sploving1	sonney2k, can we compile them both
 sploving1	I mean in configure, we need compile them both.
@sonney2k	sploving1, yes, just configure python_modular,lua_modular
 sploving1	./configure --interfaces=python_modular,lua_modular
@sonney2k	but when you already installed you don't need to
@sonney2k	,
@sonney2k	between yes
 sploving1	okay that is good
@sonney2k	sploving1, I am looking into the averaged perceptron issue
@sonney2k	(now)
@sonney2k	it is a different problem - it seems
 sploving1	yeap. I do not need
 sploving1	1 4 0
 sploving1	0 0 9
 sploving1	0 0 0
 sploving1	0 5 0
 sploving1	0 0 6
 sploving1	9 9 9
 sploving1	this is lua result
 sploving1	[[ 1.  2.  3.]
 sploving1	 [ 4.  0.  0.]
 sploving1	 [ 0.  0.  0.]
 sploving1	 [ 0.  5.  0.]
 sploving1	 [ 0.  0.  6.]
 sploving1	 [ 9.  9.  9.]]
-!- sploving1 was kicked from #shogun by bettyboo [flood]
-!- sploving1 [~sploving@124.16.139.134] has joined #shogun
 sploving1	[[ 1.  2.  3.]
 sploving1	 [ 4.  0.  0.]
 sploving1	 [ 0.  0.  0.]
 sploving1	 [ 0.  5.  0.]
 sploving1	 [ 0.  0.  6.]
 sploving1	 [ 9.  9.  9.]]
-!- sploving1 was kicked from #shogun by bettyboo [flood]
-!- sploving1 [~sploving@124.16.139.134] has joined #shogun
 sploving1	[[ 1.  2.  3.]
 sploving1	 [ 4.  0.  0.]
 sploving1	 
 sploving1	 [ 0.  0.  0.]
 sploving1	 [ 0.  5.  0.]
 sploving1	 [ 0.  0.  6.]
 sploving1	 [ 9.  9.  9.]]
 sploving1	this is python result
 sploving1	sonney2k, which is correct?
@sonney2k	sploving1, what is the original input?
 sploving1	a=RealFeatures(A), a.set_feature_vector(array([1,4,0,0,0,9], dtype=float64), 0) will affect a.get_feature_matrix()??
 sploving1	matrix=array([[1,2,3],[4,0,0],[0,0,0],[0,5,0],[0,0,6],[9,9,9]], dtype=float64)
 sploving1	this  is the original input
 sploving1	sonney2k, in lua it is : matrix = {{1,2,3},{4,0,0},{0,0,0},{0,5,0},{0,0,6},{9,9,9}}
-!- f-x [~user@117.192.204.84] has quit [Ping timeout: 260 seconds]
 sploving1	as I do not know features' meaning, I have no idea which result is correct
@sonney2k	sploving1, did you do the set_feature_vector in lua too?
 sploving1	sonney2k, yeap . a:set_feature_vector({1,4,0,0,0,9}, 0)
@sonney2k	sploving1, I am a bit lost - please comment the set_feature_vector in both languages
@sonney2k	and then just show what you get in python (first) and then lua
@sonney2k	sploving1, maybe use gist.github.com for pasting...
@sonney2k	or /query me
 sploving1	https://gist.github.com/1103717
 sploving1	sonney2k, take a look at it!
@sonney2k	the python one is correct
 sploving1	you mean set_feature_vector has no effect on the result? sonney2k??
 sploving1	sonney2k, then why set_features_vector effect the result in lua?? so strange!
@sonney2k	sploving1, it should not yes
@sonney2k	sploving1, please don't do set_feature_vector for now and check
@sonney2k	it is probably wrong in lua nevertheless
@sonney2k	(just result transposed - I guess different order in typemap)
 sploving1	sonne2k, without it(set_..), it is the correct result
 sploving1	sonney2k
 sploving1	it is the origin input
 sploving1	the same with
 sploving1	different order? sonney2k, can you expain it more detail?
 sploving1	so I can fix it
@sonney2k	sploving1, yes that can happen when both set_feature_matrix and get_feature_matrix use a different ordering
@sonney2k	sploving1, when you uncomment
@sonney2k	    print a.get_num_vectors()
@sonney2k	    print a.get_num_features()
@sonney2k	what do these display in lua?
@sonney2k	sploving1, in the end i suspect that just this array[i * cols + j] statement in lua is wrong
 sploving1	3 6
@sonney2k	that is correct
@sonney2k	sploving1, if you write  matrix = {{1,2,3},{4,0,0},{0,0,0},{0,5,0},{0,0,6},{9,9,9}} how many tables are these?
@sonney2k	6 right?
@sonney2k	so that should match rows
@sonney2k	and cols is 3 since each table has 3 elements
 sploving1	6 yeap
-!- f-x [~user@117.192.204.152] has joined #shogun
 sploving1	6 is rows, 3 is cols
@sonney2k	sploving1, does that match the meaning you have in swig_typemaps.i?
 sploving1	yeap. now I understand set_feature_vector mean
 sploving1	it is to set the first columen
@sonney2k	yes
 sploving1	I will take a look at the file and fix it
@sonney2k	sploving1, but please not that the vector set function is correct
@sonney2k	it must be the in /out typemap for SGMatrix that are *both* wrong
 sploving1	you mean SGVector is correct and SGMatrix is wrong?
@sonney2k	yes
 sploving1	sonney2k, okay
@sonney2k	I think it should be array[j * rows + i] in line 156 in swig_typemaps.i
@sonney2k	and same indexing in line 176
@sonney2k	sploving1, ^
@sonney2k	then it should work
 sploving1	I fixed it and now recompiling
 sploving1	I thought shogun store in rows first, but that is wrong
 sploving1	shogun store in colmn first then rows, sonney2k
@sonney2k	yes it is always column by column
@sonney2k	like fortran, matlab, r, octave, ...
-!- blackburn [~blackburn@188.122.239.253] has joined #shogun
@sonney2k	... but not python :)
 sploving1	oh. I know
@sonney2k	(in python numpy one can specify that one wants fortran order - so it works there too :)
 sploving1	sonney2k, now I want to support rubby narray
@sonney2k	sploving1, does it work now?
@sonney2k	I mean lua matrix?
 sploving1	just compiling
 sploving1	I fetch upstream
 sploving1	so I git clean and compiling from new fresh
@sonney2k	ok
 sploving1	I just see narray examples,but no api
 sploving1	like numpy
@sonney2k	serialhex, in case you are around again ping us
@sonney2k	sploving1, I would do the same kind of low-level support that you did for lua
 sploving1	sonne2k, ?
 sploving1	sonney2k
 sploving1	what do you mean?
@sonney2k	sploving1, just using arrays
-!- sploving1 [~sploving@124.16.139.134] has quit [Remote host closed the connection]
@sonney2k	?
@sonney2k	hmmhh it looks like narray is still being developed
@sonney2k	so it is probably worth supporting
@sonney2k	and the api is in narray.h
-!- sploving1 [~sploving@124.16.139.194] has joined #shogun
 sploving1	sonney2k,/../../src/interfaces/lua_modular/modshogun.so: undefined symbol: _ZN6shogun4CGMM10train_smemEiidid
 sploving1	my machine crashed just now. I need reboot and just run make(cannot run other application) to compile shogun
 sploving1	but I met the  problem: ua: error loading module 'modshogun' from file '../../../src/interfaces/lua_modular/modshogun.so':
 sploving1	../../../src/interfaces/lua_modular/modshogun.so: undefined symbol: _ZN6shogun4CGMM10train_smemEiidid
@sonney2k	sploving1, yes it needs 1.5 G to compile
 sploving1	I have git clean -dfx and configure it with lua/python modular
@sonney2k	sploving1, I am doing now too
@sonney2k	sploving1, btw you can use narray.h for the api of ruby's narray
 sploving1	oh. i hope there is a api doc
@sonney2k	sploving1, I couldn't find any - but the .h does contain the needed info
@sonney2k	e.g. IsNArray() to test if the obj. is an narray
 sploving1	okay. I will take a look at it
@sonney2k	and there is RNArray with the data
 sploving1	if it is similar to python, that maybe not difficult
 sploving1	numpy, i mean
@sonney2k	it is definitely similar and not beautiful :)
@sonney2k	which example did not work?
@sonney2k	sploving1, ^
 sploving1	feature
 sploving1	features_simple_real_modular.lua, sonney2k
 sploving1	does fresh shogun work well in your computer??
@sonney2k	I just did git clean -dfx and recompiled
@sonney2k	it works....
@sonney2k	how do you run the example?
 sploving1	export LUA_PATH=../../../src/interfaces/lua_modular/?.lua\;?.lua
 sploving1	export LUA_CPATH=../../../src/interfaces/lua_modular/?.so
 sploving1	then lua features_simple_real_modular.lua
@sonney2k	(I ran ./check.sh)
@sonney2k	yes that works too
 sploving1	oh. I wll compile it again
-!- sploving1 [~sploving@124.16.139.194] has left #shogun []
-!- warpyyy [~theuser@212.179.28.34] has joined #shogun
@sonney2k	blackburn, does current master compile and run for you?
 blackburn	min
-!- warpyyy [~theuser@212.179.28.34] has quit [Read error: Connection reset by peer]
 blackburn	sonney2k: yes, all ok, interfaces=java_modular
@sonney2k	ok then it must be sth on splovings side
* sonney2k is transitioning CLabels for SGVector
 CIA-87	shogun: Soeren Sonnenburg master * re9d4632 / src/interfaces/lua_modular/swig_typemaps.i :
 CIA-87	shogun: Merge pull request #232 from sploving/master
 CIA-87	shogun: fix matrix typemap(columns first then rows) - https://github.com/shogun-toolbox/shogun/commit/e9d463232d97ebb290e9db25ae31905469c22acf
 CIA-87	shogun: Baozeng Ding master * rad8130a / src/interfaces/lua_modular/swig_typemaps.i : fix matrix typemap(columns first then rows) - https://github.com/shogun-toolbox/shogun/commit/ad8130a578f008d729f4f2525b83631b52ba4620
-!- sploving1 [~sploving@124.16.139.194] has joined #shogun
 sploving1	now lua features_simple_real works!
 sploving1	sonney2k, do you know why classifier_averaged_perceptron_modular.py reduce different results?
@sonney2k	sploving1, and kernel too?
@sonney2k	sploving1, I am working on that perceptron issue
@sonney2k	since that issue appears in python too - it must be some general problem
 sploving1	I am tring kernel now
 sploving1	kernel_gaussian_modular, https://gist.github.com/1103871
 sploving1	sonney2k, not the same. lua generate so long result!!!!
@sonney2k	sploving1, could you please test km_train first?
 sploving1	okay
@sonney2k	it should be as big as number of columns
@sonney2k	times number of columns
@sonney2k	IIRC 92x92
 sploving1	lua: km_train: 92*8
 sploving1	sonney2k, how to print python??
 sploving1	it has ... in the result
@sonney2k	you mean print(x) ?
 sploving1	yeap
@sonney2k	repr()
@sonney2k	x.repr()
@sonney2k	sploving1, how can km_train for lua be 92x8 ?
@sonney2k	not possible...
 sploving1	sonney2k,  'numpy.ndarray' object has no attribute 'repr'
 sploving1	km_train.repr()??
 sploving1	sonney2k, what it should be? 92*92?
@sonney2k	repr(km_train)
@sonney2k	yes
 sploving1	sonney2k, https://gist.github.com/1103871, the python output still cannot dump using repr. it has ... output
 sploving1	I mean it omit some results
 sploving1	using repr, or print
@sonney2k	sploving1, then use numpy.savetxt('somefilename', km_train)
@sonney2k	but that is not the actual problem...
@sonney2k	btw the lua matrix is 92x92 here too
 sploving1	https://gist.github.com/1103887
 sploving1	this is the result
 sploving1	92* 92? how dou you know that??
 sploving1	I just count rows = 92, cols = 8 about, sonney2k
@sonney2k	nope 92x92
@sonney2k	and gives same result as in python btw
 sploving1	sonney2k, you are so great. can you tell me how do you that????
 sploving1	sonney2k, I did not see the right sitd
@sonney2k	I just write the output of the lua matrix to a file
 sploving1	sorry for that
 sploving1	you mean lua *.lua > 1.txt? sonney2k
@sonney2k	yes
 sploving1	okay. I will move to ruby
 sploving1	sonney2k I gtg bye
@bettyboo	see you
@sonney2k	sploving1, don't forget strings
@sonney2k	in lua I mean
@sonney2k	you havent' tested these yet
 sploving1	sonney2k, okay. I will test them. when output, just lua *.lua > 1.txt???
@sonney2k	sploving1, I don't understand what you mean
 sploving1	in my machine, I did not see the right side of the columns
 sploving1	1 0.026404555080215 0.00051704925476818 0.013315592813501 0.98824215105277 0.65147692801705 0.14929587268109 2.354172745438e-05 0.28323830778227
 sploving1	for instance i just see the first row is the above
 sploving1	but when I paste them on pasbin, it have the right side
 sploving1	so strange
-!- sploving1 [~sploving@124.16.139.194] has left #shogun []
@sonney2k	blackburn, why do you use m_labels in GaussianNB?
@sonney2k	I mean you only need this in train() or?
 blackburn	hmm
 blackburn	I can't remember
 blackburn	let me look ;)
 blackburn	sonney2k: m_labels is used only in train, yes
@sonney2k	ok then I remove m_labels from the .h
@sonney2k	etc
-!- heiko1 [~heiko@134.91.10.200] has joined #shogun
@sonney2k	heiko1, hey ... you survived :)
 heiko1	sonney2k, yes, but pain all over the body ;)
 heiko1	how do you say muskelkater in english? :)
 blackburn	hard weekend with gf? :D
 blackburn	the last words were 'girlfriend is waiting' IIRC :D
 heiko1	oh, yes and that also :)
 heiko1	and climbing
* sonney2k sings la la la *VERY* *LOUDLY*
 heiko1	and you guys? did you have a good weekend?
@sonney2k	blackburn had a lot of fun with java or coffee or so
 blackburn	haha
 blackburn	with HLLE too
@sonney2k	I am currently transitioning labels to use SGVector for real (internally)
@sonney2k	too
-!- heiko1 [~heiko@134.91.10.200] has quit [Ping timeout: 258 seconds]
-!- heiko1 [~heiko@134.91.52.56] has joined #shogun
@sonney2k	heiko1, do you know by heart where you recently added a SG_NOT_IMPLEMENTED?
 heiko1	yes, copy_subset() of CFeatures
 heiko1	called by KernelMachine::train
 heiko1	an example fails?
@sonney2k	yeah serialization
@sonney2k	but I don't think it was that one
 heiko1	mmh
@sonney2k	it is the gaussian kernel that is failing
 heiko1	let me check
 heiko1	compute feature vector of SimpleFeatures
@sonney2k	heiko1, yes that one!
 heiko1	this method hjust returns NULL there
 heiko1	has to be overridden or something
 heiko1	should I remove the SG_NOTIMPLEMENTED?
@sonney2k	I don't know though why it is called
 heiko1	i saw a call of it recently... i will check
 heiko1	SimpleFeatures::get_feature_vector
 heiko1	if no feature matrix is set
@sonney2k	makes sense
@sonney2k	so it could be that serializaton has some chicken / egg problem
 heiko1	yes
@sonney2k	that kernel should be loaded but features are not yet there or so
 heiko1	mmh
 heiko1	but if the SG_NOTIMPLEMENTED is removed
 heiko1	NULL is returned there
@sonney2k	and the gaussian kernel does some operation in load_serializable_post
@sonney2k	to precompute some x_i^2
 heiko1	sonney2k, the fire-alarm just started howling here
 heiko1	i will check whats happening
@sonney2k	heiko1, ok
 heiko1	indeed, there is a fire
@sonney2k	wow!
 heiko1	cars ariving, i will go outside for a minute
@sonney2k	who is burning?
 heiko1	(probalby takes more)
 heiko1	dont know
 blackburn	it is what they call 'extreme programming'
 heiko1	i am in 6th floor
 heiko1	afk...
@sonney2k	blackburn, your name is heiko1 program
 blackburn	what&
 blackburn	>
 blackburn	?
@sonney2k	I see them all burning with black smoke
@sonney2k	hope heiko1 manages to escape
 blackburn	where do you see it?
@sonney2k	live tv of course ;-)
 blackburn	joke? ;)
@sonney2k	no of course not *eg*
@sonney2k	I guess this bug hunt here is driving me made
@sonney2k	which reminds me
@sonney2k	blackburn, how is java going along?
 blackburn	sonney2k: currently working on HLLE
@sonney2k	blackburn, that is not fair
@sonney2k	you can have fun
@sonney2k	I have to fix bugs
 blackburn	I'm currently fixing bugs in HLLE :D
@sonney2k	you mean it is semi-fun
@sonney2k	hmmhh not sure I can live with that
 heiko1	re
 heiko1	small fire
 heiko1	wow 3 cars and police here
 heiko1	but builing will not be evacuated
@sonney2k	heiko1, what happened?
@sonney2k	or what is going on?
 heiko1	they are already gone,
 heiko1	but I did not have the motivation to go down all 150 chair steps ;)
 heiko1	probably nothing too scary
@sonney2k	ah no elevator...
@sonney2k	you could have climbed...
 heiko1	yes, but I dont like being in the elevator
 heiko1	yes, its possible here .)
@sonney2k	anyway heiko1 I made some progress on this here
 heiko1	ok?
@sonney2k	the strange thing is that the feature object got loaded just fine
@sonney2k	and it pretends that it did also load the matrix
@sonney2k	but for some reason not?!
 heiko1	mmmh
 heiko1	are this simple features?
@sonney2k	yes
@sonney2k	heiko1, I only noticed because I changed all of labels and thus lots of classifiers
@sonney2k	and so I recognized that some examples fail...
 heiko1	is the load method of SimpleFfeatures called?
@sonney2k	enabling debug I see that that simple features are already loaded...
@sonney2k	heiko1, w/ debug on I see that
@sonney2k	[DEBUG] START LOADING CSGObject 'SimpleFeatures'
@sonney2k	....
@sonney2k	[DEBUG] Loading parameter 'feature_matrix' of type 'Matrix<float64>'
@sonney2k	[DEBUG] DONE LOADING CSGObject 'SimpleFeatures' (0x3c973b0)
@sonney2k	but then in kernel lhs=0x3c973b0 '(nil)' num_vec_fm=0 num_feat_fm=0 num_vec=20 num_feat=2
@sonney2k	the nil there corresponds to no feature matrix around
@sonney2k	and the num_vec/feat_fm =0 indicate that the matrix indeed did not get loaded
 heiko1	strange :(
@sonney2k	heiko1, does the matrix stuff work at all?
@sonney2k	let me create a fool proof example
 heiko1	what do you mean with matrix stuff?
 heiko1	ok
@sonney2k	answer is no
@sonney2k	from modshogun import *
@sonney2k	from numpy import array
@sonney2k	feats=RealFeatures(array([[1.0,2,3],[4,5,6]]))
@sonney2k	fstream = SerializableAsciiFile("foo.asc", "w")
@sonney2k	feats.save_serializable(fstream)
@sonney2k	but in foo.asc we have feature_matrix Matrix<float64> 0 0 ()
 heiko1	so which part is not working? save or load?
@sonney2k	save
@sonney2k	heiko1, vector works though
@sonney2k	l=Labels(array([1.0,2,3]))
@sonney2k	fstream = SerializableAsciiFile("foo2.asc", "w")
@sonney2k	l.save_serializable(fstream)
@sonney2k	labels Vector<float64> 3 ({1}{2}{3})
@sonney2k	heiko1, do you have an idea where I should look for the bug?
@sonney2k	or do you even know what the problem could be?
 heiko1	mmh
 heiko1	i mean the save code of SimpleFeatures is short.
 heiko1	writer->f_write(feature_matrix, num_features, num_vectors);
@sonney2k	heiko1, not the save code of simplefeatures
@sonney2k	serialization
 heiko1	ah sorry
@sonney2k	like where you did add support for SGVector / SGMatrix :)
 heiko1	oh, ... mmh, perhaps the add methods of Parameter I did are wrong
@sonney2k	or not - we have to find out
@sonney2k	that was in shogun/base/Parameter.cpp?
 heiko1	yes
 heiko1	all the add methods
 heiko1	with SGVector SGMatrix
 heiko1	hope theres no mistake in them
@sonney2k	but we are not even using SGMatrix etc
 heiko1	oh
 heiko1	mmh
 heiko1	then this cant be the mistake
@sonney2k	we use the add_matrix stuff
 heiko1	but this wasnt touched recently or?
@sonney2k	maybe for the subsetting business
 heiko1	yes
@sonney2k	heiko1, I mean there are feature_matrix_num_vectors etc
@sonney2k	and these are 0 too
 heiko1	oh
 heiko1	I just got an idea
 heiko1	let me check
@sonney2k	and indeed they are
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
 heiko1	perhaps this has to do something with the variables that i changed
 heiko1	the add methods have also been changed
 heiko1	At sometime I replaced the features by a SGVector
@sonney2k	heiko1, no I think it is a bug in simplefeatures somehow
 heiko1	but undid this
 heiko1	yes
 heiko1	in SimpleFeatures
@sonney2k	I mean dimensions of feature matrix need to be non-zero
@sonney2k	could very well be my fault too...
 heiko1	mmh
 blackburn	hooray to new heisenbug in arpack wrapper!
 heiko1	sonney2k, I have an appointment in a few minutes, sorry for that, but I will be back later
@sonney2k	heiko1, found the bug!
 heiko1	where?
@sonney2k	heiko1, in the set_feature_matrix for SGMatrix type
@sonney2k	it was forgotten to set featuer_matrix_num_vectors
 heiko1	alright
 heiko1	then
@sonney2k	everywhere else it was ok
 heiko1	glad you found it :)
 heiko1	so, see you this in the evening
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 276 seconds]
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
 CIA-87	shogun: Soeren Sonnenburg master * rd85cb65 / (23 files in 9 dirs):
 CIA-87	shogun: remove unused confidences from labels and add SGVector in methods
 CIA-87	shogun: utilizing labels when possible - https://github.com/shogun-toolbox/shogun/commit/d85cb655161d037c2d0a0c2f0970f44de3a9e131
 CIA-87	shogun: Soeren Sonnenburg master * rd2f13fb / examples/undocumented/python_modular/serialization_matrix_modular.py : add example to just serialize matrix - https://github.com/shogun-toolbox/shogun/commit/d2f13fb17d3f9998af1a175dfd4e2bea4544fb3d
 CIA-87	shogun: Soeren Sonnenburg master * r58fd62c / (10 files in 7 dirs):
 CIA-87	shogun: various bugfixes related to SGVector/SGMatrix transition
 CIA-87	shogun: - in out typemap of python_modular for vectors
 CIA-87	shogun: - in simplefeatures set_feature_matrix
 CIA-87	shogun: - KRR double free
 CIA-87	shogun:  ... - https://github.com/shogun-toolbox/shogun/commit/58fd62cc8e2765e82b9e4fc5926603942355d47e
-!- CIA-87 was kicked from #shogun by bettyboo [flood]
-!- CIA-87 [~CIA@cia.atheme.org] has joined #shogun
@sonney2k	blackburn, yay!
@sonney2k	all examples work again :)
@sonney2k	python_modular only of course
 blackburn	nice
 CIA-87	shogun: Soeren Sonnenburg master * r00d57d5 / examples/undocumented/python_modular/clustering_gmm_modular.py : fix clustering example - https://github.com/shogun-toolbox/shogun/commit/00d57d584f283a8ed9512f97e74b6a8413b7b662
 CIA-87	shogun: Sergey Lisitsyn master * rbcb7bb8 / (2 files): Added DORGQR routine wrapper for lapack - https://github.com/shogun-toolbox/shogun/commit/bcb7bb8f5a965e5eb7e51f7c158a4015411f5cf5
 CIA-87	shogun: Sergey Lisitsyn master * r988360c / (5 files in 2 dirs): Introduced Hessian Locally Linear Embedding preprocessor - https://github.com/shogun-toolbox/shogun/commit/988360ca58dfa56181aedb7cb7781b7d64ee0a3d
 CIA-87	shogun: Sergey Lisitsyn master * r179091b / (2 files): Fixed LLE and added HLLE python modular example - https://github.com/shogun-toolbox/shogun/commit/179091b1af287374bc754b852f2a833321e8704e
 blackburn	sonney2k: vodka?
-!- gsomix [~gsomix@109.169.131.11] has joined #shogun
 gsomix	hi all
-!- f-x [~user@117.192.204.152] has quit [Remote host closed the connection]
 gsomix	sonney2k, i saw own ohloh account. I think i did something wrong with commiting. :)
 gsomix	All Languages. Total Lines Changed: 221,575.
 heiko1	sonney2k, are you there?
@sonney2k	heiko1, yes
@sonney2k	heiko1, wassup?
 heiko1	did you receive my email?
 heiko1	http://pastebin.com/mjPq525g
 heiko1	I want to create StringFeatures
 heiko1	but it does not work
 heiko1	because CAlphabet::check_alphabet() fails
 heiko1	ALPHABET does not contain all symbols in histogram
 heiko1	CAlphabet::6210
 heiko1	610
 heiko1	and I am a bit unsure, what I am doing wrong here
 heiko1	the program creates some random char strings (this works, they are printed) and then creates a CStringFeatures instance
 heiko1	and then SG_ERROR
 CIA-87	shogun: Soeren Sonnenburg master * rfe563a7 / src/shogun/features/Alphabet.h :
 CIA-87	shogun: fix documentation for ALPHANUM and PROTEIN alphabets (they take upper
 CIA-87	shogun: case chars not lowercase) - https://github.com/shogun-toolbox/shogun/commit/fe563a7c46e90eace5ec4177ab458883510986db
@sonney2k	heiko1, ^
@sonney2k	UPPER CASE
@sonney2k	so 0x41 and more
 heiko1	ah
 heiko1	oh no :)
 heiko1	silly mistake
@sonney2k	heiko1, well documentation was wrong...
@sonney2k	it said a-z
@sonney2k	not A-Z ...
@sonney2k	blackburn, vodka!
 blackburn	not yet! found error :D
@sonney2k	blackburn, you definitely need that to start working on java :D
@sonney2k	ohh
 blackburn	with arpack the solution is kinda wrong
 heiko1	sonney2k, well however, thanks .)
 blackburn	temporary will force to use lapack
@sonney2k	gsomix, sounds like it...
@sonney2k	heiko1, IIRC there is some debug option - then it will display the histogram...
 heiko1	sonney2k, works now :)
 CIA-87	shogun: Sergey Lisitsyn master * r9ff9954 / (3 files): Added force_lapack option for Locally Linear Embedding and forced HLLE to use lapack solver - https://github.com/shogun-toolbox/shogun/commit/9ff99546af4be84dfc1c079b4318f29e7b729735
@sonney2k	heiko1, surprise ;-)
 heiko1	sonney2k, I will try to generalise the model selecttion parameters now
 heiko1	was trying to do model selection with a string kernel
 heiko1	but it has int32_t type parameters
 heiko1	that does not work yet
@sonney2k	heiko1, so you do all the other standard types like int / byte etc right?
@sonney2k	should be easy given that you have double working already
 heiko1	yes
@sonney2k	enums can be a bit problematic though - but in the end these are ints too
 heiko1	yes
 heiko1	i am not that deep into generics
 heiko1	hope it is possible to append an instance of one modelparam to another with another type
@sonney2k	heiko1, enums are usually represented as integers - so at least from C/C++ / python it will work
@sonney2k	one could specify illegal values though - but that should be catched anyways
 heiko1	uuh this is harder than i thought
 heiko1	all this generic classes build trees
 heiko1	I dont know if it is possible to have a datastructure that holds instances of generic classes with different types?
 heiko1	no, its not
 heiko1	mmh
 heiko1	sonney2k, basically this problem:
 heiko1	http://www.cplusplus.com/forum/general/1281/
@sonney2k	yes indeed that is not working
@sonney2k	heiko1, but can't you define all the types / trees that you need?
 heiko1	but one tree may have different node types
@sonney2k	I mean there are only a handful
 heiko1	like one parameter int and another float
 heiko1	both children of one node
@sonney2k	what you could certainly do is to store the node types
 heiko1	yes
 heiko1	I think I will have to do that
@sonney2k	and then have some access function that gives you the correct item from a union or so based on that type
@sonney2k	or you have a node content base class
@sonney2k	and then have derived classes for each node content type
@sonney2k	but same thing not type safe
@sonney2k	you need to store the type again
 heiko1	yes
 blackburn	LLE IS FUCKING UNSTABLE
 blackburn	:E
 heiko1	I think I will just store the type and save the data with void pointers
@sonney2k	considering that we need just int, byte, bool - it is probably easiest to just use a union
@sonney2k	or that yes
@sonney2k	blackburn, like java examples SCNR :D
 blackburn	it drives me mad
 blackburn	I thought HLLE does
 blackburn	but it is not
 blackburn	I guess I should write sth like "If embedding is shity try another data" in description
 blackburn	sonney2k: you want me doing some notfunny things, ain't you?
* blackburn wonders if there will be some ModifiedHessianLocallyLinearEmbedding
 blackburn	or even ImplicitlyRestartedModifiedStableHessianLocallySublinearMegaEmbedding
@sonney2k	;)
 blackburn	oh
 blackburn	HLLE rocks!
 blackburn	sonney2k: I guess you might understand how good it is: http://dl.dropbox.com/u/10139213/shogun/hlle-faces-k10.png
@sonney2k	sure
 blackburn	damn that guy looks just like me :D
-!- in3xes [~in3xes@180.149.49.227] has quit [Quit: Leaving]
-!- gsomix [~gsomix@109.169.131.11] has quit [Ping timeout: 240 seconds]
 CIA-87	shogun: Sergey Lisitsyn master * r053c634 / .gitignore : Added some more filetypes to ignore by git - https://github.com/shogun-toolbox/shogun/commit/053c634fce544b1efb1b2141826dc5d4d7ba3ad7
 CIA-87	shogun: Sergey Lisitsyn master * rac722b7 / src/shogun/mathematics/arpack.cpp : Improved arpack wrapper - https://github.com/shogun-toolbox/shogun/commit/ac722b7fafeb94352f855b66dbfe66aca0cdd509
 CIA-87	shogun: Sergey Lisitsyn master * r1fff667 / src/shogun/preprocessor/LocallyLinearEmbedding.cpp : Improved stability of Locally Linear Embedding - https://github.com/shogun-toolbox/shogun/commit/1fff667d78a71a7ab8fd3c0ab9ca303ef8502359
 CIA-87	shogun: Sergey Lisitsyn master * re11c4bc / src/shogun/preprocessor/HessianLocallyLinearEmbedding.cpp : Improved HLLE - https://github.com/shogun-toolbox/shogun/commit/e11c4bcfcfab8163380b8c13933aaa3bc9d171a3
 CIA-87	shogun: Sergey Lisitsyn master * r05427ed / src/shogun/preprocessor/LocallyLinearEmbedding.cpp : Changed solver for LLE and removed unnecessary default parameter - https://github.com/shogun-toolbox/shogun/commit/05427edcff160e1241f9b9b8ad6674e8fc7b31cc
-!- blackburn [~blackburn@188.122.239.253] has left #shogun []
--- Log closed Tue Jul 26 00:00:21 2011
