--- Log opened Tue Aug 21 00:00:17 2012
@sonney2k	yoh_, alright. uploaded shogun 1.1.0-6
* sonney2k starts an upgrade to wheezy
 shogun-buildbot_	build #330 of deb2 - static_interfaces is complete: Failure [failed test octave_static]  Build details are at http://www.shogun-toolbox.org/buildbot/builders/deb2%20-%20static_interfaces/builds/330  blamelist: Soeren Sonnenburg <sonne@debian.org>
 yoh_	sonney2k: haven't consciously used shogun for a while :-/
 yoh_	btw -- remembering our discussion awhile ago -- have you come up with a built-in breakage of the ties for multi-class problems relying on voting of pair-wise classifiers (e.g. how it was in SVMs)
 yoh_	?
 shogun-buildbot_	build #420 of deb3 - modular_interfaces is complete: Failure [failed test python_modular]  Build details are at http://www.shogun-toolbox.org/buildbot/builders/deb3%20-%20modular_interfaces/builds/420  blamelist: Soeren Sonnenburg <sonne@debian.org>
 shogun-buildbot_	build #331 of deb2 - static_interfaces is complete: Success [build successful]  Build details are at http://www.shogun-toolbox.org/buildbot/builders/deb2%20-%20static_interfaces/builds/331
@sonney2k	yoh_, actually the whole multiclass code got rewritten this summer by pluskid (not online right now...) - we have tons of MC methods now and I simply don't know - ask on the mailinglist he will know
-!- av3ngr [~av3ngr@60-241-222-244.static.tpgi.com.au] has joined #shogun
 shogun-buildbot_	build #421 of deb3 - modular_interfaces is complete: Success [build successful]  Build details are at http://www.shogun-toolbox.org/buildbot/builders/deb3%20-%20modular_interfaces/builds/421
 shogun-buildbot_	build #61 of nightly_none is complete: Failure [failed compile]  Build details are at http://www.shogun-toolbox.org/buildbot/builders/nightly_none/builds/61
-!- av3ngr [~av3ngr@60-241-222-244.static.tpgi.com.au] has quit [Ping timeout: 244 seconds]
 shogun-buildbot_	build #72 of nightly_default is complete: Success [build successful]  Build details are at http://www.shogun-toolbox.org/buildbot/builders/nightly_default/builds/72
-!- emrecelikten [~emre@trir-5d800761.pool.mediaWays.net] has quit [Ping timeout: 244 seconds]
-!- emrecelikten [~emre@trir-4d0d941d.pool.mediaWays.net] has joined #shogun
-!- zxtx [~zv@c-24-6-91-131.hsd1.ca.comcast.net] has quit [Ping timeout: 244 seconds]
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- uricamic [~uricamic@2001:718:2:1634:e560:dc03:82b4:748f] has joined #shogun
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Read error: Operation timed out]
-!- gsomix_ [~gsomix@80.234.27.140] has joined #shogun
-!- gsomix [~gsomix@109.169.157.174] has quit [Read error: Connection reset by peer]
-!- gsomix_ [~gsomix@80.234.27.140] has quit [Quit: Ex-Chat]
 CIA-52	shogun: Sergey Lisitsyn master * r3a54e3f / src/shogun/classifier/FeatureBlockLogisticRegression.h : Updated doc of FeatureBlockLogisticRegression - http://git.io/KlGRsw
-!- pluskid [72fa5039@gateway/web/freenode/ip.114.250.80.57] has joined #shogun
-!- yoo [2eda6d52@gateway/web/freenode/ip.46.218.109.82] has joined #shogun
 yoo	hi alll
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- pluskid [72fa5039@gateway/web/freenode/ip.114.250.80.57] has quit [Quit: Page closed]
 CIA-52	shogun: Sergey Lisitsyn master * r64f1532 / src/shogun/features/SubsetStack.h : Added get_last_subset doc - http://git.io/Zo3oEg
 _____________	oh CIA-52 you are back
 _____________	n4nd0: hey how is it going?
 CIA-52	shogun: Sergey Lisitsyn master * rf9ca4a4 / src/shogun/transfer/multitask/MultitaskLeastSquaresRegression.h : Added doc for MultitaskLeastSquaresRegression - http://git.io/V3Yehw
 CIA-52	shogun: Sergey Lisitsyn master * r2a89ec1 / src/shogun/structure/StateModel.h : Removed Math class reference in doc to avoid warnings - http://git.io/gI08Tw
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Ping timeout: 245 seconds]
@sonney2k	_____________, do you know if this is caused by the - sign in front of CMath? https://github.com/shogun-toolbox/shogun/commit/2a89ec12acb3c5946ec5c726ad3794bebba3e1d6
* wiking is talking on the phone german... after 4 years of not speaking at all :)))
 wiking	pure fun that is :D
 _____________	sonney2k: still warning??
 _____________	wiking: ich spreche etwas deutsch! :D
@sonney2k	_____________, no but I was wondering if it is possible to keep CMath::INFTY there if one just moves the - sign
 _____________	sonney2k: I had no idea so removed that
-!- zxtx [~zv@cpe-75-83-151-252.socal.res.rr.com] has joined #shogun
-!- yoo [2eda6d52@gateway/web/freenode/ip.46.218.109.82] has quit [Quit: Page closed]
 CIA-52	shogun: Sergey Lisitsyn master * r8d0d4e2 / src/shogun/lib/IndexBlockGroup.h : Updated doc of IndexBlockGroup - http://git.io/gmU9tg
 CIA-52	shogun: Sergey Lisitsyn master * rd5c67f9 / src/shogun/latent/LatentSOSVM.h : Updated doc of LatentSOSVM - http://git.io/XxPDvA
 CIA-52	shogun: Sergey Lisitsyn master * r4ec5ff5 / src/shogun/latent/LatentSOSVM.h : Updated LatentSOSVM apply doc causing warning - http://git.io/WSZk7g
-!- pluskid [72f6b389@gateway/web/freenode/ip.114.246.179.137] has joined #shogun
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
 n4nd0	hey _____________
 _____________	nice nickname I have isn't it?
 _____________	:)
 _____________	n4nd0: what's up?
 n4nd0	yeah really cool nickname
 n4nd0	_____________: not much, I just came from university
 n4nd0	_____________: you at job?
 _____________	yeah
 _____________	are your studies started already?
 n4nd0	no, not yet
 n4nd0	it was a meeting for first year students
 n4nd0	I went there to give tips and stuff like that
 _____________	did you give a tip about shoguning?
 _____________	and vodka of course
 n4nd0	haha
 n4nd0	I just told my professor about my summer in SHOGUN
 _____________	what does he think about it? :)
 n4nd0	aah he just said, oh coding summer ;)
-!- emrecelikten [~emre@trir-4d0d941d.pool.mediaWays.net] has quit [Ping timeout: 244 seconds]
 _____________	n4nd0: do you expect you manage to put HM-SVM based on BMRM before release?
 n4nd0	_____________: I don't think so since now I am focusing more on the ASR application
 _____________	I see
 n4nd0	_____________: do you think it is important to have it for now?
 _____________	well I don't know
 n4nd0	aham ok
 _____________	I'd ask you to check mosek implementation for bugs
 _____________	have you any test you used before?
 n4nd0	I've tested and compared results with the hm-svm toolbox (the one written by Gunnar and Georg)
 _____________	can there be any regression due to last changes?
 n4nd0	what do you mean with regression?
 _____________	in soft engineering mean :)
 n4nd0	still :)
 n4nd0	what do you mean? hehe
 _____________	bug introduced by some change in other code
 n4nd0	like to come back?
 n4nd0	mmm I hope not, at least I believe it isnt'
 _____________	okay
 n4nd0	at least the hm-svm application returns exactly the same results as the toolbox
 _____________	I think we will manage to release in healthy state at 1st of september
 n4nd0	the only thing I think I am going to add before release is the PLiF support
 n4nd0	this is already in a branch of my fork
-!- alexlovesdata [~binder@194.95.174.230] has joined #shogun
-!- vojtech [9320543b@gateway/web/freenode/ip.147.32.84.59] has joined #shogun
 CIA-52	shogun: Chiyuan Zhang master * r2590cf5 / (8 files in 4 dirs): added doc to disable warning. - http://git.io/ZhnLIQ
 CIA-52	shogun: Sergey Lisitsyn master * r4cbdfa4 / (8 files in 4 dirs): Merge pull request #750 from pluskid/multiclass - http://git.io/KVKzpQ
-!- pluskid [72f6b389@gateway/web/freenode/ip.114.246.179.137] has quit [Quit: Page closed]
 wiking	alexlovesdata: hey hey
-!- yoo [2eda6d52@gateway/web/freenode/ip.46.218.109.82] has joined #shogun
-!- vojtech [9320543b@gateway/web/freenode/ip.147.32.84.59] has quit [Quit: Page closed]
-!- uricamic [~uricamic@2001:718:2:1634:e560:dc03:82b4:748f] has quit [Quit: Leaving.]
-!- yoo [2eda6d52@gateway/web/freenode/ip.46.218.109.82] has quit [Quit: Page closed]
 n4nd0	see you later guys
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Quit: leaving]
-!- yoh_ is now known as yoh
-!- blackburn [~blackburn@83.234.54.182] has joined #shogun
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
 n4nd0	hey guys, one question about the final evaluation: is there a part to submit by the mentors too?
 blackburn	ooooohh nice you mentioned that i totally forgot
 blackburn	yes mentors do submit it too
@sonney2k	yoh, https://buildd.debian.org/status/package.php?p=shogun looks good - powerpc still has the clang error but otherwise all good
@sonney2k	yoh, so now where do you think shall I ask?
@sonney2k	n4nd0, I can tell that it is important to have BMRM based SO stuff... otherwise we won't have many users...
 n4nd0	sonney2k: ok, I understand
 blackburn	n4nd0: if you want I could help you somehow with it
 n4nd0	blackburn: sure
@sonney2k	n4nd0, it is like one is invited to that *great* party but one needs a car to get there (or boat if you want :)
 blackburn	n4nd0: for example I could introduce a risk for your HM model
 blackburn	and you could test it
 blackburn	I mean compare with that G&G :D implementation
 n4nd0	blackburn: I think that the generic risk function in the StructuredModel should work
 n4nd0	blackburn: otherwise, what risk would you like to introduce in the HM model?
 blackburn	well the same but not generic
 blackburn	I could add generic if you want :)
 blackburn	and then you could test
 blackburn	is that better for you?
 n4nd0	ok
 blackburn	sonney2k: hey we have a shogun party on mars
 blackburn	but no curiosities left though
 blackburn	so you have to swim
-!- octopine is now known as audy
 blackburn	n4nd0: did you ever eat cat food?
 blackburn	I did but nobody else did :D
 blackburn	didn't*
 blackburn	sonney2k: may be you? :D
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Ping timeout: 244 seconds]
-!- alexlovesdata [~binder@194.95.174.230] has left #shogun []
 yoh	sonney2k: well -- #debian-release or whatever it is ... ?  or the mailing list... or 'reportbug release.debian.org' although that one probably would be stale for a while and IRC or ML might be a better choice for the discussion
-!- blackburn1 [~blackburn@188.168.13.81] has joined #shogun
 blackburn1	sonney2k: any idea what is wrong wtih gp regression and apply_regression extend ?
-!- blackburn [~blackburn@83.234.54.182] has quit [Ping timeout: 240 seconds]
 CIA-52	shogun: Sergey Lisitsyn master * r6d6770b / (2 files): Removal for last multiclass warnings - http://git.io/aFKt-Q
 blackburn1	n4nd0 hmm multiclass risk is more efficient as it is now
 blackburn1	it uses no compute joint feature vector
-!- yoo [575b08cb@gateway/web/freenode/ip.87.91.8.203] has joined #shogun
 yoo	hi all
 blackburn1	yoo: hey
 yoo	blackburn1: hey! how is it going ?
 blackburn1	pretty busy last days because of getting back to job I had before gsoc :)
 blackburn1	lets do that multiclass output stuff you wanted tonight
 yoo	nice
 yoo	I am figuring out stuff with yaml format
 yoo	since today I ahve only work with shogun c++
 blackburn1	so you switched to C++?
 yoo	but plotting is a pretty hard task, using boost python ..
 yoo	I try to switch to python modular
 blackburn1	okay - what makes it hard to use in python?
 yoo	sorry, you didnt understand, I usually use c++ but now I try python
 blackburn1	oh :D
 blackburn1	I mix things now because quite a few users use python or C++
 yoo	what did people use usually ?
 blackburn1	I use python mainly
 blackburn1	but recently I've used C++ for problem where opencv is required
 blackburn1	opencv's python interface is crappy
 yoo	I am from image processing then I have opencv routines as well
 yoo	yep it is !
 yoo	thats why I am trying to export xml or yml (opencv) format to python
 blackburn1	recently I switched to vlfeat because it has quite efficient and clear phow (dense sift aka hog) implementation
 blackburn1	opencv's hog is something like WTF
 yoo	I have coded my own HOG last year
 blackburn1	it is much easier to try to convert your problem to pedestrian detection than extract HOG features from it :D
 yoo	opencv's hog uses some weird buffer etc ..
 yoo	I/O in opencv is crappy for everythg except c++ and I must admit that the opencv community is not as "kind" as shogun one :p
 blackburn1	yoo: there is http://www.vlfeat.org/api/dsift.html I like most
 blackburn1	I never talked to any of them - what is wrong?
 yoo	maybe that because opencv community is very big ^^
 blackburn1	yeah we have only a few developers
 yoo	it seems that the bindings for python are not very well update
 yoo	File storage is very messy for python
 blackburn1	why?
 yoo	do you know YML format ?
 blackburn1	no
 blackburn1	what is it for?
 yoo	it permits to store data like matrix (data, size, type) and sequences .. in a hash talbe style
 yoo	format
 yoo	very efficient to parse
 yoo	BUT python bindings are not up-to-date then ndoes are not well recognize etc ..
 blackburn1	I see
 blackburn1	sonney2k: okay if everything is correct HM SVM should work with BMRM now
 blackburn1	I will commit in a min
 yoo	BMRM ?
 blackburn1	bundle method for risk minimization
 blackburn1	currently HM works only with mosek
 yoo	yes I have seen that
 yoo	I try to keep an eye on whats new here every weeks :p
 blackburn1	things will stop now unfortunately
 blackburn1	however I am thinking about CRF and other stuff like that research so may be I will extend it
 yoo	yet, YML format is a right way to store data for large scale machine learning.
 yoo	I was quite surprise that shogun dont have is own i/O format
 yoo	for now it is only libsvm style right ?
 blackburn1	for serialization? we have quite a few
 blackburn1	json, xml, pure ascii
 blackburn1	hdf5
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
 yoo	<blackburn1> things will stop now unfortunately : what do you mean ? End of gsoc ?
 blackburn1	yeah exactly
 blackburn1	n4nd0: I am done with generic risk probably - will commit in a minute
 n4nd0	blackburn1: I have just seen logs about the BMRM
 n4nd0	blackburn1: nice!
 blackburn1	waiting it to finish compilation
 blackburn1	resultset you added is pretty useful
 blackburn1	implementation is really straightforward with it - I thought of something more difficult
 n4nd0	blackburn1: well after that conversation we had, it seemed it was going to be something easy
 blackburn1	lets check if everything is correct
 blackburn1	in a minute more
 blackburn1	:D
 n4nd0	maybe we have misunderstood something
 n4nd0	blackburn1: let me know when you commit please
 blackburn1	I am pretty sure we didn't do any mistake out there
 blackburn1	at least it fits with multiclass things just perfectly
 n4nd0	yeah
 blackburn1	n4nd0: generic risk in multiclass is really bad
 blackburn1	it would make very sparse vectors go all around
 blackburn1	so specific should stay
 n4nd0	blackburn1: what do you mean with very sparse vectors go all around?
 n4nd0	what is the difference between making it generic and specific?
 blackburn1	n4nd0: okay imagine we have 5000 dimensional vectors and 50 classes
 blackburn1	what is dim of joint feature vector
 blackburn1	and how many zeros out there?
 blackburn1	current multiclass risk do not allocate anything
 blackburn1	generic will allocate two 250000 dimensional vectors for each feature vector
 blackburn1	and will add that to subgrad
 n4nd0	I understand what you mean
 n4nd0	although using sparse data structures that shouldn't be a problem, right?
 blackburn1	yeah but currently we are not using that
 n4nd0	ok
 n4nd0	do you want to carry out the test for HM-SVM in any case?
 n4nd0	using the StructuredAccuracy class it shouldn't take much to get the feeling it the idea is correct at least
 n4nd0	just as proof of concept
 blackburn1	so you want me to test it too? :D
 blackburn1	I don't mind but I don't know how
 n4nd0	I can test it
 CIA-52	shogun: Sergey Lisitsyn master * r530155d / src/shogun/structure/StructuredModel.cpp : Added generic risk function implementation for structured model class - http://git.io/96jvMg
 n4nd0	I was not saying it like "do this"
 blackburn1	do we have an example?
 n4nd0	now that is commited I can make it ;)
 blackburn1	oh we need to add an example I guess
 blackburn1	hmm
 blackburn1	is HMSVMModel guarded for mosek?
 blackburn1	oh that's cool
 blackburn1	nothing to change
 n4nd0	if a bundle method is used, of course
 blackburn1	n4nd0: I will add an example right now
 n4nd0	blackburn1: I am trying to work out one too :)
 blackburn1	n4nd0: what is accuracy in that example you did add to python modular?
 n4nd0	99%
 n4nd0	do we have BMRM in modular interfaces?
 blackburn1	http://www.reactionface.info/sites/default/files/images/YvEN9.png
 blackburn1	yes
 n4nd0	haha
 blackburn1	ohhh
 blackburn1	matrix features issue
 blackburn1	michal's svm wants to have dot features
 n4nd0	game over
 blackburn1	nope
 n4nd0	any good guess for lambda?
 blackburn1	I don't have any
 n4nd0	let's change DotFeatures -> Features?
 blackburn1	yeah seems that is required
 n4nd0	yes
 n4nd0	in the DualLibQPBMSOSVM it seems that it doesn't affect much
 n4nd0	btw, CDualLibQPBMSOSVM::train_machine looks weird to me
 blackburn1	n4nd0: problem is that dlqpbmsosvm is linear SO machine
 n4nd0	blackburn1: what problem do you find with it?
 blackburn1	n4nd0: why?
 blackburn1	ahh I though linear SO machine needs dot features
 blackburn1	why do we need to have features in svm at all?
 blackburn1	if we have model that have features?
 n4nd0	I was wondering that right now too
 n4nd0	and this is actually a decision I made...
 n4nd0	I don't think we need them there too
 n4nd0	it is like the labels
 n4nd0	probably they shouldn't be given in the constructor of SOMachine
 n4nd0	but just take the ones that are in the model
 n4nd0	blackburn1: around
 n4nd0	?
 blackburn1	sorry got interrupted by email
 blackburn1	okayy
 blackburn1	I don't know to be sure
 blackburn1	to be honest :D
 n4nd0	hehe
 blackburn1	n4nd0: we could remove it completely - they are not needed for training
 n4nd0	I agree
 n4nd0	blackburn1: do you want to do the change or do you want me to do it?
 blackburn1	I will do
 n4nd0	ok
 blackburn1	one thing is unclear here
 blackburn1	n4nd0: take a look at
 blackburn1	apply_structured
 blackburn1	in linear so machine
 blackburn1	n4nd0: we have to set features for model, rgiht?
 blackburn1	currently it looks like something wrong for me
 n4nd0	set_features sets the ones for the model too
 blackburn1	ah got it
 n4nd0	blackburn1: is sonney2k still around?
 blackburn1	n4nd0: okay I think best way is to emulate it has m_features
 blackburn1	but handle model's features
 n4nd0	blackburn1: ok
 blackburn1	good for you?
 n4nd0	blackburn1: so you are just planning to remove them from the constructor?
 n4nd0	the features param I mean
 blackburn1	from class
 n4nd0	the member too?
 blackburn1	??
 blackburn1	oops
 blackburn1	yes I mean
 n4nd0	ok
 n4nd0	so get_features and set_features are still there
 n4nd0	just calling the ones of the model
 n4nd0	is that what you meant with emulate?
-!- blackburn [~blackburn@37.61.181.133] has joined #shogun
 blackburn	here
 blackburn	okay
 blackburn	n4nd0: so we don't need features in dual lib qp bm so svm too, right?
 n4nd0	blackburn: if they go out of the LinearSOMachine, they go out of this one too
 n4nd0	dual lib qp bm so svm gets it by inheritance
 blackburn	yes
 yoo	re
-!- blackburn1 [~blackburn@188.168.13.81] has quit [Ping timeout: 244 seconds]
 blackburn	okay lets see if that works
 n4nd0	blackburn: how does it look like?
 blackburn	compiled lets check
 blackburn	I need to adjust examples now
 blackburn	uh
 blackburn	hm svm objective grows exponentially :D
 n4nd0	wow, really?
 n4nd0	I am not sure if that is good
 blackburn	yeah I think it is caused by some error
 n4nd0	blackburn: let me know if I can help with something
 blackburn	n4nd0: you could check a risk function I added
 blackburn	are signs of psi_pred and psi_truth I have added correct?
 n4nd0	http://git.io/96jvMg?
 blackburn	yes
 blackburn	Psi(x,hat y) - Psi(x,y), right?
 n4nd0	yeah, I think so
 n4nd0	blackburn: I think there might be a problem with the definition though
 blackburn	n4nd0: definition of?
 n4nd0	http://pastebin.com/PUfyBp9P
 n4nd0	the risk function
 blackburn	ahhhhh
 n4nd0	it is the max of that thing there
 blackburn	I forgot delta
 n4nd0	but are you sure that using the argmax is correct?
 blackburn	yes, pretty sure
 blackburn	I forgot delta - that is the bug for sure
 n4nd0	shouldn't it be checked for every y which one is it that maximizes l + <w, psi(x,y) - psi(x,y)>
 n4nd0	blackburn: mmmm why are you so pretty sure?
 n4nd0	I mean
 blackburn	I can't see nothing wrong here
 n4nd0	the argmax will tend to minimize the first term
 n4nd0	Delta
 n4nd0	while maximize the second
 n4nd0	right?
 blackburn	I think I got what you mean
 blackburn	so you mean we minimize only <w,predict-truth>
 blackburn	not delta?
 n4nd0	look that in his code Michal is actually maximizing the whole thing
 n4nd0	what I mean is that
 n4nd0	what you did for the risk function assumes
 n4nd0	that max_y [Delta(yi, y) +  <w, Psi(xi, y) - Psi(xi,yi)>]
 n4nd0	is always given by the argmax
 n4nd0	and I think that is not true
 blackburn	are you going to say delta changes the game?
 n4nd0	blackburn: do you understand what I mean?
 n4nd0	yes, it is Delta what changes it :)
 blackburn	I have no proof for that but it is true for multiclass and I induce it for other so stuff
 n4nd0	blackburn: what is true for multiclass?
 n4nd0	it is not true that the max of the equation
 n4nd0	\max_{y \in \mathcal{Y}} \left[ \ell(y_i, y) + \langle {\bf w}, \Psi(x_i, y) - \Psi(x_i, y_i)  \rangle  \right]
 blackburn	argmax_y [ <w, Psi(xi,y) - Psi(xi,yi) > ] = argmax_y [ Delta(yi,y) + <w, Psi(xi,y) - Psi(xi,yi) > ]
 n4nd0	that is not true
 n4nd0	because the term Delta(yi,y) is going to be minimized by the argmax_y
 blackburn	no it is maximized
 n4nd0	mmm why?
 wiking	mmm woah i need to read this conversation :D
 n4nd0	wiking: haha
 blackburn	because you maximize difference between dot products
 wiking	btw: anybody knows why tar -t <tarfile> doesn't work :)
 blackburn	you make them farther from each other
 blackburn	and loss grows
 blackburn	something like that in my mind
 n4nd0	blackburn: look just at the term delta
 n4nd0	Delta(yi,yi) = 0
 n4nd0	right?
 blackburn	yes
 n4nd0	ok, now Delta(yi,y)
 n4nd0	the closer y is to yi -> the smaller Delta(yi,y) is
 n4nd0	ok?
 blackburn	yes
 n4nd0	when you have a good model
 n4nd0	i.e. a good w
 n4nd0	you will get good predictions
 blackburn	yes
-!- yoo [575b08cb@gateway/web/freenode/ip.87.91.8.203] has quit [Ping timeout: 245 seconds]
 n4nd0	i.e argmax_y for the example xi will be close to yi
 blackburn	yes
 n4nd0	then Delta(yi,y) is minimized for y = argmax
 n4nd0	?
 blackburn	no, it is maximized by argmax of <w,Psi(x_i,y_i)-Psi(x_i,y)>
 n4nd0	wait
 n4nd0	we are not talking about that term now
 blackburn	Delta is max then y_i is far away from y
 n4nd0	we are just seeing how Delta(yi,y) behaves
 n4nd0	exactly
 blackburn	I see no problem out there
 n4nd0	the contrary happens for the other term
 blackburn	the second term is zero then y_i is close to y
 blackburn	and vice versa if y_i is far away from y
@sonney2k	wiking, tar -tf
 n4nd0	blackburn: I am writing it down and thinking of it :D
 blackburn	okay I will try to find true bmrm minimizer meanwhile
 blackburn	okay unstoppable grow of objective was caused by too small lambda
 blackburn	that happened to me with other method so I guess that is ok
 n4nd0	blackburn: ok
 blackburn	no I get reasonable w
 blackburn	but I broke apply probably
 blackburn	oh yes I did
 blackburn	sonney2k: captain we are fixing SO!
 blackburn	oh segfault now cool
 blackburn	okay fixed
 n4nd0	blackburn: look at this
 n4nd0	blackburn: in your reasoning
 blackburn	Accuracy = 0.9948
 n4nd0	blackburn: you say that when y_i is far away from y
 blackburn	n4nd0: my reasoning works :D
 n4nd0	blackburn: hahaha
 n4nd0	blackburn: c'mon ... it cannot be true!
 blackburn	n4nd0: they have the same interface for increasing/decreasing
 n4nd0	blackburn: I need you to tell this
 blackburn	both decrease when y_i is close
 blackburn	yes tell me
 n4nd0	both are zero when y_i = y
 n4nd0	I am ok with that
 n4nd0	but when y_i is very different from y, what happens then?
 n4nd0	Delta is maximized
 blackburn	both delta and loss are big
 n4nd0	but what about the second term?
 n4nd0	delta and loss are the same, aren't they?
 blackburn	the same?
 blackburn	why the same?
 n4nd0	what about < w, Psi(xi, y) - Psi(xi,yi)>
 n4nd0	let's sync notation
 n4nd0	for me the loss is Delta(y,yi) -> that's why I said that Delta and the loss are the same :D
 n4nd0	I guess that you meant the dot product with delta
 blackburn	ah
 blackburn	yeah
 n4nd0	ok
 n4nd0	so now
 blackburn	okay let delta be 1st
 blackburn	and w, psi the second
 n4nd0	good
 n4nd0	then
 n4nd0	what happens with the second when yi and y are very far from each other
 n4nd0	?
 blackburn	second is really big then
 n4nd0	why so?
-!- yooo [575b08cb@gateway/web/freenode/ip.87.91.8.203] has joined #shogun
 blackburn	because they are far away in feature space
 n4nd0	well
 n4nd0	it could be that by chance, this Psi(xi,y) ~ 0
 n4nd0	then
 n4nd0	<w, psi(xi,y) - psi(xi,yi)> ~ -<w, psi(xi,yi)>
 blackburn	it is not the case to help the understanding
 blackburn	:)
 n4nd0	hahaha
 n4nd0	:D
 blackburn	imagine classes
 blackburn	to the left
 blackburn	and to the right
 blackburn	what happens with the second term
 blackburn	then one is to the left
 n4nd0	yes
 blackburn	but it should be to the right
 n4nd0	but you are assuming that
 n4nd0	y very different from yi => psi(x,y) very different from psi(x,yi)
 blackburn	yes, it is
 n4nd0	there is no condition for that
 blackburn	try to get back to typical svm
 n4nd0	let me give you a more formal argument a moment
-!- yoo [~eric@bdv75-2-87-91-8-203.dsl.sta.abo.bbox.fr] has joined #shogun
-!- yoo [~eric@bdv75-2-87-91-8-203.dsl.sta.abo.bbox.fr] has quit [Client Quit]
 n4nd0	blackburn: let's go
 n4nd0	so we have
 n4nd0	max_y [ Delta(yi, y) + < w, Psi(xi,y) - Psi(xi,yi) > ]
 n4nd0	now, the second Psi doesn't depend on y, then
 n4nd0	max_y [ Delta(yi, y) + < w, Psi(xi, y) > ] - Psi(xi, yi)
 n4nd0	agree?
 blackburn	seems to be
 n4nd0	ok
 n4nd0	by definition
 n4nd0	the argmax_y maximizes < w, Psi(xi, y) >
 n4nd0	AND
 n4nd0	providing that we have a good model
 n4nd0	the argmax_y makes y close to yi; therefore, minimizes Delta(yi, y)
 blackburn	no, it is a loss term
 blackburn	it finds most violating y
 blackburn	not most complying one
 n4nd0	http://www.cs.cornell.edu/people/tj/publications/tsochantaridis_etal_04a.pdf
 n4nd0	equations (1) and (2)
 n4nd0	I think that the concept of finding the mos violating one is related to the way of optimization
 n4nd0	cutting plane algorithm
 n4nd0	it is not related to the argmax definition
 blackburn	(1) and (2) are decision functions
 blackburn	they are not related with risk
 n4nd0	it was just to tell you that the idea of mos violated doesn't appear there
 n4nd0	most*
--- Log closed Wed Aug 22 00:00:05 2012
