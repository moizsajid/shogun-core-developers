--- Log opened Sun Jul 31 00:00:59 2011
 blackburn	sonney2k: no leaks I'd say
@sonney2k	ok then good...
 blackburn	sonney2k: tried with init and loaded features - no leak, jigsaw memory usage :D
 blackburn	1.3 1.5 1.9 2.3 1.6 1.9 2.2 1.0 ...
 blackburn	2M kernels
 blackburn	sonney2k: so the only thing to get to work - examples..
@sonney2k	yeah looks like
@sonney2k	blackburn, I have a new suggestion for SGVector btw:
@sonney2k	        static virtual void SGVector& get_vector(SGVector &src, bool own=true)
@sonney2k	        {
@sonney2k	            if (!own)
@sonney2k	                return src
@sonney2k	            orig.do_free=false;
@sonney2k	            return SGVector(src.vector, src.vlen);
@sonney2k	        }
 blackburn	better
@sonney2k	I think I will go with this one
 blackburn	sonney2k: week before I made some tester for java
 blackburn	but can't decide if I should continue..
@sonney2k	blackburn, don't for now
@sonney2k	this needs discussion
@sonney2k	the question really is - how do we want to compare if things are the same in our test suite
 blackburn	sonney2k: I think we should split examples to some 'unit-tests' and complex examples
@sonney2k	not a good idea
 blackburn	sonney2k: I think we shouldn't compare if things are the same
@sonney2k	no one will maintain tests
 blackburn	sonney2k: unit tests should be autogenerated
@sonney2k	?
 blackburn	well all the kernels are the same..
@sonney2k	blackburn, so?
@sonney2k	we had that before
@sonney2k	the test suite now is 100% useless because no one updates it
@sonney2k	it is enough work to 'just' update examples
 blackburn	I mean we can only write templates
@sonney2k	doesn't help
 blackburn	why?
@sonney2k	too much work
@sonney2k	you need to do that for everything
@sonney2k	and there is not that much that you can generatlize
@sonney2k	there are always exceptions
 blackburn	but agree, we can't maintain tests for java,python,...
@sonney2k	it really is much easier to write examples for everything (which we have to have anyways)
@sonney2k	and then return some reasonable number or so
-!- in3xes [~in3xes@180.149.49.227] has quit [Quit: Leaving]
@sonney2k	that we use to compare results
 blackburn	I don't like the way of comparing results or so
@sonney2k	blackburn, because?
 blackburn	sonney2k: well it looks strange to me..
@sonney2k	blackburn, yeah but why?
 blackburn	I think we should only test if no errors
@sonney2k	what does no errors mean?
 blackburn	no compile-time errors, no runtime errors like segfaults..
@sonney2k	blackburn, but then that might mean we return just crap
 blackburn	return to?
@sonney2k	I could replace train() with random()
@sonney2k	and no one would recognize
 blackburn	can we recognize it now?
@sonney2k	yes
 blackburn	how?
@sonney2k	blackburn, because we know that at time point T1 everything was correct
@sonney2k	now we develop sth else
@sonney2k	and just compare whether result at T2 is the same as T1
 blackburn	we don't use it at all..
@sonney2k	blackburn, yeah because no one runs it
@sonney2k	and we have no build bot that does it automagically
@sonney2k	but we also have a problem
@sonney2k	because results are GaussianKernel et
@sonney2k	c
@sonney2k	and we pickle.dump
@sonney2k	and internally we changed formats so serialization results are different and we can no longer load results
 blackburn	bad bad
@sonney2k	blackburn, yes.
@sonney2k	the issue is here how can we keep the format constant or at least compatible
 CIA-87	shogun: Soeren Sonnenburg master * r42595fa / src/interfaces/java_modular/swig_typemaps.i : use simple swig enums - https://github.com/shogun-toolbox/shogun/commit/42595fa7a75037216c096aeb9879f265c37fdbfe
 CIA-87	shogun: Soeren Sonnenburg master * r95f11a0 / (5 files in 3 dirs): Hopefully fix compiler errors in GMM/Gaussian. Utilize destroy_*. - https://github.com/shogun-toolbox/shogun/commit/95f11a02929a4db4404c4bfdd0670d43cebf0610
 blackburn	sonney2k: what do you think, is it better to use Gaussian things in GaussianNaiveBayes?
@sonney2k	blackburn, not our biggest problem now - rather think about what we do to keep the serialization format compatible
 blackburn	it simply fits a gaussian for each class
@sonney2k	maybe we need some kind of  variable x is now y
 blackburn	I'm just talking about it because there is a little bug in GNB :D
@sonney2k	or things like this
 blackburn	sonney2k: how it looks now?
@sonney2k	blackburn, talk to alesis-novik about this :)
@sonney2k	blackburn, well we basically register all member varibles
@sonney2k	the problem is that we introduced new ones now
@sonney2k	and we renamed old ones or even changed types...
 blackburn	any automagic way ?
 blackburn	btw in java we would have way hehe
@sonney2k	how do you do that in java?
@sonney2k	if you change an object and rename variables?
 blackburn	well it is possible to get variable types and names
 blackburn	in java*
@sonney2k	blackburn, how does this help our problem?
 blackburn	'nohow', we aren't using java :)
@sonney2k	if you rename a variable - the serialized file would be different and so you couldn' load the serialized object
@sonney2k	the old one I mean
 blackburn	ah i see
@sonney2k	that is our problem atm
@sonney2k	we need to extend the serialized format to store some transitioning information
@sonney2k	like version when this varialbe appeared
@sonney2k	and when this thing vanished or whatever
@sonney2k	and renamed
 blackburn	sonney2k: didn't you say that it doesn't really matter now because we changed much things already?
@sonney2k	and transition form verions x to version y functions
@sonney2k	blackburn, no
@sonney2k	it is the only way to ensure testing...
 blackburn	do you want to make it still compatible?
@sonney2k	the only other alternative is to check each method individually
@sonney2k	by hand that is
 blackburn	okay time for some sleep
 blackburn	sonney2k: see you
@sonney2k	cu
-!- blackburn [~blackburn@188.122.224.26] has quit [Quit: Leaving.]
-!- f-x [~user@117.192.192.42] has joined #shogun
-!- f-x` [~user@117.192.222.125] has joined #shogun
-!- f-x [~user@117.192.192.42] has quit [Ping timeout: 260 seconds]
-!- f-x [~user@117.192.222.125] has joined #shogun
 f-x	sonney2k: you here?
-!- f-x` [~user@117.192.222.125] has left #shogun ["ERC Version 5.3 (IRC client for Emacs)"]
-!- f-x [~user@117.192.222.125] has quit [Quit: ERC Version 5.3 (IRC client for Emacs)]
-!- f-x [~user@117.192.222.125] has joined #shogun
-!- f-x is now known as Guest33029
-!- Guest33029 is now known as f-x`
 f-x`	sonney2k: should i use enums to identify the loss function for SVMSGD?
@sonney2k	f-x`, no just use a loss member
@sonney2k	then you can add some flag or so for this < comparison
 f-x`	sonney2k: i don't know.. what kind of flag do you suggest?
-!- f-x` is now known as f-x
-!- heiko [~heiko@134.91.52.15] has joined #shogun
@sonney2k	f-x, for SGD/QN you don't really need that flag if you treat the learning algorithms differently
@sonney2k	but if you don't want to do that then you could introduce some flag needs_extra_update or so that is set true for some...
@sonney2k	heiko, hi...
 heiko	sonney2k, hi!
@sonney2k	heiko, I was trying to stabilize things and came across one issue...
 heiko	which one?
@sonney2k	that is our test suite no longer works due to all the variable addtions (like subset) and renames and SGVector stuff
@sonney2k	so heiko I was wondering if you would have time to work on this a bit ...
 heiko	yes, I can do this
 heiko	Think I will finish the KMeans stuff today
 heiko	but what exactly is the problem?
@sonney2k	heiko, you did subset for string features right, but sparse is still missing?
@sonney2k	and kernel / distance machine will work too - at least soon
 heiko	no sparse features already have subset
 heiko	an example is there too
 heiko	in c++
 heiko	yes, kernel machines work
 heiko	but only with simple/string features
 heiko	no modelselection for sparse features currently
@sonney2k	heiko, ok so then todo was only some nicer python syntax typemaps and different sampling techniques
 heiko	yes
@sonney2k	heiko, why not - how is it different?
@sonney2k	^sparse & ms
 heiko	cross validation needs a method of CFeatures that is not implemented for sparse
 heiko	copy_subset
@sonney2k	heiko, btw when you use SGVector in a class, could you please use vector.destroy_vector() in destructor?
@sonney2k	heiko, I just don't understand the difference to any other feature object ...
 heiko	sonney2k, yes, you changed this, i forgot
@sonney2k	ahh ok
@sonney2k	(I was just reading your pull request/patch)
 heiko	sonney2k, have a look at void CKernelMachine::store_model_features
 heiko	there, this method is called
@sonney2k	heiko, yeah I understand
 heiko	sonney2k, pull request updated
@sonney2k	heiko, but that copy* fucntion should be trivial for sparse
@sonney2k	it is the same like for strings...
 heiko	yes, should be simple
@sonney2k	(mor or less :)
@sonney2k	more
 heiko	but has to be implemented :)
@sonney2k	yes yes
@sonney2k	ok then I would say finish distance, the sparse copy and then it would be very very good if you could help getting serialization more cross-version compatilbe
@sonney2k	so here is the problem we have:
@sonney2k	all these m_parameters->add() stuff registers the variables to be serialized
@sonney2k	now that is all good and works
@sonney2k	however in shogun version+1 we might add a new variable, like in this case subset :D
 heiko	yes
@sonney2k	suddenly older objects can not really be loaded (well they could but issue a warning)
@sonney2k	so the plan would be to store a version in addition
@sonney2k	so add a int version to the m_parameter->add() call
 heiko	and then to check version upon deserialization
@sonney2k	and each object then has a separate version nr defined that is then just passed
@sonney2k	heiko, yes, so it means we load only things of that specific version even we are in a newer version object
@sonney2k	so that would solve the problem of *additions*
@sonney2k	now we have the problem of type changes and renames too
 heiko	yes
@sonney2k	for example we have lots of changes that do double* vector, int len -> SGVector vec
 heiko	yes
 heiko	already an idea for that?
@sonney2k	I am not 100% sure yet how to properly fix this but I think there is no other way than providing some transition table
 heiko	ok
 heiko	tricky
@sonney2k	i.e. in that table there would be the old variable names registered and a transition function that returns the new one
 heiko	and this table has to be updated when someone changes a variable
 heiko	or should this go automatically?
@sonney2k	so e.g. old_names = {vector, len} new_name= vector , transition function = transform_double_len_to_sgvector()
@sonney2k	heiko, this cannot go automatically
 heiko	ok
@sonney2k	we would have to update that for all classes such until the whole test suite runs trough again
 heiko	but these functions are only for serialization
 heiko	or deserialization of old data
@sonney2k	(test suite currently is the tester.py in testsuite)
@sonney2k	yes
 heiko	i will have alook
@sonney2k	deserialization only
@sonney2k	in serialization we write things out only in the newest format I would say... not sure if this is good - very much M$ Word style...
 heiko	puh, that is a lot of stuff
@sonney2k	heiko, lets start with the low hanging fruits, that is additions
 heiko	ok
 heiko	and the version id
@sonney2k	yeah I think this can be solved via the version id
 heiko	ok
 heiko	I will probably start on this tommorrow and then bother you with my problems :)
@sonney2k	heiko, btw in line 146 you can use SGVector<float64_t>(k)
@sonney2k	this will alloc a vector of len k
@sonney2k	heiko, yeah
 heiko	sonney2, another question:
 heiko	there are many feature classes that do not support subset or model selection
 heiko	this is because the inheritance sturcture
 heiko	basically there are only three classes, and the things work for these
 heiko	but for all these specializations, the methods are not implemented
 heiko	for example all dot features
 heiko	because in the class DotFeatures itself, it is not possible to implement the missing methods
 f-x	sonney2k: sorry for persisting, but the 'if (z < 1)' between the #if-#endif should function something like 'if (z < 1 || loss->is_log_loss())' right? i'm not understanding where this needs_extra_update flag would go
@sonney2k	heiko, it should be do-able in dotfeatures too - problem is that this needs another change in the features beneath, like dot() would call compute_dot() with the right subset
@sonney2k	heiko, so lets postpone that for now.
 heiko	ok
 heiko	this will automatically detected if people encouter the "class XYZ" is not ready for model-selection yet :)
@bettyboo	heiko, haha
 heiko	(SG_ERRORS)
@sonney2k	f-x, it is not just for LOGLOSS but also for LOGLOSSMARGIN
 f-x	sonney2k: right.. so these two loss functions should have some common property we should be able to check for
 f-x	or we could have enum types for all loss functions and check for those enums
@sonney2k	f-x, that is why I was suggesting a needs_extra_update or so flag
 f-x	where would this be? in the SGD class?
 f-x	i didn't understand properly
@sonney2k	f-x, in the losses
@sonney2k	f-x, in the end you either create all losses in one file or multiple like you do (up to you)
 f-x	sonney2k: and they will be used only in SGD/SGD-QN?
@sonney2k	if in one file they would be in mathematics/*
 f-x	(the flag)
@sonney2k	(BTW there is already one loss thingy in there which should be modified/removed)
@sonney2k	f-x, yes
@sonney2k	if you do it in one file then you will have to use enums for selecting the loss
@sonney2k	otherwise classes - which is what you do now.
 f-x	sonney2k: but it wouldn't be good to modify loss functions for the sake of learning algorithms right?
 f-x	or will this flag be of use generally as well?
@sonney2k	f-x, how else would you solve that problem?
@sonney2k	the only other chance I see is to change the learning algorithm completely depending on loss
 f-x	define a global list of enums for all loss functions in some header file
@sonney2k	and then?
 f-x	and each loss function returns that enum
 f-x	check for that enum from SGD
 f-x	whether the enum is LOGLOSS or LOGLOSSMARGIN or whatever
@sonney2k	yes sure that is also fine
@sonney2k	this is what features/ preprocessors do
@sonney2k	kernels / distances too btw
 f-x	sonney2k: so where should i add the loss function enums?
@sonney2k	they all have an enum
@sonney2k	in Loss.h
 f-x	hmm right
 f-x	sonney2k: ok. sounds good, i'll do that.
 f-x	sonney2k: btw VW also adds a couple of methods to the loss functions it uses
 f-x	like get_update() and get_square_grad()
 f-x	(which are basically used mainly for VW)
 f-x	so i shouldn't put these into the loss function classes right? (coz they'll probably only be used by VW)
@sonney2k	f-x, put them in the losses
 f-x	and also I don't know how they'd look for any general loss function - I know it only for those loss functions used in VW
@sonney2k	they belong there because they do some extra stuff
@sonney2k	then return SG_NOTIMPLEMENTED for the other losses there
 f-x	sonney2k: okay.. that's nice.. i'll add them too.
@sonney2k	it is totally fine if not all losses support such functions
@sonney2k	(or not implemented as in this case)
 f-x	great.. we could implement them later.. by solving a recurrence relation in john's paper.. but i don't think I'll do it now.
 f-x	SG_NOTIMPLEMENTED is the way to go
 f-x	sonney2k: is the compilation fixed? or was it compiling for you already?
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
@sonney2k	f-x, I have an older gcc version so it always compiled here
@sonney2k	but I hope I fixed it yes
 f-x	ok.thanks! mine's 4.6.1.. and i'll report if it doesn't work here
 CIA-87	shogun: Soeren Sonnenburg master * rfd09670 / (2 files):
 CIA-87	shogun: Merge pull request #253 from karlnapf/master
 CIA-87	shogun: made KMeans serializable and SGVector replacement (+8 more commits...) - https://github.com/shogun-toolbox/shogun/commit/fd0967097e615f9f234f1a18c6269e89d57a2ab4
@sonney2k	alesis-novik, so can you avoid the memcpy stuff?
-!- blackburn [~blackburn@188.122.224.26] has joined #shogun
 heiko	sonney2k, I think, it makes sense to implement apply for any DistanceMachine, i.e. move the implementation from KMeans to DIstanceMachine
 heiko	but then, one has to ensure that every distance machine stores its cluster centers in the lhs of the underlying distance variable
 heiko	what do you think about this?
 heiko	then any distance machine would implement the apply method
 heiko	well, every distance machine that builds cluster centers in training
 heiko	KNN then would override apply by its own method
 blackburn	sonney2k: openmp? ;)
@sonney2k	heiko, ok
@sonney2k	blackburn, slow
 blackburn	sonney2k: why slow?
 blackburn	many things could be easily adapted for openmp because of #pragma notation..
 blackburn	is it really slow?
@sonney2k	blackburn, for simple things it is fast yes
 CIA-87	shogun: Heiko Strathmann master * r3ac5c53 / (2 files): another SGVector replacement and usage of CMath::sqrt instead of std::sqrt - https://github.com/shogun-toolbox/shogun/commit/3ac5c53a62eec98dd1d0b68a2dd80453755f9a1d
 CIA-87	shogun: Soeren Sonnenburg master * ra6586d5 / (2 files):
 CIA-87	shogun: Merge pull request #254 from karlnapf/master
 CIA-87	shogun: SGVector replacement - https://github.com/shogun-toolbox/shogun/commit/a6586d545c32c38ee414efd277d49f41bc8352a0
 blackburn	sonney2k: and when it is slow?
-!- heiko [~heiko@134.91.52.15] has quit [Ping timeout: 258 seconds]
@sonney2k	blackburn, in my attempts when I called functions in the parallelized pragma stuff
@sonney2k	blackburn, so plain for loops without functions should become faster...
 blackburn	so what we might use for multithreading?
@sonney2k	pthreads
-!- mrsrikanth [~mrsrikant@59.92.22.26] has joined #shogun
-!- in3xes_ [~in3xes@210.212.58.111] has joined #shogun
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
-!- mrsrikanth [~mrsrikant@59.92.22.26] has quit [Read error: Connection reset by peer]
-!- f-x [~user@117.192.222.125] has quit [Ping timeout: 260 seconds]
-!- in3xes_ is now known as in3xes
-!- srikanth [~mrsrikant@59.92.22.26] has joined #shogun
-!- srikanth [~mrsrikant@59.92.22.26] has quit [Quit: Leaving]
 alesis-novik	sonney2k, around?
 alesis-novik	Well, I did what you asked, and valgrind doesn't seem to complain, so that's good.
-!- f-x [~user@117.192.207.49] has joined #shogun
-!- in3xes_ [~in3xes@180.149.49.227] has joined #shogun
 f-x	sonney2k: hey! what kind of objects as a rule do you think should inherit from CSGObject?
-!- in3xes [~in3xes@210.212.58.111] has quit [Ping timeout: 240 seconds]
@sonney2k	f-x, all except those for which this would be too much overhead
 CIA-87	shogun: Alesis Novik master * rf8fc62c / src/shogun/clustering/GMM.cpp : Removed copying - https://github.com/shogun-toolbox/shogun/commit/f8fc62c7b365df87859be00ef74bde3c6d2b7cdd
 CIA-87	shogun: Soeren Sonnenburg master * r380af5a / (3 files in 2 dirs):
 CIA-87	shogun: Merge pull request #252 from alesis/gmm
 CIA-87	shogun: Memory problem fixes. - https://github.com/shogun-toolbox/shogun/commit/380af5acf4bba4d7cb226fd9dc90ab625b2ac149
@sonney2k	alesis-novik, well you are the master of your algorithm... as long as you don't destroy the vector under your feat you should be fine.
 alesis-novik	sonney2k, well, I think in this case we might still have a few variables floating around because the object isn't deleted. Nothing major though.
-!- in3xes_ is now known as in3xes
@sonney2k	alesis-novik, ok...
 alesis-novik	sonney2k, found another potential memory problem, committing.
 blackburn	alesis-novik: do you know what is gaussian naive bayes is?
 blackburn	I've been thinking about GNB+Gaussian integration
@sonney2k	alesis-novik, I suggest you compile with --trace-memory-allocs and check for leaks too :)
 alesis-novik	sonney2k, will do
 alesis-novik	what did you have in mind blackburn
 blackburn	alesis-novik: well now it uses gaussian pdf
 blackburn	may be it is even possible to fit gaussians for every class not only with diag cov
 blackburn	one issue with GNB now is that sometimes it leads to underflow or so
 blackburn	I mean for every class the probability becomes so small that decision is not correct
 alesis-novik	but how do you want to integrate it with Gaussian?
 blackburn	I'm not sure if CGaussian is exactly what I mean :)
-!- in3xes [~in3xes@180.149.49.227] has quit [Quit: Leaving]
 CIA-87	shogun: Alesis Novik master * r2d2fbf8 / src/shogun/clustering/GMM.cpp : added SG_UNREF where needed - https://github.com/shogun-toolbox/shogun/commit/2d2fbf8433c9bfb0621a046dc7408aa49f15c2d8
 CIA-87	shogun: Soeren Sonnenburg master * ra514bca / src/shogun/clustering/GMM.cpp :
 CIA-87	shogun: Merge pull request #255 from alesis/gmm
 CIA-87	shogun: added SG_UNREF where needed - https://github.com/shogun-toolbox/shogun/commit/a514bca4f7200429d1696953ca9d3cadacb80a5f
@sonney2k	alesis-novik, btw would it be possible to set the matrix of means etc for the gaussians in one go?
@sonney2k	now it seems one has to set multiple vectors
@sonney2k	alesis-novik, I mean you could just add as set_* function which takes an SGMatrix as argument and then call the respective SGVector functions multiple times...
 alesis-novik	sonney2k, that's because it's just calling the underlying CGaussian::set_mean(...)
@sonney2k	alesis-novik, yes but you can emulate that right?
@sonney2k	I mean split up the mean matrix etc
 alesis-novik	but what about the covariance one then?
@sonney2k	alesis-novik, covariance is for every gaussian right?
@sonney2k	I mean you have 1 per gaussian?
 alesis-novik	yes
@sonney2k	so in you GMM you would have multiple cov matrices ?
@sonney2k	then it doesn't make sense indeed
 alesis-novik	Well, every Gaussian in the mixture model has a mean and cov. While making a bulk set_means makes sense, I don't really think that a bulk set_covs using SG* would make sense
@sonney2k	alesis-novik, yes I agree so we keep it like it is then
-!- f-x [~user@117.192.207.49] has quit [Remote host closed the connection]
-!- serialhex [~quassel@99-101-148-183.lightspeed.wepbfl.sbcglobal.net] has quit [Ping timeout: 250 seconds]
-!- serialhex [~quassel@99.101.148.183] has joined #shogun
--- Log closed Mon Aug 01 00:00:11 2011
