--- Log opened Sun Apr 15 00:00:19 2012
-!- karlnapf [~heiko@host86-181-156-211.range86-181.btcentralplus.com] has quit [Quit: Leaving.]
-!- blackburn [~qdrgsm@109.226.90.219] has left #shogun []
-!- harshit_ [~harshit@182.68.181.141] has quit [Remote host closed the connection]
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Quit: leaving]
-!- wiking [~wiking@huwico/staff/wiking] has quit [Quit: wiking]
-!- wiking [~wiking@78-23-189-112.access.telenet.be] has joined #shogun
-!- wiking [~wiking@78-23-189-112.access.telenet.be] has quit [Changing host]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Quit: wiking]
-!- wiking [~wiking@78-23-189-112.access.telenet.be] has joined #shogun
-!- wiking [~wiking@78-23-189-112.access.telenet.be] has quit [Changing host]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@vpna079.ugent.be] has joined #shogun
-!- wiking [~wiking@vpna079.ugent.be] has quit [Changing host]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Quit: wiking]
-!- blackburn [~qdrgsm@109.226.90.219] has joined #shogun
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
 n4nd0	good morning!
 blackburn	hey
 blackburn	easter today here finally :D
 n4nd0	good :)
 n4nd0	is it holidays there too?
-!- naywhayare [~ryan@spoon.lugatgt.org] has quit [Ping timeout: 272 seconds]
-!- naywhayare [~ryan@spoon.lugatgt.org] has joined #shogun
 blackburn	n4nd0: yes kind of
 n4nd0	blackburn: bye bye to m_q then?
 blackburn	n4nd0: yeah I think so
 n4nd0	blackburn: should we wait and ask sonney2k?
 blackburn	I don't think so
 blackburn	n4nd0: can you imagine it is useful?
 n4nd0	blackburn: well it is just an heuristic to give more importance to the class of the points that are closer
 n4nd0	I think it actually makes sense
 blackburn	it would give ~ result as reducing k
 n4nd0	but to tell the truth I don't know if it makes a real difference in practice
 n4nd0	yes exactly
 blackburn	no need for that
 n4nd0	all right
 n4nd0	will drop it then ;)
 blackburn	please :)
 blackburn	hahahahah putin came with moscow's mayor instead of wife lol
 blackburn	http://www.youtube.com/watch?feature=player_detailpage&v=OGqCvTHrXmo
 blackburn	n4nd0: check this shameful cults full of gold and opulence
 n4nd0	yeah, I am watching it
 n4nd0	I don't understand that much though :P
 blackburn	n4nd0: just imagine how much money they spent on this golden shit
 blackburn	n4nd0: and there is funny story that putin dumped his wife - and we were laughing he bring mayor instead of his wife :D
 blackburn	it is ok to be gay lol
 n4nd0	haha
 blackburn	n4nd0: btw legislatives here are going to introduce law that punishes gays because of gay advocacy
 blackburn	how wild is it for you?
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
 blackburn	n4nd0: http://www.parashift.com/c++-faq-lite/containers.html#faq-34.1 we should adv it to soeren
 n4nd0	blackburn: yeah, we should do that!
 blackburn	n4nd0: btw do you like libs extraction?
 n4nd0	blackburn: what is it?
 blackburn	https://github.com/shogun-toolbox/shogun/pull/454
 blackburn	n4nd0: this one ^
 n4nd0	blackburn: aham, so to move code from another libraries to an specific directory?
 blackburn	yes
 n4nd0	blackburn: yes, I like it - I think the project is better ordered like that
 n4nd0	blackburn: is there any con?
 blackburn	I don't see any
 n4nd0	I agree then
 n4nd0	I should probably move the cover tree there
 blackburn	n4nd0: to avoid some merge issues better stay it there
 blackburn	and we will move it later
 n4nd0	ok
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
 n4nd0	blackburn: I like the article about containers
 blackburn	yeah this faq is nice
 blackburn	there are other faqs covering whole C++
 n4nd0	blackburn: I'd actually like to see more C++ style in the project
 blackburn	any other suggestions?
 n4nd0	I like the STL too
 blackburn	algorithms?
 n4nd0	and as I think I told you once and pluskid also suggested, smart pointers could be a good idea
 blackburn	smart pointers - yes, agree
 n4nd0	blackburn: for algorithms and for the containers
 blackburn	ohhh
 n4nd0	blackburn: to use iterators
 blackburn	I would like to see a good example of <algorithm> usage
 n4nd0	like how to use some of the functions there?
 wiking	blackburn:  :))))
 wiking	sonney2k: does not like STL afaik
 wiking	i've told 2 months ago to have iterators on various types like sgvector or sgmatrix would be great to see
 blackburn	n4nd0: yes I can't see because I do not think in the <algorithm> way :D
 blackburn	wiking: we have iterators on dot features
 n4nd0	we should buy a present to sonney2k
 n4nd0	http://www.amazon.com/Effective-STL-Specific-Standard-Template/dp/0201749629
 wiking	ahahhahaha
 blackburn	send one  for me please too
 n4nd0	haha
 wiking	lol man i've completely overseen this function set_combined_kernel_weight
 blackburn	okay nevermind I downloaded a torrent lol
 wiking	\o/
 n4nd0	blackburn: be careful man, logged chat :P
 blackburn	n4nd0: recall the country I am from
 blackburn	HEY THERE I AM DOWNLOADING TORRENTS
 n4nd0	in that book they recommend and talk a bit about for_each
 wiking	hehehehe
 wiking	<algorithm> is cool
 wiking	but then again
 wiking	it really depends
 n4nd0	I have never used myself it but it looks nice
 wiking	sometimes the whole STL thing is a bit like: "shooting on a bird with a cannon"
 wiking	so sometimes you are just better of with valarray
 wiking	it's much faster than any other STL container tyoe
 wiking	type
 wiking	so for example in case of SGVector having a valarray would be good
 n4nd0	wih valarray so mena a normal T*?
 n4nd0	I'm sorry I didn't understand properly :S
 n4nd0	and what about to use the array class in C++11 for that?
 wiking	mmm valarray by far the fastst
 wiking	fastest
 wiking	and simply have a vector like
 wiking	valarray<float64_t> vec
 wiking	and still you can have some funky operations on them
 wiking	with the help of apply function
 wiking	n4nd0: and the problem with array that its c++11
 wiking	so you'll have some problems with code portability
 wiking	while valarray is standard C++
 n4nd0	I had no idea about this valarray
 n4nd0	I think it is cool
 wiking	anyhow valarray is just a good container when you have SIMD operations on your machine... so like any normal cpu nowadays
 wiking	mmx, sse, sse2
 wiking	any gcc will be able to optimize your code for using SIMD
 wiking	with valarray
 wiking	but valarray is basically good for built-in types... like float, int etc
 wiking	it's not a good container for objects or pointers for objects
 wiking	basically it's not meant for that :))
 n4nd0	I see
 wiking	for that u use some standard STL container
 wiking	maaaan amazing :)
 wiking	done it :)
 wiking	\o/
 wiking	blackburn: what ya say about the valarray?
 blackburn	wiking: I heard it is something like bringing fortran speed to C++?
 wiking	ahahahahha
 wiking	you must have read stackoverflow
 wiking	:>
 blackburn	yeap :D
 blackburn	wiking: I need to override protected method of base of base class in derived. how do you think it should be done?
 wiking	so you want to override the function a in class c where c : b, b: a
 wiking	?
 wiking	a sorry this was stupid
 wiking	so you want to override the function x in class c where c : b, b: a and a.x ?
 wiking	so that x is defined in a...
 wiking	why would virtual not work?
 blackburn	wiking: exactly
 blackburn	wiking: it would work but it is not visible from c
 wiking	ah ok
 blackburn	I guess I need to declare it in b?
 wiking	well in case of c a class is simply a struct no?
 blackburn	wiking: damn that would be easy if it was public but I don't want to expose these methods :D
 wiking	mmm wait a second
 blackburn	however I need to redeclare it again in C
 wiking	how do you want to access that method at all in c?
 blackburn	wiking: isn't it possible?
 wiking	well afaik no
 wiking	i mean it's not going to create you function pointers or something
 wiking	and simple c does not understand the syntax a.function ()
 wiking	unless there's a function pointer there in struct a
 blackburn	wiking: but it will work in case of public virtual
 wiking	mmm have u tried?
 wiking	i mean i'll try it for you now :)
 blackburn	wiking: why not? I guess I did not describe it right
 blackburn	I need to redefine method of A in C
 wiking	yeah i got that
 wiking	i'm just curious how will it work at all in ansi c
 blackburn	the only problem is visibility
 wiking	while you are having classes
 wiking	so for example my compiler dies on this :)
 wiking	http://snipt.org/uhfL6#expand
 wiking	if u compile it with gcc
 wiking	as if you want to have c
 blackburn	what's this?
 wiking	well a very simple example trying to use a class in ansi c
 wiking	that this way it won't work
 wiking	so i'm wondering how do you want to call a class and it's method in ansi c
 blackburn	why ansi c?
 wiking	aah fuck
 wiking	:DDD
 wiking	blackburn: wiking: it would work but it is not visible from c
 wiking	:D
 blackburn	damn :D
 blackburn	wiking: no the problem is
 wiking	i've got it
 wiking	what's the problem now
 wiking	why wouldn't it inherit
 blackburn	I have protected virtual method in A
 wiking	yes i got that now
 blackburn	will it be available from C?
 wiking	i wonder why isn't it getting as well down to C
 blackburn	hmm let me check again
 wiking	class friend is not transitive i know
 wiking	but inheritance of a fund...
 wiking	function..
 wiking	works for me
 wiking	blackburn http://snipt.org/uhfM8#expand
 blackburn	hmm fine thanks
 wiking	what was the problem?
 blackburn	wiking: currently I have no idea if there was a problem :D
 wiking	ah ok
 wiking	anyhow this what you've told should work
 wiking	even if it's not redefined in class B
 blackburn	wiking: yes it seems so :)
 blackburn	wiking: btw did you postponed latent stuff a little?
 wiking	blackburn: no why?
 blackburn	just curious :)
 wiking	blackburn: working on it...
 wiking	just had a chat with alex the other day
 wiking	we are finally on the same page
 wiking	as obviously from our emails we had a misunderstanding
 blackburn	I see
 wiking	now we 'realized' that yes actually we need the LatentFeatures
 wiking	class
 blackburn	hah
 blackburn	wiking: btw where did you place your classes?
 wiking	otherwise we couldn't optimize the latent part
 wiking	blackburn: well now they are still there where i've committed them first
 blackburn	I suggest latent both for latentfeatures and other latent stuff
 blackburn	shogun/latent/
 blackburn	I mean
 wiking	aha
 wiking	so you want to move everything under shogun/latent
 wiking	at one place?
 wiking	even LatentLabel and LatentFeature ?
 blackburn	wiking: why not?
 wiking	i'm just asking
 blackburn	I think it is a domain for that :)
 wiking	it's a bit confusion
 blackburn	do you think so?
 wiking	i mean even to have the features and the labels
 wiking	i got the point for having the machine separated
 blackburn	n4nd0: what do you think about that ^?
 n4nd0	blackburn: I have to go back in the conversation
 blackburn	my opinion is that base stuff should be as is but domains are separated
 wiking	although maybe it'  should be something like
 wiking	shogun/classifier/latent
 wiking	just as in case of mkl
 blackburn	n4nd0: do you think all the latent stuff should go shogun/latent?
 wiking	or?
 n4nd0	blackburn: I am correcting a Spanish easy for my gf and I lost part of the conversation
 wiking	n4nd0: :DDD
 blackburn	:D
 blackburn	n4nd0: no need to dig into whole conversation :)
 blackburn	wiking: can't it be multiclass?>
 wiking	well it should be
 blackburn	regression?
 wiking	for me otherwise it's not much of a use
 blackburn	or even structured output? :)
 wiking	yep
 wiking	yep
 wiking	structured output
 blackburn	if so I think it should go to shogun/latent
 blackburn	because it won't be 'classifier' in standart mean
 blackburn	however let us discuss it later with soeren
 wiking	ok
 wiking	let me know
 wiking	i'll continue as i've started earlier
 blackburn	yes it is not hard change so
 blackburn	no need to worry :)
 wiking	yeah just a git mv
-!- wiking_ [~wiking@78-23-189-112.access.telenet.be] has joined #shogun
-!- wiking_ [~wiking@78-23-189-112.access.telenet.be] has quit [Changing host]
-!- wiking_ [~wiking@huwico/staff/wiking] has joined #shogun
-!- wiking [~wiking@huwico/staff/wiking] has quit [Ping timeout: 265 seconds]
-!- wiking_ is now known as wiking
-!- wiking [~wiking@huwico/staff/wiking] has quit [Remote host closed the connection]
-!- wiking [~wiking@vpna032.ugent.be] has joined #shogun
-!- wiking [~wiking@vpna032.ugent.be] has quit [Changing host]
-!- wiking [~wiking@huwico/staff/wiking] has joined #shogun
 n4nd0	blackburn: why LatentFeatures wouldn't be ok in shogun/features?
-!- pluskid [~pluskid@1.204.107.190] has joined #shogun
 blackburn	n4nd0: it would but it makes sense to store domain-specific things in separate folder (for me)
 n4nd0	blackburn: yes, it actually makes sense too
 n4nd0	blackburn: I guess that in this case, LatentFeatures, it is a matter of taste
 n4nd0	blackburn: it probably makes sense both under latent/ and under shogun/features
 pluskid	I moved MulticlassSVM.{h,cpp} to multiclass folder, but why I always get: *** No rule to make target `../../shogun/classifier/svm/MultiClassSVM.h', needed by `modshogun_wrap.cxx'.  Stop.
 pluskid	I can't find where the path is specified
 blackburn	pluskid: find shogn | xargs grep svm/MultiClassSVM.h
 blackburn	shogun*
 blackburn	find shogun*
 blackburn	:D
 n4nd0	blackburn: deleting .o would solve that?
 blackburn	n4nd0: yes and .cxx. but only in the case that MultiClassSVM is not referenced by some include
 blackburn	pluskid: did you change include in multiclasslibsvm?
 pluskid	I changed a lot
 pluskid	btw, the refactoring is a huge project
 blackburn	pluskid: did you 'make clean'?
 pluskid	there are dozens of multiclass svm subclasses
 pluskid	and even GUI staffs are related
 pluskid	yes I did make clean
 pluskid	I'll try again
 blackburn	Scatter, LaRank, GMNP, LibSVM
 blackburn	MKLMulticlass
 blackburn	ah no
 blackburn	shogun/classifier/svm/ScatterSVM.h:#include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	shogun/classifier/svm/LibSVMMultiClass.h:#include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	shogun/classifier/svm/MultiClassSVM.cpp:#include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	shogun/classifier/svm/GMNPSVM.h:#include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	shogun/classifier/svm/LaRank.h:#include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	pluskid: ^
 pluskid	I see
 pluskid	thanks
 blackburn	interfaces/modular/Classifier_includes.i: #include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	interfaces/modular/Classifier.i:%include <shogun/classifier/svm/MultiClassSVM.h>
 blackburn	pluskid: so are you unhappy doing it ? :)
 pluskid	blackburn, I'd rather not give up at this moment
 pluskid	:p
 blackburn	pluskid: it requires some careful work but it is really needed
 pluskid	blackburn, yes, I see
 pluskid	there are lots of duplicated/similar code in MulticlassSVM
 pluskid	and the new multiclass facility
 blackburn	pluskid: similar to multiclass machine?
 blackburn	yes
 blackburn	pluskid: actually you may remove it
 pluskid	MulticlassSVM?
 blackburn	no
 pluskid	I don't think so, too :)
 blackburn	pluskid: I mean duplicated stuff
 pluskid	yes
 blackburn	pluskid: just derive it from kernel multiclass machine
 blackburn	and add there required methods
 pluskid	yes, that's what I'm doing
 pluskid	:)
 blackburn	pluskid: did you catch the idea of multiclass stuff?
 blackburn	i mean that generic thing
 pluskid	I think yes
 blackburn	while GMNP/LaRank/Scatter are all OvR
 blackburn	it would be easy
 pluskid	but the generic multiclass training procedure is much different from our multiclass-svms
 pluskid	it seems
 blackburn	it should be overrided with svms
 pluskid	so GMNP/LaRank/Scatter all cannot support OvO?
 blackburn	pluskid: no
 blackburn	GMNP is modified Weston-Watkins formulation
 blackburn	and LaRank is primal Crammer-Singer
 blackburn	so no way to do OvO
 n4nd0	see you later guys
 pluskid	didn't know those methods
 blackburn	pluskid: for example in gmnpsvm all you need is to change set_svm part
 pluskid	cu n4nd0
 blackburn	n4nd0: see you
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Quit: leaving]
 pluskid	yes
 blackburn	I mean training will stay
 blackburn	all you need is to set machines
-!- ishaanmlhtr [~chatzilla@101.63.111.140] has joined #shogun
 pluskid	currently, I'm using some regression test to assure the result before and after refactoring is the same
 pluskid	ensure
 blackburn	nice
 blackburn	pluskid: the formulations are
 blackburn	C-S: \sum_m ||w_m||^2 + C \sum_i \xi_i      s.t.    <w_yi,x_i>-<w_m,x_i>\geq 1 - \delta_yi,m + \xi_i
 blackburn	W-W: \sum_m ||w_m||^2 + C!!! \sum_m !!! \sum_i \xi_i      s.t.    <w_yi,x_i>-<w_m,x_i>\geq 1 - \delta_yi,m + \xi_i
 blackburn	m = 0, ... , K  --- classes
 blackburn	i = 1, ... , N --- train vectors
 blackburn	the only difference is class slacks
 pluskid	what is C-S and W-W denoting?
 blackburn	Crammer-Singer
 blackburn	Weston-Watkins
 blackburn	http://www.cis.uab.edu/zhang/Spam-mining-papers/On.the.Algorithmic.Implementation.of.Multiclass.Kernel.based.Vector.machines.pdf
 pluskid	ah, thanks for the reference
 blackburn	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.9594
 blackburn	pluskid: W-W is rather infeasible but GMNP does a magic trick for that
 pluskid	magic? :D
 blackburn	it maps the problem to single-class one using kesler's transformation
 blackburn	yes kind of
 pluskid	hmm, lots of magic for me :D
 pluskid	I won't understand all those before reading the papers
 blackburn	ok have to go
 blackburn	pluskid: see you
-!- blackburn [~qdrgsm@109.226.90.219] has left #shogun []
 pluskid	cu
-!- PhilTillet [~Philippe@157.159.42.154] has joined #shogun
-!- ishaanmlhtr [~chatzilla@101.63.111.140] has quit [Quit: ChatZilla 0.9.88.2 [Firefox 9.0.1/20111220165912]]
-!- pluskid [~pluskid@1.204.107.190] has quit [Ping timeout: 260 seconds]
-!- pluskid [~pluskid@173.254.214.60] has joined #shogun
-!- Netsplit *.net <-> *.split quits: naywhayare
-!- Netsplit *.net <-> *.split quits: shogun-buildbot, PhilTillet, pluskid, CIA-64, @sonney2k, wiking, vikram360
-!- Netsplit over, joins: pluskid, PhilTillet, wiking, naywhayare, vikram360, shogun-buildbot, CIA-64, @sonney2k
-!- Netsplit *.net <-> *.split quits: naywhayare, pluskid, PhilTillet, shogun-buildbot, CIA-64, wiking, @sonney2k, vikram360
-!- Netsplit over, joins: pluskid, PhilTillet, wiking, naywhayare, vikram360, shogun-buildbot, CIA-64, @sonney2k
-!- blackburn [~qdrgsm@188.168.2.179] has joined #shogun
-!- pluskid [~pluskid@173.254.214.60] has quit [Quit: Leaving]
-!- blackburn [~qdrgsm@188.168.2.179] has quit [Ping timeout: 252 seconds]
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Ping timeout: 245 seconds]
-!- PhilTillet [~Philippe@157.159.42.154] has quit [Ping timeout: 265 seconds]
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
-!- PhilTillet [~Philippe@npasserelle10.minet.net] has joined #shogun
-!- flxb [~cronor@fb.ml.tu-berlin.de] has joined #shogun
 flxb	If I use a kernel normalizer and init the same kernel object with train data first and test data later, can i be sure the normalization is done properly? so that test and train data are normalized the same way?
 flxb	also, does shogun provide methods to center data in feature space?
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Quit: leaving]
-!- PhilTillet [~Philippe@npasserelle10.minet.net] has quit [Remote host closed the connection]
-!- blackburn [5bde8018@gateway/web/freenode/ip.91.222.128.24] has joined #shogun
 blackburn	flxb: 1) yes while normalizers are applied on-the-fly 2) PruneVarSubMean should work for that
 flxb	blackburn: PruneVarSubMean centers the input features, correct? Is there a way to center in kernel feature space?
 blackburn	flxb: uh you mean center kernel matrix?
 flxb	blackburn: yes
 blackburn	flxb: I have to ask you how big is your kernel?
 flxb	blackburn: 3k x 3k
 flxb	not very big
 blackburn	ah nevermind I find it
 blackburn	ZeroMeanCenterKernelNormalizer
 blackburn	(I was thinking about suggesting to compute whole matrix and center it)
 flxb	ahh, there is a normalizer for that? now how can i apply to normalizers? zero mean and avg diag
 blackburn	flxb: what do you mean?
 blackburn	you need both?
 flxb	yes
 flxb	i want to center the kernel and normalize it
@sonney2k	blackburn, hmmhh where do you draw the line with the 'external' patch?
 blackburn	sonney2k: what line? how to choose if lib is external?
@sonney2k	for example svmlight is also taken from other source, SVMLin too
 blackburn	sonney2k: ah I just was lazy to complete that
@sonney2k	libsvm is heavily modified, svmlight too
 blackburn	and wanted to show proof-of-concept
 blackburn	sonney2k: yes modifications are marked with shogun_*
 blackburn	flxb: I am afraid this is not really possible now
 blackburn	however I can suggest a trick for that
 blackburn	you may attach first normalizer and compute custom kernel with it
 blackburn	and the attach second to the custom kernel
 flxb	blackburn: yes this is a good idea
@sonney2k	blackburn, ok
 blackburn	sonney2k: do you like it?
@sonney2k	I don't really understand what this is for though?
@sonney2k	what does it help?
 blackburn	sonney2k: better structure
 blackburn	I was (and others were) confused with a lot of .h and .cpp in svm folder
 blackburn	another issue is cross-dependencies of stuff like ocas
 blackburn	multiclass ocas was referencing to classifier/svm/libocas
 flxb	blackburn: now, i would do: k = GaussianKernel(), k.set_normalizer(), k.init(train,train), k_tr = k.get_kernel_matrix(), k.init(train, test), k_te = k.get_kernel_matrix(). Would this work? I'm worried that train and test kernel have a different normalization afterwards...
@sonney2k	blackburn, yeah but isn't it more natural to include shogun/classifier/svm/Tron.h than sth in lib/external ?
@sonney2k	for users of libshogun I mean
 blackburn	I don't know - they should know what tron is
 blackburn	flxb: should work I think however that can be different.. sonney2k can you help out there?
@sonney2k	flxb, yes that will work init is done based on lhs only if there is some train data specific stuff to do
 blackburn	wow farbrausch code
 flxb	sonney2k, blackburn: cool thanks you two! i will try it out
 blackburn	I always wanted to check this demoscene code
@sonney2k	blackburn, yeah crazy... and he blogged about some cooperative garbage collector significantly simplifying code w/o speed loss
@sonney2k	and no longer incref / decref :)
@sonney2k	s/he/chaos/
 blackburn	sonney2k: these guys can teach us a lot of things :D
@sonney2k	blackburn, regarding lib/external I have a different opinion: I think we should put everything in there that is not meant to be seen from the outside
@sonney2k	but not the things that make sense to be directly included
@sonney2k	for example: if we have a severely modifed CLibSVM class that is meant to be directly used then it should stay in the hierarchy
@sonney2k	the SVM_libsvm.{cpp,h} stuff should be in lib/external
 blackburn	sonney2k: could you please provide example what should stay and what should go to lib/external?
@sonney2k	that was the example ^
 blackburn	uhoh what's with me
 blackburn	I can't understand :D
@sonney2k	CTron / LibLinear would stay
 blackburn	why?
@sonney2k	libqp would go
 blackburn	because of serious modifications?
@sonney2k	it is CLibLinear now
@sonney2k	SVM_linear* would go
 blackburn	sonney2k: ok is there any library I moved that should go back?
@sonney2k	Tron
 blackburn	sonney2k: because of?
 wiking	has there been any decision now on where the latent stuff should reside?
 blackburn	wiking: no but thanks for remind :)
 wiking	heheh sonney2k read logs ;)
 blackburn	sonney2k: I mean Tron is not visible from the outside and it is external code, why should it stay here
-!- gsomix [~gsomix@188.168.128.177] has joined #shogun
 gsomix	hi there
@sonney2k	blackburn, it is a CSGObject derived object that can be used from external code
 blackburn	sonney2k: makes sense then
@sonney2k	wiking, ?
 blackburn	sonney2k: ok another issue - do you agree all the latent stuff
 blackburn	should go to shogun/latent
 blackburn	latentfeatures,latentlabels,anything
 wiking	sonney2k: blackburn is just explaining it...
@sonney2k	blackburn, why not have it under features and classifier as normal?
@sonney2k	I mean we don't have shogun/dot or anything
 blackburn	sonney2k: I think that domain specific separation is nice
@sonney2k	so a few more classes dont' hurt
@sonney2k	latent? no? domainadaptation -> yes
 blackburn	so for transfer learning stuff you agree?
@sonney2k	yes - there is a sh*tload of algorithms for that
@sonney2k	so it makes sense to have them in separate dir
 blackburn	shitload? it seems you like transfer learning :D
@sonney2k	no
@sonney2k	it usually doesn't gain you anything
@sonney2k	(like MKL)
 blackburn	sonney2k: 'like'
@sonney2k	there are exceptions to the rule but the costs are exorbitantly massive
 wiking	sonney2k: great to see that you even had a JMLR article about it ;P
 blackburn	sonney2k: but chris reported some gain ;)
 blackburn	with tree stuff
-!- harshit_ [~harshit@182.68.246.67] has joined #shogun
@sonney2k	wiking, 2 even!
 wiking	and you are 'trashing' it on irc :)))
 blackburn	sonney2k: that is something funny :)
 blackburn	sonney2k: I am afraid to ask what do you think about ocas?
@sonney2k	wiking, in the first article we just developed a faster algorithm to solve MKL
 blackburn	and cutting planes in general?
@sonney2k	and we used it to understand the classifier
@sonney2k	not to improve learning result - which mkl only rarely does
@sonney2k	in second article (Lp norm mkl) we managed to improve some state-of-the-art results *slightly*
 wiking	:))))
 wiking	and that's still worth a jmlr article :)
 wiking	so nice
 wiking	i really need one journal article soon :)
 blackburn	and my $#@$@ work doesn't worth :E
@sonney2k	the papers have a lot more than that
 wiking	heheheh yeah i guessed
@sonney2k	but that is the essence of MKL
-!- PhilTillet [~Philippe@npasserelle10.minet.net] has joined #shogun
@sonney2k	if you want to use it - try L2-norm MKL
 wiking	i've treid
 wiking	tried... was giving really bad results :(
@sonney2k	that will always improve or stay the same (at least on data I had)
@sonney2k	smells like a bug then
@sonney2k	wiking, I guess you used multiclass? no idea if alex binders implementation is well tested
 wiking	dunno really i've just tried MKL on a dataset... i'm getting 7-8% worse results (in accuracy) than with MKL
@sonney2k	we did only binary
 wiking	i was mening
@sonney2k	parse error...
 wiking	meaning worse results than with simple combined kernel
 wiking	and GMNP
 wiking	gmnp with combined kernel is better than mkl
 wiking	the only thing is that if i want the 'best' result than i have to do a parameter search on the weights for the various kernels
@sonney2k	wiking, the (unweighted) combined kernel is *very hard* to beat
 flxb	why is it called set_full_kernel_matrix_from_full and not simply set_kernel_matrix? does it do something special?
 blackburn	sonney2k: !! I think we should remove m_q
 blackburn	totally useless
 blackburn	flxb: it can be set full from triangle
 blackburn	and triangle from full
 blackburn	some cases there
 flxb	ah
 blackburn	sonney2k: no I shall convince you to remove q
-!- puffin444 [230bf329@gateway/web/freenode/ip.35.11.243.41] has joined #shogun
 puffin444	Hi
 blackburn	hey
 puffin444	How's it going?
@sonney2k	blackburn, why is it useless?
 blackburn	sonney2k: it is VERY similar to reducing k
 blackburn	sonney2k: small q leads to k=1 virtually
 blackburn	it is not of much interest when you do knn
@sonney2k	blackburn, why is the buildbot failing?
 blackburn	I didn't know it is failing :D
@sonney2k	blackburn, http://shogun-toolbox.org/buildbot/builders/nightly_none/builds/210/steps/compile/logs/stdio
@sonney2k	puffin444, strong currents currently :)
@sonney2k	blackburn, Re q I don't see this as a problem
 blackburn	sonney2k: hmm I thought I fixed that
 blackburn	sonney2k: it is useless
@sonney2k	blackburn, so q>=1 has no effect on solution right?
 blackburn	sonney2k: q>=1??
@sonney2k	except if distance < 1
 blackburn	it will be farest neighbor
 blackburn	:D
 blackburn	sonney2k: that is not related to distances
 blackburn	while it is a multiplier
* sonney2k checks the formula
 blackburn	sonney2k: with q=1 it +1 to class histogram
 blackburn	with q=0.5 it +0.5, +0.25, ... to class histogram
@sonney2k	ahh so points far away get less influence
 blackburn	yes sounds consistent but it makes no sense in practice
@sonney2k	why not?
 blackburn	better use parzen windows for that
@sonney2k	that is not really a reason
 blackburn	I don't think this kind of weighting makes sense
@sonney2k	I do
 blackburn	sonney2k: and it makes things harder while covertree do not place vectors in order
@sonney2k	yeah but that doesn't really matter
@sonney2k	I mean we can just have a speed up for q=1
@sonney2k	and otherwise default to what we had
 blackburn	sonney2k: ah okay it is hard to convince you :)
@sonney2k	which we should keep anyways
@sonney2k	blackburn, well you implemneted this and it sounds like it makes a lot of sense to me
 blackburn	sonney2k: I still think it is a stupid heuristics
@sonney2k	it protects you a bit from outliers - so I could imagine higher k to be useful in k-NN (I'ver never seen k>7 or so to be useful for q=1)
 blackburn	sonney2k: with k=10 and q=0.5 farest vector will have weight 1e-5 :)
@sonney2k	with C=1e-5 SVM alphas will be mostly 0
 blackburn	it is the same like there was no vector at all
@sonney2k	obviously 0.5 is an extreme choice already...
 blackburn	0.9? 0.99?
 blackburn	almost no influence at all
@sonney2k	0.9**10 -> 0.34
@sonney2k	anyways
@sonney2k	what to do with harshits TSVM?
 blackburn	sonney2k: ask harshit_ :D
@sonney2k	blackburn, what I meant is have you looked at the code?
@sonney2k	PhilTillet, I think for nearest centroid it doesn't make sense to have a store_model_features method at all
 blackburn	sonney2k: no while he said it is incomplete
@sonney2k	I mean you have to store the centroids
@sonney2k	that is what was meant with store model features
 PhilTillet	hmm okay, and I have no real choice but to store them during training :p
@sonney2k	in the context of e.g. SVM it makes sense to not copy all the SVs (because there can be many and they can be huge) but just their indices
 PhilTillet	yep
 PhilTillet	got it
@sonney2k	so store_model_features has a meaning :)
@sonney2k	PhilTillet, ok so please put the call to kernel init back in and all good :)
@sonney2k	PhilTillet, another thing - you assume that you are getting CSimpleFeatures - but you should check (at least add some ASSERT()'s checking for get_feature_class() = CT_SIMPLE())
@sonney2k	PhilTillet, ahh and I am wondering where m_shrinking is used?
 PhilTillet	Ah, I had a question concerning that :D (not yet implemented)
 PhilTillet	should the centroids be shrunken after training, or shrunken when classifying?
 PhilTillet	first option make classification faster but second option avoid re-training when changing m_shrinking
 PhilTillet	so I don't really know :p
@sonney2k	PhilTillet, I guess doing both would resolve the conflict :)
@sonney2k	i.e. do it in training
@sonney2k	and add some apply_shrinked()
@sonney2k	function that would do the shrinking would do
 PhilTillet	ok:p
 gsomix	good night, guys
-!- flxb [~cronor@fb.ml.tu-berlin.de] has left #shogun []
@sonney2k	gsomix, nite
 puffin444	Is it night for most people here?
 wiking	sonney2k blackburn so we have a consensus that the latent stuff will go to the usual places, i.e. latentfeatures and latentlabel class under shogun/features and the latentlinearclassifier under classifier/svm/ ?
@sonney2k	wiking, yes
 wiking	ok cool
@sonney2k	puffin444, yes 22hrs  here and 24hrs in russia :)
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has joined #shogun
@sonney2k	hi n4nd0
 n4nd0	hey sonney2k
 n4nd0	how is it going?
 n4nd0	and hi to everyone else too :)
 puffin444	Hey
@sonney2k	n4nd0, just had a look at your patch - nice job :)
@sonney2k	puffin444, can we get some nice shiny graphical example for your GP implementation - I think olivier has sth in his (python based) toolbox :)
 n4nd0	sonney2k: thanks! I have your messages, I will start making the modifications and continue with the job this next week
-!- blackburn [5bde8018@gateway/web/freenode/ip.91.222.128.24] has quit [Ping timeout: 245 seconds]
 puffin444	That's something I could try. Is there visualiztion stuff in the Python extensions?
@sonney2k	puffin444, not really - just use the excellent matplotlib - it's gallery/examples certain have what you need :)
 puffin444	Sounds good.
 n4nd0	I think there is a nice plot in scikits for GPs
 n4nd0	puffin444: you could use it as inspiration
 puffin444	I'll take a look at it.
 n4nd0	http://scikit-learn.org/0.10/modules/gaussian_process.html
@sonney2k	n4nd0, heh - they certainly have some nice documentation
@sonney2k	I don't understand why no one here wants to document things
 n4nd0	sonney2k: you mean within the code or in web format?
@sonney2k	even though it is much easier just to write a little doxygen code no one writes complex formulas or anything in the class descripton
 n4nd0	sonney2k: aham I see
 n4nd0	personally I like more something like the one in scikits
 n4nd0	I think they do it with sphinx
 n4nd0	sonney2k: would you like to see that in shogun?
@sonney2k	n4nd0, the problem is more that someone has to write all this stuff
@sonney2k	*and* keep it up-to-date
 n4nd0	maybe we could start with it step by step
 harshit_	sonney2k: hey, TSVM is not complete for now. I think there is some problem with tsvm part of ssl.cpp, which currently I am figuring out.
 harshit_	also due to university tests i am not able to give much time so hopefully that would be done by next sunday
 puffin444	Yeah it looks pretty, but I don't know how much time a scikit-like graph would take.
 n4nd0	puffin444: do you mean to code it?
@sonney2k	harshit_, ok np! university tests are certainly more important!
 puffin444	Yeah I don't know if I can code this week or not.
 puffin444	*code it*
 n4nd0	puffin444: aham, all right
 n4nd0	puffin444: anyway, in case you didn't see it, here it is the code to do that figure
 n4nd0	http://scikit-learn.org/0.10/auto_examples/gaussian_process/plot_gp_regression.html
 n4nd0	puffin444: if the method you have implemented in shogun has a similar interface to the one there, you wouldn't need to change much more than the actual part to train the GP
 n4nd0	all the plot stuff is there
 n4nd0	you can change the colours, the function to predict and you have there a complete new example of your own :)
 puffin444	Oh I see.
 n4nd0	puffin444: it is just a suggestion of course, I hope you don't mind I said it!
 puffin444	No that's fine. Yes this might work. My time is just somewhat limited as I have a major exam next week.
-!- blackburn [5bde8018@gateway/web/freenode/ip.91.222.128.24] has joined #shogun
 blackburn	n4nd0: have you seen sonney2k wants q in knn?
 n4nd0	blackburn: yeah
 n4nd0	blackburn: I told you it could be better to wait ;)
 n4nd0	blackburn: no way to convince him about it?
 blackburn	n4nd0: I am still think it should go :D
 blackburn	sure*
 blackburn	n4nd0: I don't know probably no way :D
 n4nd0	haha ok, let's keep it then
 blackburn	I am sorry I said it should go
-!- vikram360 [~vikram360@117.192.163.162] has quit [Ping timeout: 276 seconds]
 n4nd0	blackburn: no problem man! no need to say sorry ;)
@sonney2k	indeed no need to be sorry
@sonney2k	nite guyes!
 puffin444	bye!
 n4nd0	good night
 puffin444	I actually have to go too.
 puffin444	I'll see what I can do with GP visualization.
 n4nd0	cool
-!- puffin444 [230bf329@gateway/web/freenode/ip.35.11.243.41] has left #shogun []
-!- blackburn [5bde8018@gateway/web/freenode/ip.91.222.128.24] has quit [Quit: Page closed]
 n4nd0	another that abandons, good night people!
-!- n4nd0 [~nando@s83-179-44-135.cust.tele2.se] has quit [Quit: leaving]
-!- PhilTillet [~Philippe@npasserelle10.minet.net] has quit [Ping timeout: 264 seconds]
-!- PhilTillet [~Philippe@npasserelle10.minet.net] has joined #shogun
-!- PhilTillet [~Philippe@npasserelle10.minet.net] has quit [Remote host closed the connection]
--- Log closed Mon Apr 16 00:00:19 2012
