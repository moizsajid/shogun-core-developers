--- Log opened Thu Jul 28 00:00:38 2011
@sonney2k	uh
 serialhex	yeah, when you install it with a gem it installes it as a subdir in the gems dir
@sonney2k	then I guess you need some magic to get that detected too
 serialhex	yes, that is the plan for today, lots of magic!
 serialhex	and i shall become, THE ALL POWERFUL COMPATIBILITY WIZARD!!!! BUAHAHAHAHAHAHAAHAHAHAHAHAHAHAHAH1!!!!!!!!1!!!!!
 CIA-87	shogun: Soeren Sonnenburg master * r1454579 / (7 files in 2 dirs):
 CIA-87	shogun: enable memory tracing when compiled with --enable-trace-mallocs
 CIA-87	shogun: just call CSGObject::list_memory_allocs() or list_memory_allocs() if
 CIA-87	shogun: you've include lib/memory.h - https://github.com/shogun-toolbox/shogun/commit/14545797611459782da964cb59559e79063f3464
 CIA-87	shogun: Soeren Sonnenburg master * r57066ac / (47 files in 9 dirs): Merge branch 'master' of github.com:shogun-toolbox/shogun - https://github.com/shogun-toolbox/shogun/commit/57066ac8ec89a73253a7a21e8d9f215efb9aae55
@sonney2k	blackburn, please try the new malloc tracing stuff!
 blackburn	sonney2k: I will, but now I'm messing with some lapack shit
@sonney2k	blackburn, it gives you a good feeling when you do list_memory_allocs() at the end of a script when it then says 0 objects alloc'd :)
@sonney2k	blackburn, I have a dejavu
 blackburn	why?
@sonney2k	didn't you say this yesterday and the day before and the day before the day before and ... too ?
 blackburn	yes I have regular sex with lapack
 serialhex	LOLZ blackburn
 blackburn	well as I said before - there are some fancy dsyevr I haven't tried before
* sonney2k senses blackburns lapackorgasm ahead
 blackburn	this time I got only segfault
 blackburn	sudo apt-get install valgrind
 blackburn	hmm got it
 blackburn	I got semi-lapack-orgasm
 blackburn	serialhex: I'm gradually sth .. OR I gradually sth?
 serialhex	i'm gradually sth
 serialhex	^ blackburn
 blackburn	thanks :)
@bettyboo	ha ha
 serialhex	so how ya been blackburn?  i've been away but you're still here :D
@bettyboo	:^)
 blackburn	sonney2k: when you was away? i'm fine, and you?
 blackburn	shhhhh
 blackburn	serialhex:
 blackburn	sonney2k: not you :D
@bettyboo	hihi
@sonney2k	pfff!
 serialhex	HAHAHA!!!
 serialhex	confusion EVRYWHER!!!
 blackburn	for everyone
 blackburn	feel free to confuse ;)
 serialhex	it's what i do best!! :)
 blackburn	I shall remove my test prints
 blackburn	FUCKED 205 50FUCKEY NOOOOOEFUCKED YEAHvector=[5.69951159312013189,193.850759879128816,858.82707863487451]
 blackburn	that's strange!
 blackburn	:D
 serialhex	wtf dude?  you alright??
 blackburn	hahaha
 blackburn	it is what my DSYEVR wrapper produce
 blackburn	for test :D
@sonney2k	blackburn is a confUSER
 blackburn	:D
* sonney2k is off to ZZZzzzz.....
@sonney2k	cu
 blackburn	cu
 blackburn	wow
 blackburn	factor of 3 speedup
* blackburn is lying
* blackburn is lying that he is lying
 blackburn	sonney2k: classicmds, 2000 examples: 75s for DSYEV (old), 27s for DSYEVR(new), 17s for Arpack
 blackburn	who's your daddy :D
 CIA-87	shogun: Sergey Lisitsyn master * r5286e94 / (2 files): Added wrapper for DSYEVR lapack routine - https://github.com/shogun-toolbox/shogun/commit/5286e9425b20f0f1dd89365208b8220ad2b4c562
 CIA-87	shogun: Sergey Lisitsyn master * r0238667 / src/shogun/preprocessor/ClassicMDS.cpp : Changed MDS solver to DSYEVR - https://github.com/shogun-toolbox/shogun/commit/0238667449394761a104560c33973a7c02c7975e
 CIA-87	shogun: Sergey Lisitsyn master * r8bc3d93 / src/shogun/mathematics/lapack.cpp : Cleaned lapack wrappers - https://github.com/shogun-toolbox/shogun/commit/8bc3d93c5fbb28609d061a7140529e874e3aa7b6
-!- blackburn [~blackburn@109.226.76.87] has quit [Quit: Leaving.]
-!- sploving1 [~sploving@124.16.139.134] has joined #shogun
-!- sploving1 [~sploving@124.16.139.134] has left #shogun []
-!- gsomix [~gsomix@80.234.50.40] has joined #shogun
-!- f-x [~user@117.192.219.5] has joined #shogun
-!- f-x [~user@117.192.219.5] has quit [Ping timeout: 260 seconds]
-!- f-x [~user@117.192.203.28] has joined #shogun
-!- f-x_ [fx@213.155.190.134] has quit [Remote host closed the connection]
-!- blackburn [~blackburn@109.226.76.87] has joined #shogun
-!- gsomix [~gsomix@80.234.50.40] has quit [Ping timeout: 258 seconds]
-!- f-x [~user@117.192.203.28] has quit [Quit: ERC Version 5.3 (IRC client for Emacs)]
-!- blackburn [~blackburn@109.226.76.87] has quit [Quit: Leaving.]
-!- heiko [~heiko@134.91.52.218] has joined #shogun
-!- gsomix [~gsomix@88.200.214.229] has joined #shogun
-!- gsomix [~gsomix@88.200.214.229] has quit [Client Quit]
@sonney2k	heiko, try the new trace malloc stuff!
 CIA-87	shogun: Soeren Sonnenburg master * rd3602de / (5 files in 2 dirs):
 CIA-87	shogun: Merge pull request #244 from sploving/master
 CIA-87	shogun: add some ruby examples (+5 more commits...) - https://github.com/shogun-toolbox/shogun/commit/d3602de6dc7b55ba0bb8142cf700dcd0664c9f2d
 heiko	sonney2k, alright :)
@sonney2k	just call list_memory_allocs()
@sonney2k	and don't forget to enable it in configure
@sonney2k	--enable-trace-mallocs
@sonney2k	or call CSGObject::list_memory_allocs()
@sonney2k	heiko, it will tell you everything for SG_ALLOC'd memory (which line allocated / size)
@sonney2k	for SGObjects it will tell you how much you name, refcount, size and ptr
@sonney2k	for other objects just ptr & size
@sonney2k	it is pretty slick... this way I could check that examples really don't leak memory :)
 heiko	and are objects still created with new?
@sonney2k	yes
@sonney2k	heiko, and?
 heiko	sonney2k, just checked out
 heiko	how to activate the trace?
 heiko	compiling with trace mem allocs
@sonney2k	./configure  --enable-trace-mallocs
@sonney2k	and then CSGObject::list_memory_allocs()
@sonney2k	heiko, and?
 heiko	smooth :)
 heiko	works perfect
@sonney2k	heiko, yeah great isn't it?
@sonney2k	this will really help fixing memory leaks
 heiko	really cool
 heiko	yes i think so
 heiko	sad that i didnt have it while building the model tree stuff :)
 heiko	sonney2k, i found some vector related methods in CMath
 heiko	fill_vector range_fill_vector
@sonney2k	heiko, it was there - but not as good
 heiko	these should be with SGVector right?
 heiko	and also they iterator over the arrays (fill_vector)
 heiko	and clone_vector also
 heiko	instead of using memset/memcpy
 heiko	should i change this?
@sonney2k	heiko, yeah makes sense - however it would make sense to have them static in there too to just fill memory
@sonney2k	heiko, there is a lot of low-hanging fruits in there
 heiko	and another question: if i introduce the methods with SGVector, should I delete the old ones? or add a SG_DEPRECATED
 heiko	or just leave them?
@sonney2k	like max()
@sonney2k	min()
@sonney2k	etc
@sonney2k	mean
@sonney2k	all these simple operations...
@sonney2k	it is quite a bit of work though
@sonney2k	heiko, first add tehm to sgvector
@sonney2k	then transition all code and then remove all these max/min/fill_vector/mean etc stuff from CMath
 heiko	sonney2k, how to enable print of dSG_DEBUG messages
@sonney2k	obj.io.set_loglevel(MSG_DEBUG)
-!- blackburn [~blackburn@109.226.76.87] has joined #shogun
@sonney2k	hi papa blackburn
 blackburn	:D
@bettyboo	;D
 blackburn	I guess now our mds is the fastest one ;)
-!- blackburn [~blackburn@109.226.76.87] has left #shogun []
-!- blackburn [~blackburn@109.226.76.87] has joined #shogun
 blackburn	sonney2k: how can I realloc feature matrix?
 blackburn	get it and then just realloc?
@sonney2k	blackburn, I would suggest to add a function that re-allocs
-!- blackburn [~blackburn@109.226.76.87] has quit [Ping timeout: 255 seconds]
 heiko	sonney2k, I would like to add a method display_string to CMath, is that ok?
@sonney2k	heiko, shouldn't that be in SGString too?
 heiko	mmh
 heiko	true
 heiko	but same should then hold vor vector and matrix or?
 heiko	btw currently there is no difference between SGVector and SGString
 heiko	but will be if display methods are in these types
-!- alesis-novik [~alesis@188.74.87.206] has joined #shogun
-!- f-x [~user@117.192.221.29] has joined #shogun
 alesis-novik	Hey sonney2k, sorry about missing the meeting yesterday. I read the log so I'm up to date on what's what
-!- blackburn [~blackburn@109.226.104.206] has joined #shogun
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
-!- in3xes [~in3xes@180.149.49.227] has quit [Read error: Connection reset by peer]
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
-!- VojtechFranc [~quassel@gw-101.scnet.cz] has joined #shogun
 alesis-novik	Hello VojtechFranc
 VojtechFranc	hi alesis-novik
 VojtechFranc	what is the state?
 alesis-novik	Well, I was mostly working on how to fix some of the potential memory leaks
 alesis-novik	though I don't think there's a clear way of doing that, when using SG* stuff
 VojtechFranc	ok, my plan for the following weeks is the following
 VojtechFranc	first of all,  Aug 16 is suggested pencils down date
 VojtechFranc	1. could you please prepare example scripts in python demonstrating use of SMEM
 VojtechFranc	2. We need document functions you implemented.
 VojtechFranc	3. We need to test/benchmark EM and SMEM and fix potential bugs
 alesis-novik	VojtechFranc, as in more examples? There is a 2d smem example showing it's superiority over EM
 VojtechFranc	regarding task 3, I will send you a text file describing the experiments I'd like you to implement
 VojtechFranc	regarding examples, is the script included in shogun?
 alesis-novik	yes, it's under undocumented/python_modular/graphical (I think that's the path)
 VojtechFranc	ok then, I'll have a look at it
 VojtechFranc	I will also send you a brief minutes of the yesterday meeting as Soeren suggested some tasks for all shogun developers
 alesis-novik	look for smem_2d_gmm.py
 VojtechFranc	ok
 VojtechFranc	now, the most urgent is doign the experiments/benchmarks
 alesis-novik	I've read through the logs, so I'm more or less aware of the plan
 alesis-novik	ok, just send me the experiments/benchmarks you had in mind and I'll run them.
 alesis-novik	are we benchmarking run time?
 VojtechFranc	yes
 VojtechFranc	runtime,  likelihood, and if it makes sense application dependent statistics like classification error etc
 alesis-novik	ok, that makes sense
 VojtechFranc	I'll send you the experiment description + link to data today.
 VojtechFranc	I'll try to do exactly the same experiments with my implementation in order to compare the results
 VojtechFranc	One quation, do you have an experience with Open CV?
 VojtechFranc	the thing is that OpenCV which is a major CV library has also an efficient EM implemenation
 VojtechFranc	if we have time it would be great to do some benchmarking ... but it would be task 3 when the rest is finished
 alesis-novik	benchmarking openCV?
 VojtechFranc	no, only the EM implemenation from openCV
 VojtechFranc	using the very same data we will use in our experiments
 alesis-novik	I'd need to familiarize myself with openCV first
 VojtechFranc	algorithm in openCV are typically state-of-the-art implemenations so it would be nice to know how we compare
 VojtechFranc	ok, but as I said it just a bonus if we have time
 VojtechFranc	another quation regarding your implementationof SMEM: what do you use a the stopping condition of the partial EM step?
 VojtechFranc	I'm asking because it was not fully described in the paper
 VojtechFranc	In my implementation I stop it based on minimal change in likelihood
 alesis-novik	I used the same stopping conditions as for full EM: likelihood change and max iterations
 VojtechFranc	perfect
 alesis-novik	So what about the factor model EM?
 VojtechFranc	it would be also good but experiments are more important. I'm not sure we would finish everything in time
 VojtechFranc	I prefer less algorithms but careffuly implemented
 VojtechFranc	have you read the paper ?
 alesis-novik	I've skimmed it. It does seem to have an additional part to consider
 VojtechFranc	which one?
 VojtechFranc	another small issue: would you manage to translate the python examples to Matlab?
 alesis-novik	I don't think there's a Matlab modular interface, so it would be problematic. I think octave modular works now, so that would be easier
 VojtechFranc	isn't there option to use matlab static interface  ?
 blackburn	VojtechFranc: hi, have you any matlab code for kPCA?
 VojtechFranc	blackburn, yes
 blackburn	VojtechFranc: could you email it to me? blackburn91@gmail.com
 VojtechFranc	it is in matlab toolbox http://cmp.felk.cvut.cz/cmp/software/stprtool/index.html     function kpca
 blackburn	hmm okay thanks ;)
 alesis-novik	I'm not that familiar with how the static interfaces work
 alesis-novik	I could look into it
 VojtechFranc	ok, but only if it doesn't consume to much time otherwise octave is ok
 VojtechFranc	that's it from my side. So, I'll prepare the experiments and mail it to you in the evening
 alesis-novik	So I'll prioritize running the experiments and benchmarking them for now, is that fine?
 VojtechFranc	yes please
 VojtechFranc	you can also write the documentation in parallel
 alesis-novik	ok, I probably should add more specifics to the current method documentation
 VojtechFranc	ok, see you
 alesis-novik	See you VojtechFranc
-!- VojtechFranc [~quassel@gw-101.scnet.cz] has quit [Remote host closed the connection]
-!- sploving1 [~sploving@210.77.14.219] has joined #shogun
 sploving1	I met a problem
 sploving1	in kernel_gaussian_modular.rb,
 sploving1	feats_train=Modshogun::RealFeatures.new(fm_train_real)  it said: can't convert nil into String (TypeError)
 sploving1	but when I use    feats_train=Modshogun::RealFeatures.new    feats_train.set_feature_matrix fm_train_real
 sploving1	it works
 sploving1	I discussed with seriahex, he also have no idea~
 sploving1	sonney2k, any idea?
 sploving1	any suggestion?
 sploving1	I discussed with serialhex,
 heiko	sonney2k, are you there?
 heiko	I found an inconsisty in get_feature_vector:
 heiko	SimpleFeatures::get_feature_vector returns a pointer to an elements in its matrix
 heiko	StringFeatures::get_feature_vector returns a copy of a string
 heiko	which one is the desired behaviourr?
 heiko	i will set the do_free flag to true in StringFeatures
 heiko	should solve the problem
 heiko	sonney2k, ohoh another problem
 heiko	list_memory_allocs says 0 blocks
 heiko	but valgrind sais memory leak
@sonney2k	sploving1, you need to implement typecheck typemaps
@sonney2k	that will fix the issue
@sonney2k	heiko, only ever return pointers if possible
@sonney2k	heiko, what kind of leaks?
 heiko	currently checking
 sploving1	sonney2k, I added the typemap %typemap(typecheck, precedence=SWIG_TYPECHECK_POINTER) shogun::SGMatrix<SGTYPE>
 sploving1	typecheck
 heiko	but why is feature vector copied in StringFeatures? simply a mistake?
 heiko	sonney2k, mmh basic minimal memory leaks here. i will do a make clean and make again
@sonney2k	heiko, we mighta ctually leak the CSet object with the malloc traces or so ... but if you find leaks please fix them (if not too hard)
 heiko	sonney2k, ok i am onto it
@sonney2k	heiko, well there are 2 kinds of get_feature_vector
@sonney2k	one is just returning the pointer
@sonney2k	and one is returning a preprocessed copy
@sonney2k	so it might need to be freed
 heiko	SGVector<ST> get_feature_vector(int32_t num)
 heiko	makes always a copy
@sonney2k	I know
 heiko	so what about
 heiko	return SGVector<ST>(dst, l, true);
@sonney2k	I was trying to change this behavior when I recognized that I need the SG_MALLOC transition for that
@sonney2k	no
@sonney2k	don't always copy
 heiko	ok
 heiko	then I just remove the copying
@sonney2k	heiko, that will crash if there is a string preproc attached
@sonney2k	removing the whole fucntion seems more appropriate
 heiko	but a getter which returns an SGVector is nice
 heiko	also the setter below that method does copy the input parameter
@sonney2k	then changing the other get-feature_vector function to return SGVector would be the best fix
 heiko	but its entioned in the comments
 heiko	ok, will do then
@sonney2k	hmmhh
@sonney2k	wait
@sonney2k	there is a catch
@sonney2k	features provide a get_feature_vector() function
@sonney2k	and they return a bool flag 'do_free'
@sonney2k	so whenever one does get_feature_vector() one has to call free_feature_vector(..., do_free)
@sonney2k	and then the respecitive string class can clean up
@sonney2k	I am tempted to totally drop the free_feature_vector() functions and instead use the SGVector v; v.free_vector() stuff
@sonney2k	however this assumes that no feature class will do anything in addtion to free_feature_vector
@sonney2k	and I am not sure about this
@sonney2k	I don't recall exactly but I think we had features that do something extra
@sonney2k	I think filefeatures
 heiko	mmh
 heiko	btw the memory block set is not deleted, that is the cause for mem leak
@sonney2k	so the only way to get this to work then would be to use the free_feature_vector(SGVector v) for the feature classes
@sonney2k	heiko, as I guessed...
 heiko	sonney2k, but add a SG_UNREF(sg_mallocs) causes memory errors
 heiko	sonney2k,  i will be afk for some time, taking a break
@sonney2k	heiko, k, I am adding the malloc stuf to exit_shogun ... lets see what happens
@sonney2k	sploving1, did you commit that somewhere?
@sonney2k	heiko, I understand why it crashes - it is because it wants to store the de-allocation
@sonney2k	but the object is deleted under its feet
 sploving1	https://github.com/shogun-toolbox/shogun/commit/c4fc6c56b4039b949f0a939559a46ebad65c0662
@sonney2k	sploving1, and the typecheck typemaps triggers when called in constructor or not?
 sploving1	yeap
@sonney2k	sploving1, I mean like printf("yes I am in typecheck typemap") and printf("yes this is a valid array\n")
 sploving1	I donot understand clearly, can you expain it more ? sonney2k
@sonney2k	sploving1, please add some printf's in the typecheck typemap like the ones above
@sonney2k	if these trigger on your example we know that the problem is sth else
 sploving1	okay. I know
@sonney2k	heiko, ok fixed
 CIA-87	shogun: Soeren Sonnenburg master * r5f296e7 / (src/shogun/base/DynArray.h src/shogun/base/init.cpp):
 CIA-87	shogun: print leaks on exit when configure with --enable-trace-mallocs and do
 CIA-87	shogun: proper cleanup - https://github.com/shogun-toolbox/shogun/commit/5f296e7885667071409c47300c54b67bb1f7991d
@sonney2k	heiko, I was thinking more about the string features get_function.
@sonney2k	and other feature access functions for that matter
@sonney2k	if everything is in one function then we have to rely on  SGVector v.free_vector() support
@sonney2k	heiko, and since nothing in shogun uses this so far (just checked) I am very much in favour of doing it this way
@sonney2k	heiko, so removing free_feature_vector from all feature classes and only ever returning SGVector<xxx> with properly set free flag is the way to go
@sonney2k	then one would just call v.free_vector() at the end and all good
-!- sploving1 [~sploving@210.77.14.219] has left #shogun []
 heiko	sonney2k, ok that sounds good
-!- sploving1 [~sploving@210.77.14.219] has joined #shogun
 sploving1	sonney2k, not trigger the typecheck
@sonney2k	sploving1, in both cases not?
@sonney2k	heiko, the other thing is fixed now
 heiko	sonney2k, ok
 heiko	just checked out
 heiko	currently compiling
 sploving1	no. in feats_train=Modshogun::RealFeatures.new
 sploving1	feats_train.set_feature_matrix fm_train_real it trigger typecheck and typemap
 sploving1	feats_test=Modshogun::RealFeatures.new(fm_test_real) this does not trigger
 sploving1	sonney2k, what do you suggest?
@sonney2k	sploving1, does that happen with labels too?
@sonney2k	I mean when you do Labels([1,2,3])
 sploving1	I have not tried that
@sonney2k	vs. x=Labels() x.set_labels([1,2,3])
@sonney2k	(not sure what the ruby syntax is but you get the idea)
 sploving1	x=Modshogun::Labels.new([1,2,3]) it works
 sploving1	sonney2k, Labels work!
@sonney2k	sploving1, hmmhh so then matrix should too
 sploving1	maybe the syntax erro
 heiko	sonney2k, tracing  works perfect now
 sploving1	but i do not know
 sploving1	sonney2, I gtg bye
@sonney2k	sploving1, can you show me the example?
 sploving1	sonney2k,  it is in the example dir
@sonney2k	heiko, that memory tracing stuff is tricky/hacky - lots of unwanted side effects...
 sploving1	kernel_gaussian_modular
@sonney2k	heiko, but all good now -> great :)
@bettyboo	strange
 heiko	everything ok now :)
 sploving1	ruby kernel_gaussian_modular.ry can work
 sploving1	but when you change it  to be feats_test=Modshogun::RealFeatures.new(fm_test_real). it cannot work
 sploving1	bye
@sonney2k	sploving1, how do I call the example?
-!- sploving1 [~sploving@210.77.14.219] has left #shogun []
@sonney2k	hmmhh, so it has to wait until tomorrow
@sonney2k	or serialhex when you are around and can tell me how I can successfully run kernel_gaussian_modular.rb - please say so
@sonney2k	heiko, so you think it makes more sense with the get_feature_vector stuff right?
 heiko	yes,
 heiko	only problem i see:
 heiko	feature cache
 heiko	because now free_feature_vector takes an index
 heiko	and unlocks the element from the cache
@sonney2k	heiko, argh
@sonney2k	I missed that
@sonney2k	big problem...
 heiko	one could search
 heiko	but that makes linear time in contrast to constant time for free
@sonney2k	heiko, the only other option I see is to attach some callback function to SGVector
@sonney2k	such that this can be called to do exactly such cleanup tasks later
@sonney2k	overhead would be one more ptr in SGVector
@sonney2k	ptr to some struct that contains a callback function and metadata
-!- f-x [~user@117.192.221.29] has quit [Quit: ERC Version 5.3 (IRC client for Emacs)]
@sonney2k	heiko, any thoughts?
 heiko	yes the callback is good
 heiko	but adds more stuff
 heiko	so using it will become more difficult
 heiko	but ok
 heiko	btw DynArray causes some problems
 heiko	valgrind sais no leaks
 heiko	but list_memory_allocs() shows allocated blocks
@sonney2k	actually we could even make it slightly better - derive a SGVectorWithCache class
 heiko	yeah thats sounds better
@sonney2k	and overload the virtual free_vector() funciton
 heiko	that i would like
 heiko	sonney2k, btw string kernel grid-search now works
 heiko	(besides list_memory_allocs complaining that there is allocated memory, which is not true)
 heiko	sonney2k, when i change the name of SGString::length
 heiko	python does not compile
 heiko	are there any maps for the types if DataType.h?
-!- in3xes_ [~in3xes@180.149.49.227] has joined #shogun
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
 heiko	sonney2k, ok got it, swig_typemaps :)
@bettyboo	:*)
 heiko	sonney2k, list_memory_allocs still broken (complains that blocks were reserved in DynArray.h)
 heiko	pushing string kernel grid-search now
-!- in3xes_ is now known as in3xes
-!- in3xes_ [~in3xes@180.149.49.227] has joined #shogun
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
-!- heiko [~heiko@134.91.52.218] has left #shogun []
-!- blackburn [~blackburn@109.226.104.206] has quit [Ping timeout: 255 seconds]
-!- in3xes_ [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
-!- blackburn [~blackburn@109.226.104.206] has joined #shogun
 CIA-87	shogun: Soeren Sonnenburg master * r72b9bef / (25 files in 12 dirs):
 CIA-87	shogun: Merge pull request #245 from karlnapf/master
 CIA-87	shogun: model selection for string kernels (+31 more commits...) - https://github.com/shogun-toolbox/shogun/commit/72b9bef392f25fd1f0971367b0d574359700d112
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
-!- in3xes_ [~in3xes@180.149.49.227] has joined #shogun
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
-!- in3xes_ is now known as in3xes
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 240 seconds]
 CIA-87	shogun: Soeren Sonnenburg master * rc8fa327 / src/interfaces/python_static/PythonInterface.cpp : fix compile error in static python interface - https://github.com/shogun-toolbox/shogun/commit/c8fa3276755a73ff426dd10ecd7ddc3aeeb10dfc
 CIA-87	shogun: Soeren Sonnenburg master * r4172a45 / examples/undocumented/libshogun/modelselection_grid_search_linear.cpp : don't allocate objects on stack (this did evade the memory tracing) - https://github.com/shogun-toolbox/shogun/commit/4172a4540b588bb845764ab14c3b5f8868e6645d
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
 CIA-87	shogun: Sergey Lisitsyn master * ra45b631 / src/shogun/preprocessor/LocallyLinearEmbedding.cpp : DSYEVR for LLE - https://github.com/shogun-toolbox/shogun/commit/a45b631cc43de354310f4d3ece5414b9fdd11399
 CIA-87	shogun: Sergey Lisitsyn master * r4c5fc86 / (2 files in 2 dirs): Merge branch 'master' of github.com:shogun-toolbox/shogun - https://github.com/shogun-toolbox/shogun/commit/4c5fc86f761bd73f1c2423282da496d10de1f2f6
 CIA-87	shogun: Soeren Sonnenburg master * r1c63169 / examples/undocumented/libshogun/features_copy_subset_string_features.cpp : fix compile error in string subset example - https://github.com/shogun-toolbox/shogun/commit/1c631698486d9708d10e14f91743d095fa99e01d
 CIA-87	shogun: Soeren Sonnenburg master * rb2464db / (4 files in 4 dirs): fix compile errors in static interfaces - https://github.com/shogun-toolbox/shogun/commit/b2464db95dfe09f80c0203452dbe1504f91bad74
-!- in3xes [~in3xes@180.149.49.227] has quit [Ping timeout: 258 seconds]
-!- in3xes [~in3xes@180.149.49.227] has joined #shogun
 blackburn	sonney2k: is there any packed storage distance matrix routine?
 blackburn	it seems no..
 blackburn	okay, will do later
@sonney2k	there is none indeed
 blackburn	sonney2k: what do you prefer, make some transition to packed storage now or later?
@sonney2k	blackburn, ?
@sonney2k	for what
 blackburn	mds, lle, ..
 blackburn	well it is a big amount of work..
 blackburn	hmm and no dsyevr analogue..
 blackburn	so I think I should do it later
@sonney2k	blackburn, is this only for speed?
@sonney2k	or memory?
@sonney2k	or both?
 blackburn	memory-wise
 blackburn	no speed impact I guess
 blackburn	it already uses all the things of symmetricity of matrices
 blackburn	so just /2 memory
@sonney2k	you already use symmetry right?
@sonney2k	or no?
 blackburn	I use it in means of some matrix-vector multiplications and so on
 blackburn	but not memory wise
 blackburn	I store the whole matrix
 blackburn	in mds, isomap, lle
@sonney2k	blackburn, I don't know ... it is quite some work to change it .... IIRC I did this for custom kernel but no operations on top...
 blackburn	let's make this change later
 blackburn	after 1.0
@sonney2k	yeah...
 blackburn	I guess it is not only my algos - kernels too
 blackburn	it would be a surprise: "Shogun now uses 1/2 of memory it used before" :D
@sonney2k	there are other things to optimize btw
@sonney2k	if one uses python, currently everything is copied
@sonney2k	and not just in python
@sonney2k	now that we use malloc & co we can do some clever hacks
@sonney2k	i.e. we can tell a python numpy matrix to no longer own the object
 blackburn	I will change preprocs to use realloc
 blackburn	some memory wise eifficiency
@sonney2k	and just own the memory
@sonney2k	this would work if a numpy array is allocated in fortran order
 CIA-87	shogun: Sergey Lisitsyn master * r5ce74be / src/NEWS : Some additions and fixes for NEWS - https://github.com/shogun-toolbox/shogun/commit/5ce74beed864066a3506f85c2a5a1d0251f63e6b
@sonney2k	blackburn, order='Fortran' argument
-!- in3xes [~in3xes@180.149.49.227] has quit [Quit: Leaving]
 blackburn	sonney2k: you forgot a half of algos I did ;)
@sonney2k	blackburn, your fault to not update NEWS
 blackburn	you didn't say we should ;)
@sonney2k	blackburn, I didn't say that you should turn shogun upside down
@sonney2k	look what you did with all that machine, SGVector etc stuff!
 blackburn	hey you did, not me :D
 blackburn	I think I will focus on doc this night
@sonney2k	blackburn, but you complained like 100 times or so...
@sonney2k	so I had no choice
 blackburn	you are the Lenin, I'm just Krupskaya ahaha
@sonney2k	much worse, I only say Stalingrad
 blackburn	hey but things become much better in some ways
 blackburn	e.g. no crazy typemaps we had before
@sonney2k	blackburn, yes it is the 1945 of shogun - I totally surrendered ;-)
@sonney2k	but more seriously - I agree
@sonney2k	the only problem we have is that the SGVector transition is not finished
 blackburn	we did for interfaces
@sonney2k	we really need to change all the get/set functions
@sonney2k	blackburn, not for all...
 blackburn	hmm
@sonney2k	and we need to do it for the internal storage to
@sonney2k	to
@sonney2k	too
 blackburn	just recalled, fyi I finish work on algos on 6th of Aug or so
 blackburn	*as planned
 blackburn	after that we will try to do some bioinf stuff with chris
@sonney2k	blackburn, you are a machine
@sonney2k	you mean as example?
 blackburn	if it successful - yes
 blackburn	is*
 blackburn	we'll try to do some manifold learning for promoters
 blackburn	I'm not sure how it would work but we'll make a try hah
@sonney2k	blackburn, which distance?
 blackburn	I don't know anything yet
@sonney2k	may I suggest you use the weighteddegreepositionkernel and create a distance from it
 blackburn	don't we have it now?
@sonney2k	(promoter detection was my subject...)
@sonney2k	yes it is available
@sonney2k	if you want to be extra good/clever, you use a combination of 3 kernels
 blackburn	the problem is LLE family algos work in euclidean space..
@sonney2k	I can point you to a paper or my diss if you want to know which - but hey actually in applications/arts there is the code for this
@sonney2k	blackburn, so?
 blackburn	I mean I can apply only distance-based preprocessors
 blackburn	with kernels, etc
@sonney2k	kernel(x_i,x_j) = <Phi(x_i), Phi(x_j)>
@sonney2k	<.,.> is dot product
 blackburn	so how to apply LLE with kernel?
@sonney2k	blackburn, use KernelDistance
 blackburn	there is no dot product inside this algo
 blackburn	it depends on feature vectors itself
 blackburn	not only pairwise distances
@sonney2k	blackburn, then no chance
 blackburn	only MDS, Isomap and Laplacian Eigenmaps involves distances
 blackburn	bad thing to me :(
 blackburn	well if I found some euclidean representation or so
 blackburn	it would work
 blackburn	sonney2k: btw your dissertation is awesome
@sonney2k	blackburn, well it is possible when you extract all n-grams before and after the promoter
@sonney2k	that works pretty well already
@sonney2k	Phi(x) for the WD kernel is just too big
@sonney2k	though you can use a WD kernel of very low degree, say 1-3
@sonney2k	then it would work too
@sonney2k	blackburn, can you use dotfeatures?
 blackburn	sonney2k: for what?
@sonney2k	for your distance stuff
@sonney2k	you won't have feature matrix access
@sonney2k	*argh*
 blackburn	distances for MDS, Isomap and Laplacian Eigenmaps and simple dense features for LLE family
 blackburn	I don't have to work with features itself only with MDS, Isomap and Lapl.eigmaps
 blackburn	dotfeatures will work too I guess, if I can get some matrix from it ;)
-!- in3xes [~in3xes@180.149.49.230] has joined #shogun
 blackburn	sonney2k: you become activated as I said about bioinf :)
@sonney2k	adding a single virtual functions increases the size of an object to at least 16 bytes
@sonney2k	blackburn, not really
@sonney2k	blackburn, I am still much more interested in the algorithms than the application
@sonney2k	but of course this varies over time
* blackburn just looked over work he done - it seems he is interested in eigenvalues computation
 blackburn	:D
@sonney2k	:)
@sonney2k	virtual functions are actually not *that* bad
@sonney2k	blackburn, I think of deriving from SGVector
 blackburn	sonney2k: what to derive?
@sonney2k	some vector class that can have additional functions that are called when the vector is freed
 blackburn	what is the example of such funcs?
@sonney2k	blackburn, when we currently do get_feature_vector in e.g. SimpleFeatures we have to call free_feature_vector later on to free it (potentially)
@sonney2k	now there is a case where the feature matrix is not in memory
@sonney2k	then just one vector is computed and *cached*
 blackburn	I see
@sonney2k	on free_feature_vector cache is updated
@sonney2k	now when we instead of the orginal free_feature_vector function want to call SGVector::free_vector() we need to do sth about it
@sonney2k	that is we need to overload the free* function
@sonney2k	and also recall the index of the vector
@sonney2k	the reationale here is that one only ever calls SGVector based methods when later manipulating the vector
 blackburn	I see
@sonney2k	that would indeed work - the problem is now that SGVector is a templated function...
@sonney2k	class
 blackburn	just ran ltsa with 6000 examples
 blackburn	it took 874mb of ram
 blackburn	ho-ho
@sonney2k	compiling shogun takes 1.5G - so that is nothing :D
 blackburn	will try 12000
 blackburn	127s taken
 blackburn	hmm
@sonney2k	isn't it growing n^2 or worse?
 blackburn	memory or speed?
@sonney2k	memory
 blackburn	oh yes, n^2
 blackburn	I shouldn't run 12000 :D
 blackburn	may be 10000 is possible on my laptop
@sonney2k	blackburn, better compute how much mem it reuqired
@sonney2k	s
@sonney2k	requires
 blackburn	2.1g for 10000
 blackburn	took a little swap for computations
@sonney2k	I don't have that much mem here
@sonney2k	and I disabled linux' overcommit
 blackburn	didn't you have macbook with >4?
@sonney2k	blackburn, how bad do you think it is to grow SGVector by 8 bytes?
@sonney2k	8G
 blackburn	why don't you have 2.1? ;)
@sonney2k	yes el-cheapo memory
 blackburn	I think it is not bad at all
@sonney2k	because there are no modules of size 21G :D
 blackburn	2.1
 blackburn	not 21
@sonney2k	blackburn, I am only concerned about short vecotrs
@sonney2k	but probably rare - you are right
 blackburn	aren't we already have redundant vector len?
 blackburn	IIRC we do badass large-scale things so a little overhead is nothing hehe
@sonney2k	blackburn, redundant?
@sonney2k	I mean we need ptr & length...
 blackburn	yes but we store length both in some internals
 blackburn	and in sgvectors
 blackburn	as much length's as much sgvectors we have
 blackburn	right?
@sonney2k	that is differnent though - I mean we have to pass the length anyway
 blackburn	hmm
 blackburn	failed with shogunexception
 blackburn	hehe
@bettyboo	8)
 blackburn	I guess no memory to store new features
 blackburn	sonney2k: http://www.youtube.com/watch?v=FhWtJ2OwZ4A&feature=player_embedded seen this? :)
@sonney2k	blackburn, didn't it say so 'no new memory' ?
 blackburn	sonney2k: no, just shogun exception
@sonney2k	I hope they have taken out the battery...
 blackburn	will it explode?
@sonney2k	blackburn, btw I discovered some issue when the new trace malloc stuff won't work
 blackburn	which one?
@sonney2k	as soon as an object is allocated on stack - we have means to discover that it gets destroyed
--- Log closed Fri Jul 29 00:00:46 2011
