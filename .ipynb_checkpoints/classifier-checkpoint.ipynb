{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the Core Developers of Shogun\n",
    "*Moiz Sajid*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['copy', 'clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import scipy.stats\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tools as t\n",
    "import re\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_string(string): \n",
    "    \"\"\"\"\n",
    "        Parse the input string and tokenize it using regular expressisons:\n",
    "        First clean the string such that it does not have any punctuation or number, it must only have a-z and A-Z.\n",
    "        Please note that while doing this, the spaces much not get disturbed, but in case of multiple spaces convert \n",
    "        them to one space.\n",
    "        Then convert the string to lower case and return its words as a list of strings.\n",
    "        \n",
    "        Example:\n",
    "        --------\n",
    "        Input :  computer scien_tist-s are,,,  the  rock__stars of tomorrow_ <cool>  ????\n",
    "        Output:  ['computer', 'scientists', 'are', 'the', 'rockstars', 'of', 'tomorrow']\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        string: string to be parsed...\n",
    "        re: regular expression to be used for the tokenization.\n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        list of tokens extracted from the string...\n",
    "    \"\"\"\n",
    "    #string = string.decode('utf8')\n",
    "    #new_string = re.sub(u\"(?:_|-)\", u\"\", string)\n",
    "    #output = re.findall(u\"[a-zA-Z_*-]+\", new_string)\n",
    "    new_string = re.sub(\"(?:_|-)\", \"\", string)\n",
    "    output = re.findall(\"[a-zA-Z_*-]+\", new_string)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_file(filename): # Parse a given file\n",
    "    \"\"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "        filename: name of text file to be read\n",
    "   \n",
    "        \n",
    "        Returns:\n",
    "        ---------\n",
    "        read file as raw string (with \\n, \\t, \\r, etc included)\n",
    "    \"\"\"\n",
    "    \n",
    "    username = []\n",
    "    text= []\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            #line = line.lstrip()\n",
    "            if line[0] == '@' and line[:6] != \"@sukey\":\n",
    "                line_split = line.split('\\t')\n",
    "                username.append(line_split[0][1:])\n",
    "                text.append(line_split[1])\n",
    "                \n",
    "    return username, text\n",
    "    \n",
    "def files_to_strings(X):\n",
    "    \"\"\"\n",
    "        Read an array (or list) of files where each file content is read in a string...\n",
    "        Input:\n",
    "        -------\n",
    "        X an array (or list) of file names\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X as a numpy array with each row containing a read string from the file...\n",
    "    \"\"\" \n",
    "    usernames = [] \n",
    "    texts = []\n",
    "    \n",
    "    for i in X:\n",
    "        username, text = parse_file(i)\n",
    "        usernames.extend(username)\n",
    "        texts.extend(text)\n",
    "        \n",
    "    return np.array(usernames), np.array(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    ''' Implements the Naive Bayes For Text Classification... '''\n",
    "    def __init__(self, classes):\n",
    "        self.classes=classes\n",
    "        self.setv = set()\n",
    "                    \n",
    "    def train(self, X, Y):\n",
    "        ''' Train the multiclass (or Binary) Bayes Rule using the given \n",
    "            X [m x d] data matrix and Y labels matrix\n",
    "            \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional examples.\n",
    "            Y: [m x 1] a label vector.\n",
    "            \n",
    "            Returns:\n",
    "            -----------\n",
    "            Nothing\n",
    "        '''\n",
    "        #vocabdict = defaultdict(int)\n",
    "        setv = set()\n",
    "        \n",
    "        self.clabel = np.unique(Y)\n",
    "        \n",
    "        self.dictlist = [ defaultdict(int) for x in range(len(self.classes)) ]\n",
    "        self.prior = np.array( [ 0.0 for x in range(len(self.classes)) ] )\n",
    "        self.count = np.array( [ 0.0 for x in range(len(self.classes)) ] )\n",
    "        \n",
    "        for cidx, class_label in enumerate(self.classes):\n",
    "            \n",
    "            class_examples = np.count_nonzero(Y == class_label)\n",
    "            \n",
    "            class_prior = class_examples / float( Y.shape[0] )\n",
    "            \n",
    "            class_data = X[Y == class_label]\n",
    "            \n",
    "            mydict = defaultdict(int)\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            for doc in class_data:        \n",
    "                myarr = parse_string(doc[0])\n",
    "                \n",
    "                for word in myarr:\n",
    "                    if word not in setv:\n",
    "                        setv.add(word)\n",
    "                    \n",
    "                    mydict[word] += 1\n",
    "                    \n",
    "                counter += len(myarr)\n",
    "        \n",
    "            self.prior[cidx] = class_prior\n",
    "            self.dictlist[cidx] = copy.deepcopy(mydict)\n",
    "            self.count[cidx] = counter\n",
    "        \n",
    "        self.setv = copy.deepcopy(setv)\n",
    "        self.prior = self.prior[:, np.newaxis]\n",
    "        self.count = self.count[:, np.newaxis]\n",
    "            \n",
    "        \n",
    "    def test(self, X):\n",
    "        \n",
    "        ''' Test the trained classifiers on the given set of examples \n",
    "        \n",
    "                   \n",
    "            Input:\n",
    "            ------\n",
    "            X: [m x d] a data matrix of m d-dimensional test examples.\n",
    "           \n",
    "            Returns:\n",
    "            -----------\n",
    "                pclass: the predicted class for each example, i.e. to which it belongs\n",
    "        '''\n",
    "        nexamples, nfeatures=X.shape\n",
    "                              \n",
    "        i = 0\n",
    "        result = [None] * nexamples\n",
    "        \n",
    "        while i < nexamples:\n",
    "            \n",
    "            words = parse_string(X[i][0])\n",
    "            \n",
    "            tprob = np.zeros((self.clabel.shape[0], 1))\n",
    "            \n",
    "            for word in words:\n",
    "                \n",
    "                class_prob = np.zeros((self.clabel.shape[0], 1))\n",
    "                \n",
    "                for cidx, clabel in enumerate(self.clabel):\n",
    "                    class_prob[cidx] += np.log10((self.dictlist[cidx][word]+1.0)/(self.count[cidx]+len(self.setv)))\n",
    "                \n",
    "                tprob += class_prob\n",
    "            \n",
    "            tprob = tprob * self.prior\n",
    "            result[i] = self.clabel[ tprob.argmax() ]\n",
    "            #print result[i]\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        return np.array(result)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "            Predict the label of given input example...\n",
    "            \n",
    "            Input\n",
    "            ---------\n",
    "            x: example (list of words)\n",
    "            \n",
    "        '''    \n",
    "        class_prob = np.zeros((self.class_label.shape[0], 1))\n",
    "        \n",
    "        for word in x:\n",
    "            for cidx, clabel in enumerate(self.clabel):\n",
    "                class_prob[cidx] += np.log10( ( self.dictlist[cidx][word] + 1.0 ) / ( self.count[cidx] + len( self.setv ) ) )\n",
    "\n",
    "        class_prob = class_prob * np.array(self.prior)\n",
    "\n",
    "        result = self.clabel[ class_prob.argmax() ]\n",
    "        \n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdir= \"./data/\"\n",
    "files=t.get_files(tdir,'*',withpath=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usernames, texts = files_to_strings(files)\n",
    "\n",
    "#usernames = usernames.reshape((usernames.shape[0], 1))\n",
    "texts = texts.reshape((texts.shape[0], 1))\n",
    "\n",
    "traindata, trainlabels, testdata, testlabels = t.split_data(texts, usernames)\n",
    "classes = np.unique(usernames)\n",
    "\n",
    "trainlabels = trainlabels[:, np.newaxis]\n",
    "testlabels = testlabels[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CoolMcCool' 'Dendemann' 'EinsZwo' 'HeikoS' 'Heikowankenobi' 'STDROCCurve'\n",
      " 'SWAGGRegation' 'afaikidk' 'besser82' 'besser82_' 'bettyboo' 'bitch'\n",
      " 'blackburn' 'blyad' 'drwiking' 'ebat' 'gausskern' 'iglesiasg'\n",
      " 'iglesiasg|afk' 'iglesiasg|shogun' 'iloveponies' 'k8D9' 'knrrrd' 'lambday'\n",
      " 'lambday_' 'lambdday' 'lisitsyn' 'mlsec' 'more_advertising'\n",
      " 'natasharomanov' 'no_sgd_regrets_y' 'rieck' 'rock_curve' 'shogun|sonney2k'\n",
      " 'sonimperator' 'sonne3000' 'sonney2k' 'sonney2k_' 'sonne|fallengod'\n",
      " 'sonne|god' 'swagg_minimizer' 'swagg_noregrets' 'swagg_regressor'\n",
      " 'thoralf' 'vowpalkardashian' 'what_heise' 'what_lisitsyn' 'what_switch'\n",
      " 'wiking']\n"
     ]
    }
   ],
   "source": [
    "print classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80286, 1) (80286, 1)\n",
      "(34404, 1) (34404, 1)\n"
     ]
    }
   ],
   "source": [
    "print traindata.shape, trainlabels.shape \n",
    "print testdata.shape, testlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] training a classifier for following classes ['CoolMcCool' 'Dendemann' 'EinsZwo' 'HeikoS' 'Heikowankenobi' 'STDROCCurve'\n",
      " 'SWAGGRegation' 'afaikidk' 'besser82' 'besser82_' 'bettyboo' 'bitch'\n",
      " 'blackburn' 'blyad' 'drwiking' 'ebat' 'gausskern' 'iglesiasg'\n",
      " 'iglesiasg|afk' 'iglesiasg|shogun' 'iloveponies' 'k8D9' 'knrrrd' 'lambday'\n",
      " 'lambday_' 'lambdday' 'lisitsyn' 'mlsec' 'more_advertising'\n",
      " 'natasharomanov' 'no_sgd_regrets_y' 'rieck' 'rock_curve' 'shogun|sonney2k'\n",
      " 'sonimperator' 'sonne3000' 'sonney2k' 'sonney2k_' 'sonne|fallengod'\n",
      " 'sonne|god' 'swagg_minimizer' 'swagg_noregrets' 'swagg_regressor'\n",
      " 'thoralf' 'vowpalkardashian' 'what_heise' 'what_lisitsyn' 'what_switch'\n",
      " 'wiking']\n",
      "[Info] Accuracy = 0.887106150448\n"
     ]
    }
   ],
   "source": [
    "print '[Info] training a classifier for following classes {}'.format(classes)\n",
    "nb=NaiveBayes(classes)\n",
    "nb.train(traindata,trainlabels)\n",
    "pclasses=nb.test(testdata)\n",
    "acc=np.sum(pclasses==testlabels)/float(testlabels.shape[0])\n",
    "print \"[Info] Accuracy = {}\".format(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
